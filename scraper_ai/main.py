"""
Point d'entr√©e principal pour le scraper AI
Scraping intelligent avec cache Supabase et s√©lecteurs dynamiques
"""
import argparse
import json
import time
import os
import sys
from pathlib import Path
from typing import List, Dict, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

# Ajouter le r√©pertoire parent au PYTHONPATH pour les imports
scraper_ai_path = Path(__file__).parent
parent_path = scraper_ai_path.parent
if str(parent_path) not in sys.path:
    sys.path.insert(0, str(parent_path))

try:
    from .intelligent_scraper import IntelligentScraper, scrape_site
    from .supabase_storage import SupabaseStorage, set_global_user
    from .config import PROMPT_VERSION
except ImportError:
    try:
        from scraper_ai.intelligent_scraper import IntelligentScraper, scrape_site
        from scraper_ai.supabase_storage import SupabaseStorage, set_global_user
        from scraper_ai.config import PROMPT_VERSION
    except ImportError:
        from intelligent_scraper import IntelligentScraper, scrape_site
        from supabase_storage import SupabaseStorage, set_global_user
        from config import PROMPT_VERSION


# Liste des couleurs communes √† ignorer pour le matching
COLOR_KEYWORDS = [
    # Fran√ßais ‚Äî couleurs de base
    'blanc', 'noir', 'rouge', 'bleu', 'vert', 'jaune', 'orange', 'rose', 'violet',
    'gris', 'argent', 'or', 'bronze', 'beige', 'marron', 'brun', 'turquoise',
    'kaki', 'sable', 'ivoire', 'creme', 'cr√®me',
    # Fran√ßais ‚Äî finitions et textures
    'brillant', 'mat', 'm√©tallis√©', 'metallis√©', 'm√©tallique', 'metallique',
    'perle', 'nacr√©', 'nacre', 'satin', 'chrome', 'carbone',
    'fonc√©', 'fonce', 'clair', 'fluo', 'neon', 'n√©on',
    # Fran√ßais ‚Äî couleurs sp√©cifiques v√©hicules (fr√©quentes dans les catalogues moto)
    'ebene', '√©b√®ne', 'graphite', 'anthracite', 'platine', 'titane',
    'cuivre', 'acier', 'cobalt', 'corail', 'ardoise', '√©tain',
    'nebuleux', 'n√©buleux', 'nebuleuse', 'n√©buleuse',
    'bonbon', 'diablo', 'champagne', 'phantom', 'fantome', 'fant√¥me',
    'combat', 'lime', 'sauge', 'cristal', 'obsidian', 'highland',
    'etincelle', '√©tincelle', 'velocite', 'v√©locit√©',
    # Anglais
    'white', 'black', 'red', 'blue', 'green', 'yellow', 'orange', 'pink', 'purple',
    'gray', 'grey', 'silver', 'gold', 'bronze', 'beige', 'brown', 'turquoise',
    'matte', 'glossy', 'metallic', 'pearl', 'satin', 'carbon',
    'dark', 'light', 'neon', 'bright',
    'ivory', 'charcoal', 'titanium', 'copper', 'steel', 'platinum', 'graphite',
    'racing', 'candy', 'phantom', 'midnight', 'arctic', 'cosmic', 'storm',
    # Descripteurs de couleur (souvent dans les noms de v√©hicules)
    'nouveau', 'nouvelle', 'special', '√©dition',
]


def _strip_accents(text: str) -> str:
    """Retire les accents d'une cha√Æne (√©‚Üíe, √®‚Üíe, etc.)"""
    import unicodedata
    nfkd = unicodedata.normalize('NFKD', text)
    return ''.join(c for c in nfkd if not unicodedata.category(c).startswith('M'))


def _deep_normalize(text: str) -> str:
    """Normalisation profonde : minuscules, sans accents, sans ponctuation, espaces unifi√©s.
    Ins√®re un espace entre lettres et chiffres coll√©s (ninja500 ‚Üí ninja 500).
    Fusionne les lettres simples cons√©cutives en un seul token (r l ‚Üí rl, s x f ‚Üí sxf)
    pour que "KLX110R L" et "KLX110RL" produisent le m√™me r√©sultat.
    """
    import re
    if not text:
        return ''
    text = text.lower().strip()
    text = _strip_accents(text)
    # Ins√©rer un espace entre lettres et chiffres coll√©s: "ninja500" ‚Üí "ninja 500"
    text = re.sub(r'([a-z])(\d)', r'\1 \2', text)
    text = re.sub(r'(\d)([a-z])', r'\1 \2', text)
    # Retirer tout sauf lettres, chiffres, espaces
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    # Unifier les espaces multiples
    text = re.sub(r'\s+', ' ', text).strip()

    # Fusionner les lettres simples cons√©cutives: "r l" ‚Üí "rl", "s x f" ‚Üí "sxf"
    # Cela uniformise "KLX110R L" (‚Üí klx 110 r l ‚Üí klx 110 rl)
    # et "KLX110RL" (‚Üí klx 110 rl) vers le m√™me r√©sultat.
    words = text.split()
    merged: list = []
    i = 0
    while i < len(words):
        if len(words[i]) == 1 and words[i].isalpha():
            # D√©but d'une s√©quence potentielle de lettres simples
            letters = [words[i]]
            j = i + 1
            while j < len(words) and len(words[j]) == 1 and words[j].isalpha():
                letters.append(words[j])
                j += 1
            if len(letters) > 1:
                merged.append(''.join(letters))
            else:
                merged.append(words[i])
            i = j
        else:
            merged.append(words[i])
            i += 1
    text = ' '.join(merged)

    return text


def remove_colors_from_string(text: str) -> str:
    """Retire les mots de couleur d'une cha√Æne de caract√®res.
    Compare des mots entiers (pas de substring) pour √©viter les faux positifs.
    """
    if not text:
        return ''

    normalized = _deep_normalize(text)
    words = normalized.split()
    filtered_words = []

    # Normaliser les mots-couleur une seule fois
    normalized_colors = set(_deep_normalize(c) for c in COLOR_KEYWORDS if c)

    for word in words:
        if word not in normalized_colors:
            filtered_words.append(word)

    return ' '.join(filtered_words)


# Liste des marques connues pour identification (tri√©e par longueur d√©croissante)
KNOWN_BRANDS = sorted([
    'kawasaki', 'honda', 'yamaha', 'suzuki', 'ktm', 'husqvarna',
    'triumph', 'cfmoto', 'cf moto', 'aprilia', 'vespa', 'piaggio', 'ducati',
    'bmw', 'harley-davidson', 'harley davidson', 'indian', 'royal enfield',
    'can-am', 'can am', 'polaris', 'arctic cat', 'sea-doo', 'sea doo',
    'ski-doo', 'ski doo', 'brp', 'segway', 'kymco', 'adly', 'beta',
    'cub cadet', 'john deere', 'gas gas', 'gasgas', 'sherco', 'benelli',
    'mv agusta', 'moto guzzi', 'zero', 'energica', 'sur-ron', 'surron',
], key=len, reverse=True)

# Normaliser les marques pour le matching
_NORMALIZED_BRANDS = [(_deep_normalize(b), b) for b in KNOWN_BRANDS]

# Sous-mod√®les significatifs : ces mots ne doivent JAMAIS √™tre ignor√©s lors du matching
# par inclusion. Si un c√¥t√© a "SE" et l'autre non, ce ne sont PAS les m√™mes produits.
SIGNIFICANT_SUBMODEL_WORDS = {
    # Variantes de performance / √©dition
    'se', 'r', 'rr', 'rs', 'x', 'xr', 'xc', 'xs', 'xd', 'xt', 'xmr', 'xtp',
    'sx', 'sxf', 'exc', 'excf',
    'factory', 'edition', 'special', 'limited', 'elite', 'premium', 'pro',
    'sport', 'sports', 'adventure', 'adv',
    # Variantes touring / utilitaire
    'touring', 'tour', 'gt', 'gts', 'trail', 'rally',
    'eps', 'dps', 'ess',  # Direction assist√©e (mod√®les diff√©rents)
    'lt', 'st',  # Light Touring, Sport Touring
    # Variantes taille / cylindr√©e
    'plus', 'max', 'mini', 'lite', 'base',
    # Variantes sp√©cifiques motos/VTT
    'enduro', 'supermoto', 'motard', 'scrambler', 'classic', 'heritage',
    'custom', 'cruiser', 'naked', 'street',
    # Combinaisons fusionn√©es de lettres (R L ‚Üí rl, etc.)
    # Toute combinaison incluant 'r' ou 'x' est significative
    'rl', 'rx', 'xl', 'fl', 'fx',
    # Tailles de cylindr√©e souvent dans le mod√®le
    '125', '150', '200', '250', '300', '350', '390', '400', '450', '500',
    '600', '650', '690', '700', '750', '790', '800', '850', '890', '900',
    '950', '1000', '1090', '1190', '1200', '1250', '1290',
}
# Normaliser les sous-mod√®les
_NORMALIZED_SIGNIFICANT = {_deep_normalize(
    w) for w in SIGNIFICANT_SUBMODEL_WORDS if w}

# Lettres simples significatives (pour v√©rification des formes fusionn√©es)
_SIGNIFICANT_SINGLE_LETTERS = {
    w for w in _NORMALIZED_SIGNIFICANT if len(w) == 1}


def _is_significant_diff(diff_words: set) -> bool:
    """V√©rifie si un ensemble de mots de diff√©rence contient un sous-mod√®le significatif.

    G√®re aussi les formes fusionn√©es: si un mot court (‚â§3 lettres) contient une lettre
    significative individuelle (r, x), il est consid√©r√© comme significatif.
    Ex: 'rl' contient 'r' ‚Üí significatif.

    Tout token num√©rique est significatif (tailles de roues, cylindr√©es, versions, etc.)
    car les nombres dans les noms de produits indiquent toujours une variante distincte.
    Ex: ELEKTRODE 20 ‚â† ELEKTRODE 16 (taille de roue diff√©rente)
    """
    for w in diff_words:
        # V√©rification directe dans le set
        if w in _NORMALIZED_SIGNIFICANT:
            return True
        # Tout nombre est significatif (taille, cylindr√©e, version, puissance)
        if w.isdigit():
            return True
        # Pour les tokens courts et purement alpha (possibles r√©sultats de fusion),
        # v√©rifier si une lettre individuelle est significative
        if 1 < len(w) <= 3 and w.isalpha() and _SIGNIFICANT_SINGLE_LETTERS:
            if any(c in _SIGNIFICANT_SINGLE_LETTERS for c in w):
                return True
    return False


# Mapping pour unifier les variantes de marques
_BRAND_ALIASES = {
    'cf moto': 'cfmoto',
    'harley davidson': 'harley davidson',
    'harley-davidson': 'harley davidson',
    'can am': 'can am',
    'can-am': 'can am',
    'sea doo': 'sea doo',
    'sea-doo': 'sea doo',
    'ski doo': 'ski doo',
    'ski-doo': 'ski doo',
    'gas gas': 'gasgas',
    'sur-ron': 'surron',
    'sur ron': 'surron',
}


def normalize_product_key(product: dict, ignore_colors: bool = False) -> Tuple[str, str, int]:
    """Cr√©e une cl√© normalis√©e pour identifier les produits (marque + mod√®le + ann√©e).

    Normalisation profonde: sans accents, sans ponctuation, espaces coll√©s entre 
    lettres/chiffres s√©par√©s, comparaison de mots entiers pour les couleurs.

    Args:
        product: Dictionnaire du produit
        ignore_colors: Si True, retire les couleurs du mod√®le pour le matching
    """
    import re

    raw_marque = str(product.get('marque', '')).strip()
    raw_modele = str(product.get('modele', '')).strip()
    annee = product.get('annee', 0) or 0

    # Nettoyer les pr√©fixes courants
    raw_marque = re.sub(
        r'^(manufacturier|fabricant|marque|brand)\s*:\s*', '', raw_marque, flags=re.I)
    raw_modele = re.sub(r'^(mod√®le|modele|model)\s*:\s*',
                        '', raw_modele, flags=re.I)

    marque = _deep_normalize(raw_marque)
    modele = _deep_normalize(raw_modele)

    # ‚îÄ‚îÄ Extraction depuis 'name' si marque ou mod√®le manquant ‚îÄ‚îÄ
    if not marque or not modele:
        name = str(product.get('name', '')).strip()
        if name:
            name_norm = _deep_normalize(name)

            detected_brand = ''
            rest_of_name = name_norm

            for norm_brand, original_brand in _NORMALIZED_BRANDS:
                if name_norm.startswith(norm_brand + ' ') or name_norm == norm_brand:
                    detected_brand = norm_brand
                    rest_of_name = name_norm[len(norm_brand):].strip()
                    break
                # Chercher la marque n'importe o√π dans le nom
                idx = name_norm.find(norm_brand)
                if idx >= 0:
                    detected_brand = norm_brand
                    rest_of_name = (
                        name_norm[:idx] + ' ' + name_norm[idx + len(norm_brand):]).strip()
                    rest_of_name = re.sub(r'\s+', ' ', rest_of_name)
                    break

            if detected_brand:
                if not marque:
                    marque = detected_brand
                if not modele:
                    # Retirer l'ann√©e du reste pour avoir le mod√®le pur
                    year_match = re.search(r'\b(20[12]\d)\b', rest_of_name)
                    if year_match:
                        if not annee:
                            annee = int(year_match.group(1))
                        rest_of_name = rest_of_name[:year_match.start(
                        )] + rest_of_name[year_match.end():]
                    modele = re.sub(r'\s+', ' ', rest_of_name).strip()
            elif not modele:
                # Aucune marque connue d√©tect√©e dans le nom ‚Äî utiliser le nom nettoy√© comme mod√®le
                # Cas fr√©quent : marque d√©j√† d√©finie (JSON-LD), nom = juste le mod√®le (ex: "Z900")
                year_match = re.search(r'\b(20[12]\d)\b', name_norm)
                if year_match:
                    if not annee:
                        annee = int(year_match.group(1))
                    name_norm = name_norm[:year_match.start(
                    )] + name_norm[year_match.end():]
                # Si la marque est d√©j√† d√©finie, la retirer du nom pour √©viter la duplication
                cleaned_name = name_norm
                if marque:
                    marque_norm = _deep_normalize(marque)
                    if cleaned_name.startswith(marque_norm + ' '):
                        cleaned_name = cleaned_name[len(marque_norm):].strip()
                    elif cleaned_name.endswith(' ' + marque_norm):
                        cleaned_name = cleaned_name[:-
                                                    len(marque_norm):].strip()
                modele = re.sub(r'\s+', ' ', cleaned_name).strip()

    # Unifier les alias de marques
    marque = _BRAND_ALIASES.get(marque, marque)

    # ‚îÄ‚îÄ Nettoyage du mod√®le : retirer les phrases parasites de localisation/concession ‚îÄ‚îÄ
    # Patterns courants : "en vente a shawinigan mvm motosport", "neuf a trois-rivieres", etc.
    _DEALER_NOISE_PATTERNS = [
        r'\b(?:en\s+vente|disponible|neuf|usage|usag[√©e])\s+(?:a|√†|chez|au)\b.*$',
        r'\b(?:mvm\s*motosport|morin\s*sports?|moto\s*thibault|moto\s*ducharme)\b.*$',
        r'\b(?:shawinigan|trois\s*[-\s]*rivi[e√®]res|montr[√©e]al|qu[√©e]bec|laval|longueuil|sherbrooke|drummondville|victoriaville|b[√©e]cancour)\b.*$',
        r'\b(?:concessionnaire|dealer|showroom|magasin|succursale)\b.*$',
    ]
    for pattern in _DEALER_NOISE_PATTERNS:
        modele = re.sub(pattern, '', modele, flags=re.I).strip()

    # Retirer les couleurs si demand√©
    if ignore_colors:
        modele = remove_colors_from_string(modele)

    # Nettoyer les espaces finaux
    marque = re.sub(r'\s+', ' ', marque).strip()
    modele = re.sub(r'\s+', ' ', modele).strip()

    return (marque, modele, annee)


def _pick_best_ref(ref_matches: List[dict], current_price: float) -> dict:
    """S√©lectionne le meilleur produit de r√©f√©rence parmi les candidats (prix le plus proche)."""
    best = None
    min_diff = float('inf')
    for ref in ref_matches:
        rp = float(ref.get('prix', 0) or 0)
        if rp > 0 and current_price > 0:
            diff = abs(current_price - rp)
            if diff < min_diff:
                min_diff = diff
                best = ref
        elif not best:
            best = ref
    return best or ref_matches[0]


def find_matching_products(reference_products: List[dict], comparison_products: List[dict],
                           reference_url: str, comparison_url: str,
                           ignore_colors: bool = False) -> List[dict]:
    """
    Trouve les produits du concurrent qui existent aussi dans le site de r√©f√©rence.

    Matching strict apr√®s normalisation profonde :
      1. Match exact (marque + mod√®le + ann√©e) ‚Äî apr√®s deepNormalize
      2. Match avec ann√©e wildcard ‚Äî si l'un des deux c√¥t√©s n'a PAS d'ann√©e (0),
         on accepte le match. Si les deux ont une ann√©e et qu'elles diff√®rent ‚Üí PAS de match.

    Les sous-mod√®les (SE, Touring, R, X, etc.) sont TOUJOURS respect√©s.
    Les ann√©es sont TOUJOURS respect√©es quand elles existent des deux c√¥t√©s.

    Retourne UNIQUEMENT les produits du concurrent qui ont une correspondance.
    """
    print(f"\n{'='*60}")
    print(f"üîç COMPARAISON AVEC LE SITE DE R√âF√âRENCE")
    print(f"{'='*60}")
    print(f"üìä R√©f√©rence: {reference_url} ({len(reference_products)} produits)")
    print(
        f"üìä Concurrent: {comparison_url} ({len(comparison_products)} produits)")
    if ignore_colors:
        print(f"üé® Mode: Ignorer les couleurs (matching √©largi)")

    # ‚îÄ‚îÄ Index des produits de r√©f√©rence ‚îÄ‚îÄ
    # Index 1 : cl√© compl√®te (marque, modele, annee) ‚Äî pour match exact
    ref_exact: Dict[Tuple, List[dict]] = {}
    # Index 2 : cl√© (marque, modele) ‚Üí liste de (annee, produit) ‚Äî pour match avec ann√©e wildcard
    ref_by_model: Dict[Tuple[str, str], List[Tuple[int, dict]]] = {}

    skipped_ref = 0
    for rp in reference_products:
        key = normalize_product_key(rp, ignore_colors=ignore_colors)
        marque, modele, annee = key

        if not modele:
            skipped_ref += 1
            continue

        # Index exact
        if key not in ref_exact:
            ref_exact[key] = []
        ref_exact[key].append(rp)

        # Index par mod√®le (pour wildcard ann√©e)
        model_key = (marque, modele)
        if model_key not in ref_by_model:
            ref_by_model[model_key] = []
        ref_by_model[model_key].append((annee, rp))

    print(
        f"   üìã Cl√©s de r√©f√©rence: {len(ref_exact)} (mod√®les uniques: {len(ref_by_model)}, ignor√©es: {skipped_ref})")
    sample_keys = list(ref_exact.keys())[:5]
    for k in sample_keys:
        print(f"      R√©f: marque='{k[0]}' modele='{k[1]}' annee={k[2]}")

    # ‚îÄ‚îÄ Matching ‚îÄ‚îÄ
    matched_products = []
    skipped_comp = 0
    match_levels = {'exact': 0, 'year_wildcard': 0, 'model_inclusion': 0}

    for product in comparison_products:
        key = normalize_product_key(product, ignore_colors=ignore_colors)
        marque, modele, annee = key

        if not modele:
            skipped_comp += 1
            continue

        current_price = float(product.get('prix', 0) or 0)
        ref_matches = None
        match_level = ''

        # ‚îÄ‚îÄ Niveau 1 : Match exact (marque + modele + annee) ‚îÄ‚îÄ
        if key in ref_exact:
            ref_matches = ref_exact[key]
            match_level = 'exact'

        # ‚îÄ‚îÄ Niveau 2 : Match avec ann√©e wildcard ‚îÄ‚îÄ
        # Seulement si au moins un c√¥t√© n'a PAS d'ann√©e (0).
        # Si les deux c√¥t√©s ont une ann√©e et qu'elles diff√®rent ‚Üí PAS de match.
        if not ref_matches:
            model_key = (marque, modele)
            candidates = ref_by_model.get(model_key, [])

            wildcard_matches = []
            for ref_annee, ref_prod in candidates:
                # Accepter si : l'un des deux n'a pas d'ann√©e
                if annee == 0 or ref_annee == 0:
                    wildcard_matches.append(ref_prod)
                # Si les deux ont une ann√©e identique (d√©j√† couvert par exact, mais au cas o√π)
                elif annee == ref_annee:
                    wildcard_matches.append(ref_prod)
                # Sinon (deux ann√©es diff√©rentes non-nulles) ‚Üí on refuse

            if wildcard_matches:
                ref_matches = wildcard_matches
                match_level = 'year_wildcard'

        # ‚îÄ‚îÄ Niveau 3 : Match par inclusion de mod√®le ‚îÄ‚îÄ
        # Si le mod√®le du concurrent est contenu dans le mod√®le de la r√©f√©rence (ou inversement),
        # on accepte le match SEULEMENT si la diff√©rence ne contient pas de sous-mod√®le significatif.
        #
        # Ex ACCEPT√â : "brute force 300" ‚Üî "brute force 300 rouge" (diff = couleur ‚Üí OK)
        # Ex REJET√â  : "450 sx" ‚Üî "450 sx se" (diff = "se" ‚Üí sous-mod√®le significatif ‚Üí REJET)
        # Ex REJET√â  : "450 sx se" ‚Üî "450 sx" (diff = "se" ‚Üí sous-mod√®le significatif ‚Üí REJET)
        if not ref_matches:
            for ref_key, ref_prods in ref_exact.items():
                ref_marque, ref_modele, ref_annee = ref_key
                if ref_marque != marque:
                    continue
                # V√©rifier compatibilit√© d'ann√©e
                if annee != 0 and ref_annee != 0 and annee != ref_annee:
                    continue
                if not ref_modele or not modele:
                    continue
                ref_words = set(ref_modele.split())
                comp_words = set(modele.split())

                is_subset = comp_words.issubset(
                    ref_words) or ref_words.issubset(comp_words)
                if not is_subset:
                    continue

                diff_words = ref_words.symmetric_difference(comp_words)
                has_significant_diff = _is_significant_diff(diff_words)

                if has_significant_diff:
                    if len(matched_products) < 50:
                        print(
                            f"      üö´ [inclusion rejet√©e] '{modele}' ‚â† '{ref_modele}' (diff significative: {diff_words})")
                    continue

                if not ref_matches:
                    ref_matches = []
                ref_matches.extend(ref_prods)
                match_level = 'model_inclusion'

        if not ref_matches:
            continue

        # S√©lectionner le meilleur match
        best_match = _pick_best_ref(ref_matches, current_price)
        ref_price = float(best_match.get('prix', 0) or 0)

        # Enrichir le produit avec les infos de comparaison
        product['prixReference'] = ref_price
        product['differencePrix'] = (
            current_price - ref_price) if current_price > 0 and ref_price > 0 else None
        product['siteReference'] = reference_url
        product['produitReference'] = {
            'name': best_match.get('name'),
            'sourceUrl': best_match.get('sourceUrl'),
            'prix': ref_price
        }

        if not product.get('sourceSite'):
            product['sourceSite'] = comparison_url

        matched_products.append(product)
        match_levels[match_level] = match_levels.get(match_level, 0) + 1

        if product['differencePrix'] is not None:
            diff_str = f"+{product['differencePrix']:.0f}$" if product['differencePrix'] >= 0 else f"{product['differencePrix']:.0f}$"
            level_icon = '‚úÖ' if match_level == 'exact' else 'üìÖ'
            print(
                f"   {level_icon} [{match_level}] {marque} {modele} {annee or ''}: {current_price:.0f}$ vs {ref_price:.0f}$ ({diff_str})")

    match_rate = (len(matched_products) / len(comparison_products)
                  * 100) if comparison_products else 0
    print(f"\n   üìã Concurrent - ignor√©s (mod√®le vide): {skipped_comp}")
    print(
        f"   üìä Matching: exact={match_levels['exact']}, wildcard ann√©e={match_levels['year_wildcard']}, inclusion mod√®le={match_levels['model_inclusion']}")

    if not matched_products and comparison_products:
        print(f"   ‚ö†Ô∏è Aucune correspondance! √âchantillon des cl√©s concurrent:")
        for p in comparison_products[:5]:
            k = normalize_product_key(p, ignore_colors=ignore_colors)
            print(
                f"      Conc: marque='{k[0]}' modele='{k[1]}' annee={k[2]} | name='{p.get('name', '')[:50]}'")

    print(
        f"\nüìà Correspondances: {len(matched_products)}/{len(comparison_products)} ({match_rate:.0f}%)")
    print(f"{'='*60}\n")

    return matched_products


def scrape_site_wrapper(args: tuple) -> Tuple[str, dict]:
    """Wrapper pour le scraping en parall√®le avec le nouveau syst√®me intelligent"""
    url, user_id, force_refresh, categories, inventory_only = args
    try:
        scraper = IntelligentScraper(user_id=user_id)
        result = scraper.scrape(
            url, force_refresh=force_refresh, categories=categories, inventory_only=inventory_only)
        return (url, {
            "companyInfo": {},
            "products": result.get('products', []),
            "metadata": result.get('metadata', {})
        })
    except Exception as e:
        import traceback
        print(f"‚ùå Erreur lors du scraping de {url}: {e}")
        print(f"üìã Trace compl√®te de l'erreur:")
        traceback.print_exc()
        return (url, {"companyInfo": {}, "products": []})


def main():
    parser = argparse.ArgumentParser(
        description=f'Scraper AI v{PROMPT_VERSION} - Scraping intelligent avec cache Supabase',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  # Extraire uniquement le site de r√©f√©rence (sans comparaison)
  python -m scraper_ai.main https://site-reference.com
  
  # Comparer des concurrents avec le site de r√©f√©rence
  python -m scraper_ai.main https://site-reference.com https://concurrent1.com https://concurrent2.com
  
  # Forcer la r√©g√©n√©ration du scraper (ignorer le cache)
  python -m scraper_ai.main --force-refresh https://site.com
  
  # Sp√©cifier l'utilisateur pour le cache Supabase
  python -m scraper_ai.main --user-id UUID https://site.com
  
  # Filtrer par cat√©gories (inventaire, occasion, catalogue)
  python -m scraper_ai.main --categories inventaire,occasion https://site.com
        """
    )
    parser.add_argument('urls', nargs='*',
                        help='URL(s) du/des site(s) √† scraper')
    parser.add_argument('--reference', '-r', dest='reference_url',
                        help='URL du site de r√©f√©rence pour comparer les prix')
    parser.add_argument('--force-refresh', '-f', action='store_true',
                        help='Forcer la r√©g√©n√©ration des scrapers (ignorer le cache)')
    parser.add_argument('--user-id', '-u', dest='user_id',
                        help='ID utilisateur pour le cache Supabase')
    parser.add_argument('--categories', '-c', dest='categories',
                        help='Cat√©gories √† scraper (inventaire,occasion,catalogue)')
    parser.add_argument('--invalidate-cache', '-i', action='store_true',
                        help='Invalider le cache pour les URLs sp√©cifi√©es')
    parser.add_argument('--ignore-colors', action='store_true',
                        help='Ignorer les couleurs lors du matching des produits (permet plus de correspondances)')
    parser.add_argument('--inventory-only', action='store_true',
                        help='Extraire seulement les produits d\'inventaire (exclut les pages catalogue/showroom)')

    args = parser.parse_args()

    urls = args.urls
    reference_url = args.reference_url
    force_refresh = args.force_refresh
    ignore_colors = args.ignore_colors
    inventory_only = args.inventory_only
    user_id = args.user_id or os.environ.get('SCRAPER_USER_ID')

    # V√âRIFICATION OBLIGATOIRE: L'utilisateur doit √™tre connect√©
    if not user_id:
        print(f"\n{'='*70}")
        print(f"‚ùå AUTHENTIFICATION REQUISE")
        print(f"{'='*70}")
        print(f"Vous devez √™tre connect√© pour utiliser le scraper.")
        print(f"\nSolutions:")
        print(f"  1. Lancez le scraping depuis le dashboard (recommand√©)")
        print(f"  2. Utilisez --user-id UUID avec votre ID utilisateur")
        print(f"  3. D√©finissez la variable d'environnement SCRAPER_USER_ID")
        print(f"{'='*70}\n")
        return

    # Parser les cat√©gories
    categories = None
    if args.categories:
        categories = [c.strip() for c in args.categories.split(',')]
    else:
        # Par d√©faut: TOUTES les cat√©gories pour extraction compl√®te
        # L'√©tat (neuf/usag√©/catalogue) est d√©tect√© automatiquement par produit
        categories = ['inventaire', 'occasion', 'catalogue']

    if not urls:
        parser.print_help()
        return

    # Mode invalidation de cache
    if args.invalidate_cache:
        if user_id:
            storage = SupabaseStorage(user_id)
            for url in urls:
                if storage.delete_scraper(url):
                    print(f"‚úÖ Cache invalid√© pour {url}")
                else:
                    print(f"‚ö†Ô∏è  Pas de cache trouv√© pour {url}")
        else:
            print("‚ö†Ô∏è  --user-id requis pour invalider le cache Supabase")
        return

    # Configurer l'utilisateur global si fourni
    if user_id:
        set_global_user(user_id)

    # D√©terminer le site de r√©f√©rence
    if not reference_url and len(urls) > 0:
        reference_url = urls[0]

    # S'assurer que le site de r√©f√©rence est dans la liste
    all_urls = list(set(urls))
    if reference_url and reference_url not in all_urls:
        all_urls.insert(0, reference_url)

    # S√©parer r√©f√©rence et concurrents
    competitor_urls = [url for url in all_urls if url != reference_url]

    print(f"\n{'='*70}")
    print(f"üöÄ SCRAPER AI v{PROMPT_VERSION} - SCRAPING INTELLIGENT")
    print(f"{'='*70}")
    print(f"‚≠ê Site de r√©f√©rence: {reference_url}")
    print(f"üì¶ Concurrents √† comparer: {len(competitor_urls)}")
    for i, url in enumerate(competitor_urls, 1):
        print(f"   {i}. {url}")
    print(f"üë§ User ID: {user_id or 'Non connect√© (local)'}")
    print(f"üìÇ Cat√©gories: {categories}")
    print(f"üé® Ignorer couleurs: {'Oui' if ignore_colors else 'Non'}")
    print(
        f"üì¶ Inventaire seulement: {'Oui (exclut catalogue/showroom)' if inventory_only else 'Non (inventaire + catalogue)'}")
    print(f"{'='*70}\n")

    start_time = time.time()

    all_sites = [reference_url] + \
        competitor_urls if reference_url else competitor_urls

    # =====================================================
    # PHASE 1: V√âRIFICATION DU CACHE
    # =====================================================
    print(f"\n{'='*50}")
    print(f"üì¶ PHASE 1: V√âRIFICATION DU CACHE")
    print(f"{'='*50}")

    storage = SupabaseStorage(user_id)
    sites_with_cache = []
    sites_without_cache = []

    for url in all_sites:
        is_valid, cached_data = storage.is_cache_valid(url)
        if is_valid and cached_data and not force_refresh:
            sites_with_cache.append(url)
            print(f"   ‚úÖ {url[:50]}... ‚Üí CACHE VALIDE")
        else:
            sites_without_cache.append(url)
            status = "FORCE REFRESH" if force_refresh else (
                "EXPIR√â" if cached_data else "NOUVEAU")
            print(f"   üÜï {url[:50]}... ‚Üí {status}")

    print(
        f"\n   üìä R√©sum√©: {len(sites_with_cache)} en cache, {len(sites_without_cache)} √† cr√©er")

    # =====================================================
    # PHASE 2: CR√âATION DES SCRAPERS (S√âQUENTIEL)
    # =====================================================
    if sites_without_cache:
        print(f"\n{'='*50}")
        print(f"üîß PHASE 2: CR√âATION DES SCRAPERS (s√©quentiel)")
        print(f"{'='*50}")
        print(
            f"   ‚è±Ô∏è  Estimation: ~{len(sites_without_cache) * 45}s ({len(sites_without_cache)} sites √ó ~45s)")
        print(f"   üí° Traitement un par un pour √©viter les limites API\n")

        failed_sites: list = []  # Sites dont le scraper n'a rien extrait

        for i, url in enumerate(sites_without_cache, 1):
            print(
                f"\n   [{i}/{len(sites_without_cache)}] üîÑ Cr√©ation du scraper pour {url[:50]}...")
            try:
                scraper = IntelligentScraper(user_id=user_id)
                # Appel avec force_refresh=True pour forcer la cr√©ation
                result = scraper.scrape(
                    url, force_refresh=True, categories=categories, inventory_only=inventory_only)
                product_count = len(result.get('products', []))
                if product_count == 0:
                    print(
                        f"   [{i}/{len(sites_without_cache)}] ‚ö†Ô∏è  Scraper cr√©√© mais 0 produits - sera re-tent√© en phase 3")
                    failed_sites.append(url)
                else:
                    print(
                        f"   [{i}/{len(sites_without_cache)}] ‚úÖ Scraper cr√©√©: {product_count} produits extraits")
            except Exception as e:
                print(f"   [{i}/{len(sites_without_cache)}] ‚ùå Erreur: {e}")
                failed_sites.append(url)

            # Petite pause entre chaque site pour √©viter le rate limiting
            if i < len(sites_without_cache):
                print(f"   ‚è≥ Pause de 2s avant le prochain site...")
                time.sleep(2)

    # =====================================================
    # PHASE 3: EXTRACTION (PARALL√àLE)
    # =====================================================
    print(f"\n{'='*50}")
    print(f"‚ö° PHASE 3: EXTRACTION DES DONN√âES (parall√®le)")
    print(f"{'='*50}")
    print(f"   üöÄ Extraction parall√®le de {len(all_sites)} sites...\n")

    results: Dict[str, dict] = {}

    with ThreadPoolExecutor(max_workers=min(len(all_sites), 10)) as pool:
        futures = {}
        for url in all_sites:
            future = pool.submit(
                scrape_site_wrapper,
                # force_refresh=False car scrapers d√©j√† cr√©√©s
                (url, user_id, False, categories, inventory_only)
            )
            futures[future] = url

        for future in as_completed(futures):
            url = futures[future]
            try:
                result_url, result_data = future.result()
                results[result_url] = result_data
                product_count = len(result_data.get('products', []))
                is_ref = " ‚≠ê" if url == reference_url else ""
                print(f"   ‚úÖ {url[:40]}...: {product_count} produits{is_ref}")
            except Exception as e:
                print(f"   ‚ùå {url[:40]}...: Erreur - {e}")
                results[url] = {"companyInfo": {}, "products": []}

    # =====================================================
    # PHASE 3b: RETRY DES SITES AVEC 0 PRODUITS
    # =====================================================
    # Identifier TOUS les sites avec 0 produits (pas juste ceux en phase 2)
    sites_with_zero_products = [
        url for url in all_sites
        if len(results.get(url, {}).get('products', [])) == 0
    ]

    if sites_with_zero_products:
        print(f"\n{'='*50}")
        print(
            f"üîÑ PHASE 3b: RETRY DES SITES SANS PRODUITS ({len(sites_with_zero_products)} sites)")
        print(f"{'='*50}")
        print(f"   ‚è≥ Nouvelle tentative avec force_refresh=True...\n")

        for url in sites_with_zero_products:
            is_ref = " ‚≠ê" if url == reference_url else ""
            print(f"   üîÑ Retry: {url[:50]}...{is_ref}")
            try:
                scraper = IntelligentScraper(user_id=user_id)
                retry_result = scraper.scrape(
                    url, force_refresh=True, categories=categories, inventory_only=inventory_only)
                retry_count = len(retry_result.get('products', []))
                if retry_count > 0:
                    results[url] = retry_result
                    print(f"   ‚úÖ Retry r√©ussi: {retry_count} produits{is_ref}")
                else:
                    print(
                        f"   ‚ö†Ô∏è  Retry: toujours 0 produits pour {url[:50]}...{is_ref}")
            except Exception as e:
                print(f"   ‚ùå Retry √©chou√©: {e}")

            # Pause entre retries pour √©viter le rate limiting
            if url != sites_with_zero_products[-1]:
                time.sleep(2)

    elapsed_time = time.time() - start_time
    print(f"\n‚è±Ô∏è  Scraping termin√© en {elapsed_time:.1f}s")

    # R√©cup√©rer les produits de r√©f√©rence
    reference_products = results.get(reference_url, {}).get('products', [])

    if not reference_products:
        print(f"\n{'='*60}")
        print(f"‚ö†Ô∏è  ATTENTION: Aucun produit trouv√© sur le site de r√©f√©rence!")
        print(f"{'='*60}")
        print(f"üåê Site: {reference_url}")
        print(f"\nüí° Causes possibles:")
        print(
            f"   1. Erreur DNS ou r√©seau temporaire (le site √©tait peut-√™tre inaccessible)")
        print(f"   2. Le site n√©cessite JavaScript (Selenium)")
        print(f"   3. Les s√©lecteurs CSS d√©tect√©s sont incorrects")
        print(f"   4. La structure du site a chang√©")
        print(f"\nüîß Solutions:")
        print(f"   - Relancez le scraping (les erreurs r√©seau sont souvent transitoires)")
        print(f"   - Utilisez '--force-refresh' pour r√©g√©n√©rer le scraper")
        print(f"   - V√©rifiez manuellement si le site affiche des produits")
        print(f"{'='*60}\n")

    # Si seulement le site de r√©f√©rence est fourni, extraire ses produits directement
    # Sinon, comparer chaque concurrent avec la r√©f√©rence
    all_matched_products = []

    if not competitor_urls:
        # Pas de concurrents : extraire tous les produits du site de r√©f√©rence
        print(f"\n{'='*60}")
        print(f"üì¶ EXTRACTION DU SITE DE R√âF√âRENCE")
        print(f"{'='*60}")
        print(f"‚úÖ {len(reference_products)} produits extraits du site de r√©f√©rence")
        all_matched_products = reference_products
    else:
        # Des concurrents sont fournis : comparer avec la r√©f√©rence
        print(f"\n{'='*60}")
        print(f"üîç COMPARAISON AVEC LES CONCURRENTS")
        print(f"{'='*60}")

        for url in competitor_urls:
            result = results.get(url, {})
            competitor_products = result.get('products', [])

            if competitor_products and reference_products:
                matched = find_matching_products(
                    reference_products=reference_products,
                    comparison_products=competitor_products,
                    reference_url=reference_url,
                    comparison_url=url,
                    ignore_colors=ignore_colors
                )
                all_matched_products.extend(matched)

    # Sauvegarder les produits
    # IMPORTANT: Inclure TOUS les produits (r√©f√©rence + TOUS les concurrents, match√©s ou non)
    # pour que le dashboard puisse afficher les produits m√™me sans correspondance

    # Marquer les produits de r√©f√©rence avec leur source
    # FORCER sourceSite (pas conditionnel) pour √©viter tout m√©lange de donn√©es
    for product in reference_products:
        product['sourceSite'] = reference_url
        product['isReferenceProduct'] = True

    # Combiner: produits de r√©f√©rence + TOUS les produits des concurrents (pas juste match√©s)
    all_products_to_save = []

    # 1. Ajouter tous les produits de r√©f√©rence
    all_products_to_save.extend(reference_products)

    # 2. Ajouter TOUS les produits des concurrents (match√©s ET non-match√©s)
    # √âviter les doublons en v√©rifiant sourceUrl (IGNORER les sourceUrl vides/None)
    reference_source_urls = {p.get('sourceUrl')
                             for p in reference_products if p.get('sourceUrl')}

    # Set pour suivre les URLs d√©j√† ajout√©es (√©viter doublons entre concurrents)
    added_source_urls = set(reference_source_urls)

    # D'abord les produits match√©s (ont d√©j√† prixReference, differencePrix)
    for matched in all_matched_products:
        source_url = matched.get('sourceUrl')
        # Ne d√©dupliquer que si sourceUrl est non-vide
        if source_url and source_url in added_source_urls:
            continue
        # FORCER sourceSite si manquant
        if not matched.get('sourceSite'):
            try:
                from urllib.parse import urlparse
                parsed = urlparse(source_url or '')
                if parsed.netloc:
                    matched['sourceSite'] = f"{parsed.scheme}://{parsed.netloc}"
            except:
                pass
        all_products_to_save.append(matched)
        if source_url:
            added_source_urls.add(source_url)

    # Ensuite TOUS les autres produits des concurrents (non-match√©s)
    for competitor_url in competitor_urls:
        result = results.get(competitor_url, {})
        competitor_products = result.get('products', [])

        for product in competitor_products:
            source_url = product.get('sourceUrl')
            # Ne d√©dupliquer que si sourceUrl est non-vide
            if source_url and source_url in added_source_urls:
                continue
            # FORCER sourceSite pour les produits concurrents
            if not product.get('sourceSite'):
                product['sourceSite'] = competitor_url
            all_products_to_save.append(product)
            if source_url:
                added_source_urls.add(source_url)

    # V√©rification: log la r√©partition par site
    site_counts = {}
    for p in all_products_to_save:
        site = p.get('sourceSite', 'unknown')
        site_counts[site] = site_counts.get(site, 0) + 1
    print(f"\nüìä R√âPARTITION PAR SITE (avant sauvegarde):")
    for site, count in sorted(site_counts.items(), key=lambda x: -x[1]):
        is_ref = " ‚≠ê" if site == reference_url else ""
        print(f"   {site[:50]}: {count} produits{is_ref}")

    final_data = {
        "products": all_products_to_save,
        "metadata": {
            "reference_url": reference_url,
            "reference_products_count": len(reference_products),
            "competitor_urls": competitor_urls,
            "total_matched_products": len(all_matched_products),
            "total_products": len(all_products_to_save),
            "scraping_time_seconds": round(elapsed_time, 1),
            "mode": "reference_only" if not competitor_urls else "comparison",
            "categories": categories,
            "prompt_version": PROMPT_VERSION
        }
    }

    # PRIORIT√â 1: Sauvegarder dans Supabase via l'API (si user_id fourni)
    saved_to_supabase = False
    if user_id:
        try:
            import requests
            api_url = os.environ.get('NEXTJS_API_URL', 'http://localhost:3000')

            scraping_payload = {
                "user_id": user_id,
                "reference_url": reference_url,
                "competitor_urls": competitor_urls,
                # IMPORTANT: inclure TOUS les produits (r√©f√©rence + match√©s)
                "products": all_products_to_save,
                "metadata": final_data["metadata"],
                "scraping_time_seconds": round(elapsed_time, 1),
                "mode": "reference_only" if not competitor_urls else "comparison"
            }

            response = requests.post(
                f"{api_url}/api/scrapings/save",
                json=scraping_payload,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('success') and not result.get('isLocal'):
                    saved_to_supabase = True
                    print(
                        f"‚òÅÔ∏è  Sauvegard√© dans Supabase (ID: {result.get('scraping', {}).get('id', 'N/A')})")
                else:
                    print(
                        f"‚ö†Ô∏è  R√©ponse API: {result.get('message', 'Sauvegarde locale uniquement')}")
            else:
                print(
                    f"‚ö†Ô∏è  Erreur API ({response.status_code}): {response.text[:200]}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur sauvegarde Supabase: {e}")

    # FALLBACK: Sauvegarder localement seulement si Supabase a √©chou√©
    output_file = Path(__file__).parent.parent / "scraped_data.json"
    if not saved_to_supabase:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, indent=2, ensure_ascii=False)
        print(f"üíæ Sauvegard√© localement: {output_file}")

    # R√©sum√©
    print(f"\n{'='*70}")
    print(f"‚úÖ SCRAPING TERMIN√â!")
    print(f"{'='*70}")
    print(f"‚≠ê Site de r√©f√©rence: {reference_url}")
    print(f"üì¶ Produits de r√©f√©rence: {len(reference_products)}")
    if competitor_urls:
        print(f"üîç Produits avec correspondance: {len(all_matched_products)}")
        print(
            f"üì¶ Total produits sauvegard√©s: {len(all_products_to_save)} (r√©f√©rence + match√©s)")
    else:
        print(f"üì¶ Produits extraits: {len(all_products_to_save)}")
    print(f"‚è±Ô∏è  Temps total: {elapsed_time:.1f}s")
    if saved_to_supabase:
        print(f"‚òÅÔ∏è  Donn√©es dans: Supabase Cloud")
    else:
        print(f"üíæ Sauvegard√©: {output_file}")

    # Aper√ßu (afficher tous les produits sauvegard√©s, pas juste les match√©s)
    if all_products_to_save:
        # Statistiques d'√©tat
        etat_counts = {}
        cat_counts = {}
        for p in all_products_to_save:
            etat = p.get('etat', 'inconnu')
            cat = p.get('sourceCategorie', 'inconnu')
            etat_counts[etat] = etat_counts.get(etat, 0) + 1
            cat_counts[cat] = cat_counts.get(cat, 0) + 1

        print(f"\nüìä R√âPARTITION PAR √âTAT:")
        etat_labels = {'neuf': 'üü¢ Neuf', 'occasion': 'üü† Usag√©',
                       'demonstrateur': 'üîµ D√©monstrateur', 'inconnu': '‚ö™ Inconnu'}
        for etat, count in sorted(etat_counts.items(), key=lambda x: -x[1]):
            label = etat_labels.get(etat, etat)
            print(f"   {label}: {count} produits")

        print(f"\nüìÇ R√âPARTITION PAR SOURCE:")
        cat_labels = {'inventaire': 'üì¶ Inventaire', 'catalogue': 'üìñ Catalogue',
                      'vehicules_occasion': 'üîÑ V√©hicules occasion', 'inconnu': '‚ö™ Inconnu'}
        for cat, count in sorted(cat_counts.items(), key=lambda x: -x[1]):
            label = cat_labels.get(cat, cat)
            print(f"   {label}: {count} produits")

        print(f"\nüìã APER√áU (10 premiers):")
        for idx, p in enumerate(all_products_to_save[:10], start=1):
            nom = p.get('name') or f"{p.get('marque', '')} {p.get('modele', '')}".strip(
            ) or p.get('sourceUrl', '')
            prix = p.get('prix', 0) or 0
            diff = p.get('differencePrix')
            site = p.get('sourceSite', '')
            etat = p.get('etat', '')
            src_cat = p.get('sourceCategorie', '')

            # Badge d'√©tat
            etat_badge = {'neuf': '[NEUF]', 'occasion': '[USAG√â]',
                          'demonstrateur': '[D√âMO]'}.get(etat, '')
            cat_badge = {'catalogue': '[CAT]', 'vehicules_occasion': '[OCC]', 'inventaire': '[INV]'}.get(
                src_cat, '')

            # Extraire le domaine du site
            try:
                from urllib.parse import urlparse
                domain = urlparse(site).netloc.replace('www.', '')[:20]
            except:
                domain = site[:20]

            if diff is not None:
                diff_str = f"+{diff:.0f}$" if diff >= 0 else f"{diff:.0f}$"
                print(
                    f"   {idx}. {nom[:30]} | {prix:.0f}$ ({diff_str}) | {domain} {etat_badge} {cat_badge}")
            else:
                print(
                    f"   {idx}. {nom[:30]} | {prix:.0f}$ | {domain} {etat_badge} {cat_badge}")

        if len(all_products_to_save) > 10:
            print(f"   ... et {len(all_products_to_save) - 10} autres")
    else:
        print(f"\n‚ö†Ô∏è  Aucun produit extrait.")


if __name__ == "__main__":
    main()
