"""
Module pour analyser le HTML avec Gemini et g√©n√©rer un scraper sp√©cifique
Le scraper g√©n√©r√© utilise Gemini pour extraire (comme scraper.py) mais avec pagination sp√©cifique au site
"""
import json
import hashlib
import re
import time
import os
from pathlib import Path
from typing import Dict, Optional, List, Set
from urllib.parse import urlparse, urljoin
import requests
from bs4 import BeautifulSoup

try:
    from .config import SCRAPER_GENERATION_SCHEMA, CACHE_DIR, EXTRACTION_SCHEMA, PROMPT_VERSION
    from .gemini_client import GeminiClient
    from .ai_tools import AITools
    from .exploration_agent import ExplorationAgent
    from .site_data_storage import SiteDataStorage
    from .scraper_generator import ScraperGenerator
except ImportError:
    from config import SCRAPER_GENERATION_SCHEMA, CACHE_DIR, EXTRACTION_SCHEMA, PROMPT_VERSION
    from gemini_client import GeminiClient
    from ai_tools import AITools
    from exploration_agent import ExplorationAgent
    from site_data_storage import SiteDataStorage
    from scraper_generator import ScraperGenerator


# Sch√©ma pour la s√©lection de pages √† analyser
PAGE_SELECTION_SCHEMA = {
    "type": "object",
    "properties": {
        "needsMorePages": {
            "type": "boolean",
            "description": "True si des pages suppl√©mentaires sont n√©cessaires pour g√©n√©rer un scraper complet"
        },
        "selectedPages": {
            "type": "array",
            "items": {"type": "string"},
            "description": "Liste des URLs √† analyser en plus (max 5 pages)"
        },
        "reasoning": {
            "type": "string",
            "description": "Explication de pourquoi ces pages sont n√©cessaires"
        }
    },
    "required": ["needsMorePages", "selectedPages", "reasoning"]
}

# PROMPT_VERSION est maintenant d√©fini dans config.py pour √©viter les imports circulaires


class HTMLAnalyzer:
    """Analyse le HTML d'un site et g√©n√®re un scraper sp√©cifique

    Le scraper g√©n√©r√© utilise Gemini pour extraire les produits (comme scraper.py)
    mais avec une logique de pagination sp√©cifique au site analys√©.
    """

    def __init__(self, user_id: Optional[str] = None):
        self.gemini_client = GeminiClient()
        self.cache_dir = Path(CACHE_DIR)
        self.cache_dir.mkdir(exist_ok=True)
        self.user_id = user_id
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.ai_tools = None  # Sera initialis√© avec l'URL de base

    def _get_cache_key(self, url: str) -> str:
        """G√©n√®re une cl√© de cache bas√©e sur l'URL"""
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '')
        return hashlib.md5(domain.encode()).hexdigest()

    def _get_cache_path(self, url: str) -> Path:
        """Retourne le chemin du fichier de cache pour une URL"""
        cache_key = self._get_cache_key(url)
        return self.cache_dir / f"{cache_key}_scraper.py"

    def _load_cached_scraper(self, url: str) -> Optional[Dict]:
        """Charge un scraper depuis le cache (Supabase ou local)

        PRIORIT√â: Supabase si user_id fourni, sinon local.
        Si trouv√© dans Supabase, supprime le fichier local pour √©viter les doublons.
        V√©rifie aussi la version du prompt depuis les commentaires.
        Si la version ne correspond pas, le cache est consid√©r√© comme invalide et sera r√©g√©n√©r√©.
        """
        cache_key = self._get_cache_key(url)

        # PRIORIT√â 1: Essayer Supabase si utilisateur connect√©
        if self.user_id:
            try:
                scraper_data = self._load_from_supabase(cache_key)
                if scraper_data:
                    scraper_code = scraper_data.get('scraper_code', '')
                    metadata = scraper_data.get('metadata', {})

                    # V√©rifier la version du prompt
                    cached_version = metadata.get('prompt_version', '1.0')
                    if cached_version != PROMPT_VERSION:
                        print(
                            f"‚ö†Ô∏è Version du prompt diff√©rente (cache: {cached_version}, actuelle: {PROMPT_VERSION})")
                        print(f"   Le prompt a √©t√© modifi√©, invalidation du cache...")
                        # Supprimer de Supabase
                        self._delete_from_supabase(cache_key)
                        return None

                    # Supprimer le fichier local s'il existe (priorit√© Supabase)
                    cache_path = self._get_cache_path(url)
                    if cache_path.exists():
                        try:
                            cache_path.unlink()
                            print(
                                f"üóëÔ∏è  Fichier local supprim√© (donn√©es dans Supabase)")
                        except Exception as e:
                            print(f"‚ö†Ô∏è  Erreur suppression fichier local: {e}")

                    # Reconstruire le format de donn√©es attendu
                    cached_data = {
                        'scraperCode': scraper_code,
                        'siteAnalysis': {
                            'siteName': metadata.get('site_name', ''),
                            'siteUrl': metadata.get('site_url', url),
                            'structureType': metadata.get('structure_type', 'unknown')
                        },
                        'fieldMappings': {
                            'products': metadata.get('selectors', {})
                        },
                        'metadata': metadata
                    }

                    print(
                        f"‚úÖ Scraper charg√© depuis Supabase (cache_key: {cache_key})")
                    print(f"   Version prompt: {cached_version}")
                    return cached_data
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur Supabase: {e}, fallback sur cache local")

        # PRIORIT√â 2: Fallback sur cache local
        cache_path = self._get_cache_path(url)
        if cache_path.exists():
            try:
                # Lire le fichier Python
                with open(cache_path, 'r', encoding='utf-8') as f:
                    scraper_code = f.read()

                # Extraire les m√©tadonn√©es depuis les commentaires
                metadata = self._extract_metadata_from_code(scraper_code)

                # V√©rifier la version du prompt
                cached_version = metadata.get('prompt_version', '1.0')
                if cached_version != PROMPT_VERSION:
                    print(
                        f"‚ö†Ô∏è Version du prompt diff√©rente (cache: {cached_version}, actuelle: {PROMPT_VERSION})")
                    print(f"   Le prompt a √©t√© modifi√©, invalidation du cache...")
                    cache_path.unlink()  # Supprimer le cache obsol√®te
                    return None

                # Reconstruire le format de donn√©es attendu
                cached_data = {
                    'scraperCode': scraper_code,
                    'siteAnalysis': {
                        'siteName': metadata.get('site_name', ''),
                        'siteUrl': metadata.get('site_url', url),
                        'structureType': metadata.get('structure_type', 'unknown')
                    },
                    'fieldMappings': {
                        'products': metadata.get('selectors', {})
                    },
                    'metadata': metadata
                }

                print(f"‚úÖ Scraper charg√© depuis le cache local: {cache_path}")
                print(f"   Version prompt: {cached_version}")
                return cached_data
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors du chargement du cache local: {e}")
        return None

    def _extract_metadata_from_code(self, code: str) -> Dict:
        """Extrait les m√©tadonn√©es depuis les commentaires du code Python"""
        metadata = {}

        # Chercher les m√©tadonn√©es dans les commentaires/docstring
        # Format attendu: # Version prompt: 3.3
        patterns = {
            'prompt_version': r'Version prompt:\s*([\d.]+)',
            'cache_key': r'Cache key:\s*([a-f0-9]+)',
            'site_url': r'Site URL:\s*(https?://[^\s]+)',
            'site_name': r'Site name:\s*([^\n]+)',
            'structure_type': r'Structure type:\s*([^\n]+)',
            'generation_date': r'Date g√©n√©ration:\s*([^\n]+)',
            'urls_count': r'URLs d√©couvertes:\s*(\d+)',
            'selectors_count': r'S√©lecteurs d√©tect√©s:\s*(\d+)'
        }

        for key, pattern in patterns.items():
            match = re.search(pattern, code, re.IGNORECASE)
            if match:
                value = match.group(1).strip()
                # Convertir les nombres
                if key in ['urls_count', 'selectors_count']:
                    try:
                        metadata[key] = int(value)
                    except ValueError:
                        pass
                else:
                    metadata[key] = value

        # Extraire les s√©lecteurs depuis le code (SELECTORS = {...})
        # Chercher SELECTORS = { ... } avec support multi-lignes
        selectors_match = re.search(
            r'SELECTORS\s*=\s*(\{.*?\})', code, re.DOTALL)
        if selectors_match:
            try:
                # √âvaluer le dictionnaire de s√©lecteurs en Python
                selectors_str = selectors_match.group(1)
                # Utiliser eval() avec un contexte s√©curis√© (seulement pour les s√©lecteurs)
                # Les s√©lecteurs sont des cha√Ænes simples, donc relativement s√ªr
                metadata['selectors'] = eval(
                    selectors_str, {"__builtins__": {}})
            except:
                # Fallback: essayer avec json si possible
                try:
                    selectors_str = selectors_match.group(1).replace("'", '"')
                    metadata['selectors'] = json.loads(selectors_str)
                except:
                    pass

        return metadata

    def _save_scraper_to_cache(self, url: str, scraper_data: Dict) -> str:
        """Sauvegarde un scraper dans le cache (fichier Python) et retourne le chemin du fichier"""
        cache_path = self._get_cache_path(url)
        try:
            # Le code Python contient d√©j√† les m√©tadonn√©es en commentaires
            scraper_code = scraper_data.get('scraperCode', '')
            if not scraper_code:
                raise ValueError("Le scraper_data doit contenir 'scraperCode'")

            # Sauvegarder directement le code Python
            with open(cache_path, 'w', encoding='utf-8') as f:
                f.write(scraper_code)
            return cache_path
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la sauvegarde du cache: {e}")
            return ""

    def _save_to_supabase(self, site_url: str, cache_key: str, scraper_code: str, metadata: Dict) -> Optional[str]:
        """Sauvegarde un scraper dans Supabase via l'API"""
        try:
            api_url = os.environ.get('NEXTJS_API_URL', 'http://localhost:3000')
            save_url = f"{api_url}/api/scraper-ai/cache/save"

            response = requests.post(
                save_url,
                json={
                    "user_id": self.user_id,
                    "site_url": site_url,
                    "cache_key": cache_key,
                    "scraper_code": scraper_code,
                    "metadata": metadata
                },
                timeout=10
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('success'):
                    return result.get('cache_key', cache_key)
                else:
                    raise Exception(
                        f"Supabase API error: {result.get('error', 'Unknown error')}")
            else:
                raise Exception(f"Supabase API error: {response.status_code}")

        except requests.exceptions.Timeout:
            raise Exception("Timeout: Supabase ne r√©pond pas")
        except requests.exceptions.ConnectionError:
            raise Exception("Connexion impossible: Supabase inaccessible")
        except requests.exceptions.RequestException as e:
            raise Exception(f"Erreur r√©seau: {e}")

    def _load_from_supabase(self, cache_key: str) -> Optional[Dict]:
        """Charge un scraper depuis Supabase via l'API"""
        try:
            api_url = os.environ.get('NEXTJS_API_URL', 'http://localhost:3000')
            load_url = f"{api_url}/api/scraper-ai/cache/load"

            response = requests.get(
                load_url,
                params={
                    "user_id": self.user_id,
                    "cache_key": cache_key
                },
                timeout=10
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('found'):
                    return {
                        'scraper_code': result.get('scraper_code', ''),
                        'metadata': result.get('metadata', {})
                    }
            elif response.status_code == 404:
                return None
            else:
                raise Exception(f"Supabase API error: {response.status_code}")

        except requests.exceptions.Timeout:
            raise Exception("Timeout: Supabase ne r√©pond pas")
        except requests.exceptions.ConnectionError:
            raise Exception("Connexion impossible: Supabase inaccessible")
        except requests.exceptions.RequestException as e:
            raise Exception(f"Erreur r√©seau: {e}")

    def _delete_from_supabase(self, cache_key: str) -> bool:
        """Supprime un scraper de Supabase via l'API"""
        try:
            api_url = os.environ.get('NEXTJS_API_URL', 'http://localhost:3000')
            delete_url = f"{api_url}/api/scraper-ai/cache"

            response = requests.delete(
                delete_url,
                params={
                    "user_id": self.user_id,
                    "cache_key": cache_key
                },
                timeout=10
            )

            return response.status_code == 200
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur suppression Supabase: {e}")
            return False

    def _enforce_local_cache_limit(self):
        """Applique la limite de 10 scrapers locaux pour utilisateurs non connect√©s"""
        try:
            # Lister tous les fichiers .py dans le cache
            cache_files = list(self.cache_dir.glob("*_scraper.py"))

            if len(cache_files) >= 10:
                # Trier par date de modification (plus ancien en premier)
                cache_files.sort(key=lambda f: f.stat().st_mtime)

                # Supprimer le plus ancien
                oldest_file = cache_files[0]
                oldest_file.unlink()
                print(
                    f"üóëÔ∏è  Scraper local supprim√© (limite 10 atteinte): {oldest_file.name}")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de l'application de la limite: {e}")

    def _fetch_html(self, url: str, max_retries: int = 3) -> str:
        """R√©cup√®re le contenu HTML d'une URL avec retry pour erreurs transitoires.

        Args:
            url: URL √† r√©cup√©rer
            max_retries: Nombre maximum de tentatives (d√©faut: 3)
        """
        import time as _time
        last_error = None

        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                return response.text
            except Exception as e:
                last_error = e
                error_str = str(e).lower()
                is_transient = any(kw in error_str for kw in [
                    'nameresolution', 'name resolution', 'nodename nor servname',
                    'timeout', 'timed out', 'connectionerror', 'connection refused',
                    'connectionreset', 'remotedisconnected', 'max retries exceeded',
                    'newconnectionerror', '502', '503', '504',
                ])
                if attempt < max_retries - 1 and is_transient:
                    wait_time = 2 ** attempt * 2  # 2s, 4s, 8s
                    print(
                        f"‚ö†Ô∏è Tentative {attempt + 1}/{max_retries} √©chou√©e pour {url}: {e}")
                    print(f"   üîÑ Retry dans {wait_time}s...")
                    _time.sleep(wait_time)
                else:
                    break

        print(f"‚ö†Ô∏è Erreur lors de la r√©cup√©ration de {url}: {last_error}")
        return ""

    def _extract_links(self, html_content: str, base_url: str) -> List[str]:
        """Extrait tous les liens d'une page HTML"""
        soup = BeautifulSoup(html_content, 'html.parser')
        parsed_base = urlparse(base_url)
        base_domain = parsed_base.netloc.replace('www.', '')

        links = set()
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            if not href or href.startswith('#') or href.startswith('javascript:'):
                continue

            full_url = urljoin(base_url, href)
            parsed = urlparse(full_url)
            link_domain = parsed.netloc.replace('www.', '')
            if link_domain == base_domain:
                clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                if parsed.query:
                    clean_url += f"?{parsed.query}"
                links.add(clean_url)

        return sorted(list(links))

    def _ask_gemini_which_pages_to_analyze(self, url: str, html_content: str,
                                           available_links: List[str]) -> Dict:
        """Demande √† Gemini quelles pages suppl√©mentaires analyser"""
        links_to_show = available_links[:100]
        links_str = "\n".join([f"- {link}" for link in links_to_show])

        prompt = f"""Tu es un expert en scraping web. Analyse la page d'accueil d'un site de vente de v√©hicules motoris√©s et d√©cide si tu as besoin de voir d'autres pages.

URL DE BASE: {url}

CONTENU HTML DE LA PAGE D'ACCUEIL (extrait):
{html_content[:30000]}

LIENS DISPONIBLES:
{links_str}

QUESTION: As-tu besoin d'autres pages pour comprendre la structure compl√®te? (max 5 pages)

S√©lectionne des pages de:
- Listing de produits (inventaire, catalogue)
- D√©tail d'un produit
- Contact (pour infos entreprise)
"""

        try:
            result = self.gemini_client.call(
                prompt=prompt,
                schema=PAGE_SELECTION_SCHEMA,
                show_prompt=True
            )
            return result
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la s√©lection de pages: {e}")
            return {"needsMorePages": False, "selectedPages": [], "reasoning": "Erreur"}

    def _generate_scraper_with_context(self, url: str, pages_content: Dict[str, str]) -> Dict:
        """G√©n√®re le scraper avec tout le contexte des pages analys√©es

        Le scraper g√©n√©r√© utilise Gemini pour extraire (comme scraper.py)
        mais avec une logique de pagination sp√©cifique au site.
        """
        schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)

        pages_context = ""
        for page_url, html_content in pages_content.items():
            truncated = html_content[:20000]
            pages_context += f"\n\n{'='*40}\nPAGE: {page_url}\n{'='*40}\n{truncated}"

        prompt = f"""Tu es un expert en scraping web. G√©n√®re un scraper Python pour ce site de vente de v√©hicules motoris√©s.

URL DE BASE: {url}

PAGES ANALYS√âES:
{pages_context}

SCH√âMA JSON √Ä RESPECTER:
{schema_str}

APPROCHE REQUISE:
Le scraper DOIT utiliser l'extraction locale avec BeautifulSoup (SANS Gemini), en utilisant les URLs d√©j√† d√©couvertes et d√©dupliqu√©es par l'AI Agent.

‚ö†Ô∏è CRITIQUE - SCRIPT D'EXTRACTION PUR ET EXPLICITE (0% AMBIVALENT):
Le script g√©n√©r√© DOIT √™tre un script d'extraction Python PUR, pas une explication de ce que l'AI Agent a fait.
- Les URLs sont D√âJ√Ä d√©couvertes et d√©dupliqu√©es par l'AI Agent (une URL par mod√®le+ann√©e, couleurs ignor√©es)
- Le script g√©n√©r√© doit utiliser ces URLs comme outils pour savoir exactement o√π aller chercher les donn√©es
- Chaque √©tape d'extraction doit √™tre 100% EXPLICITE et sans ambigu√Øt√©:
  * Quelle m√©thode utiliser pour r√©cup√©rer le HTML (get/browser_get/smart_get) et POURQUOI
  * Quelle strat√©gie d'extraction utiliser (JSON-LD ‚Üí fieldMappings ‚Üí patterns g√©n√©riques) et dans quel ordre
  * Comment extraire chaque champ (s√©lecteur CSS exact, code d'extraction d√©taill√©)
- Le script ne doit PAS expliquer comment les URLs ont √©t√© d√©couvertes, mais doit √™tre explicite sur comment les utiliser pour l'extraction

‚ö†Ô∏è CRITIQUE - HARDCODER LES DONN√âES DANS LE SCRIPT:
Le script g√©n√©r√© DOIT contenir les URLs et s√©lecteurs HARDCOD√âS directement dans le code Python.
- NE PAS utiliser exploration_result.get() au runtime - les URLs doivent √™tre dans une liste Python hardcod√©e
- NE PAS utiliser field_mappings au runtime - les s√©lecteurs doivent √™tre dans un dictionnaire Python hardcod√©
- Format OBLIGATOIRE:
  ```python
  # URLs hardcod√©es (d√©j√† d√©couvertes par l'AI Agent)
  PRODUCT_URLS = [
      "https://site.com/product1",
      "https://site.com/product2",
      # ... toutes les URLs de exploration_result['all_product_urls']
  ]
  
  # S√©lecteurs hardcod√©s (d√©tect√©s par l'AI Agent)
  SELECTORS = {{
      'name': 'h1.product-title',
      'prix': '.price',
      'image': 'img.product-image::attr(src)',
      # ... tous les s√©lecteurs de field_mappings['products']
  }}
  ```
- Le script doit √™tre COMPL√àTEMENT AUTONOME - pas besoin de exploration_result ou field_mappings au runtime

STRUCTURE DU SCRAPER √Ä G√âN√âRER:

```python
def scrape(base_url):
    \"\"\"
    Scraper g√©n√©r√© pour {url}
    Utilise Gemini pour extraire les produits (comme scraper.py)
    
    IMPORTANT: gemini_client et session sont d√©j√† disponibles dans le namespace global.
    NE PAS les passer en param√®tres, les utiliser directement.
    \"\"\"
    # gemini_client et session sont d√©j√† disponibles globalement
    # Utiliser directement: gemini_client.call(prompt, EXTRACTION_SCHEMA)
    
    # √âTAPE 1: UTILISER LES URLs PR√â-D√âCOUVERTES PAR L'AI AGENT
    # ‚ö†Ô∏è CRITIQUE: Les URLs ont D√âJ√Ä √©t√© d√©couvertes par l'AI Agent et sont d√©dupliqu√©es
    # NE PAS red√©couvrir les URLs - utiliser directement exploration_result['all_product_urls']
    
    print(f"\\n{{'='*60}}")
    print(f"üìç √âTAPE 1: UTILISATION DES URLs PR√â-D√âCOUVERTES")
    print(f"{{'='*60}}")
    
    # R√©cup√©rer les URLs d√©j√† d√©couvertes par l'AI Agent
    all_product_urls = exploration_result.get('all_product_urls', [])
    
    if not all_product_urls:
        print("‚ùå Aucune URL de produit pr√©-d√©couverte par l'AI Agent")
        return {{'companyInfo': {{}}, 'products': []}}
    
    print(f"‚úÖ {{len(all_product_urls)}} URLs de produits pr√©-d√©couvertes (d√©j√† d√©dupliqu√©es)")
    print(f"   Exemples: {{all_product_urls[:3]}}")
    response = session.get(base_url)
    soup = BeautifulSoup(response.text, 'html.parser')
    html_content = str(soup)
    
    # Extraire tous les liens de la page
    all_links = []
    for a_tag in soup.find_all('a', href=True):
        href = a_tag['href']
        if href and not href.startswith('#') and not href.startswith('javascript:'):
            full_url = urljoin(base_url, href)
            all_links.append(full_url)
    
    print(f"   ‚úÖ {{len(all_links)}} liens trouv√©s")
    
    # √âTAPE 2: FILTRER les URLs pour ne garder que les pages de produits
    # Mots-cl√©s de pages de produits: inventory, inventaire, products, moto, vehicle, listing, stock, catalog, catalogue, vehicule, quad, atv, motoneige, etc.
    # Mots-cl√©s √† EXCLURE: contact, about, policy, privacy, terms, blog, news, service, appointment, financing, home, index, login, register, account, cart, checkout, wishlist, search, faq
    product_keywords = ['inventory', 'inventaire', 'products', 'product', 'moto', 'vehicle', 'listing', 'stock', 'shop', 'category',
        'catalog', 'catalogue', 'vehicule', 'quad', 'atv', 'motoneige', 'snowmobile', 'scooter', 'marine', 'moto-marine', 'side-by-side', 'sxs']
    exclude_keywords = ['contact', 'about', 'policy', 'privacy', 'terms', 'blog', 'news', 'service', 'appointment',
        'financing', 'home', 'index', 'login', 'register', 'account', 'cart', 'checkout', 'wishlist', 'search', 'faq']
    
    product_pages = []
    for link in all_links:
        link_lower = link.lower()
        # Exclure si contient un mot-cl√© d'exclusion
        if any(keyword in link_lower for keyword in exclude_keywords):
            continue
        # Inclure si contient un mot-cl√© de produit OU si c'est la page d'accueil
        if any(keyword in link_lower for keyword in product_keywords) or link == base_url:
            product_pages.append(link)
    
    print(
        f"   ‚úÖ {{len(product_pages)}} pages de produits identifi√©es (sur {{len(all_links)}} liens)")
    
    if not product_pages:
        print(f"   ‚ö†Ô∏è Aucune page de produits trouv√©e, utilisation de la page d'accueil")
        product_pages = [base_url]
    
    # √âTAPE 3: D√âTECTER LE PATTERN DE PAGINATION
    # Analyser les liens de pagination dans le HTML
    # Chercher: ?page=2, /page/2/, ?p=2, etc.
    # Chercher le bouton "Suivant" / "Next" et son href
    
    pagination_pattern = None
    pagination_type = None
    
    # Exemples de d√©tection:
    pagination_links = soup.find_all('a', href=re.compile(r'page|p=\\d+', re.I))
    if pagination_links:
        # Analyser le premier lien pour trouver le pattern
        first_link = pagination_links[0].get('href', '')
        # Extraire le pattern: ?page=, /page/, ?p=, etc.
    
    # √âTAPE 4: BOUCLE DE PAGINATION (max 100 pages)
    all_product_pages = list(set(product_pages))  # D√©dupliquer
    visited_urls = set(all_product_pages)
    page = 1
    max_pages = 100
    
    while page <= max_pages:
        # Construire l'URL de la page selon le pattern d√©tect√©
        # Exemples selon le site:
        # - page_url = f"{{base_url}}?page={{page}}"
        # - page_url = f"{{base_url}}/page/{{page}}/"
        # - page_url = f"{{base_url}}/inventaire?p={{page}}"
        # - page_url = urljoin(base_url, f"/inventaire/page/{{page}}/")
        
        # IMPORTANT: Adapter selon le pattern r√©el d√©tect√© dans le HTML
        # Si aucun pattern d√©tect√©, essayer les patterns communs
        
        # √âviter les doublons
        if page_url in visited_urls:
            break
        
        visited_urls.add(page_url)
        
        # R√©cup√©rer la page
        try:
            page_response = session.get(page_url, timeout=30)
            page_response.raise_for_status()
            page_soup = BeautifulSoup(page_response.text, 'html.parser')
            
            # V√©rifier s'il y a des produits sur cette page
            # Si pas de produits ou page vide, arr√™ter
            # Exemple: if not page_soup.find_all(class_='product'): break
            
            all_product_pages.append(page_url)
            print(f"   Page {{page}} trouv√©e: {{page_url}}")
            
            # V√©rifier s'il y a une page suivante
            # Chercher le bouton "Suivant" ou le lien de la page suivante
            # Si non trouv√©, break
            
            page += 1
        except Exception as e:
            print(f"   Erreur page {{page}}: {{e}}")
            break
    
    print(f"   ‚úÖ Total pages de produits trouv√©es: {{len(all_product_pages)}}")
    
    # 3. R√âCUP√âRER LE HTML DE TOUTES LES PAGES
    pages_data = []
    for page_url in all_product_pages:
        response = session.get(page_url)
        soup = BeautifulSoup(response.text, 'html.parser')
        pages_data.append({{
            'url': page_url,
            'html': str(soup),
            'text': soup.get_text()
        }})
        print(f"HTML r√©cup√©r√©: {{page_url}} ({{len(str(soup))}} caract√®res)")
    
    # 4. UTILISER GEMINI POUR EXTRAIRE (comme scraper.py)
    # Pr√©parer le prompt avec tout le HTML
    pages_html = ""
    for i, page_data in enumerate(pages_data, 1):
        pages_html += f"\\n{{'‚îÄ'*60}}\\n"
        pages_html += f"PAGE {{i}}: {{page_data['url']}}\\n"
        pages_html += f"{{'‚îÄ'*60}}\\n"
        pages_html += f"{{page_data['html']}}\\n\\n"
    
    prompt = f\"\"\"Tu es un expert en extraction de donn√©es. Extrais TOUS les v√©hicules motoris√©s depuis ces pages HTML.

HTML COMPLET DES PAGES:
{{pages_html}}

Extrais UNIQUEMENT les V√âHICULES INDIVIDUELS avec marque et mod√®le sp√©cifiques.
Ignore les cat√©gories, les liens de navigation, les pages d'information.

Pour chaque v√©hicule, extrais:
- name, description, category, marque, modele, prix, disponibilite, image, annee, kilometrage, cylindree
- sourceUrl: URL de la page o√π le produit a √©t√© trouv√©
- sourceSite: base_url
- sourceCategorie: "inventaire", "catalogue", ou "vehicules_occasion"
\"\"\"
    
    # Appeler Gemini avec le sch√©ma (gemini_client est d√©j√† dans le namespace)
    result = gemini_client.call(
        prompt=prompt,
        schema=EXTRACTION_SCHEMA,
        show_prompt=False
    )
    
    products_count = len(result.get('products', []))
    print(f"‚úÖ Gemini a extrait {{products_count}} produits")
    
    # 5. Retourner le r√©sultat
    return {{
        'companyInfo': result.get('companyInfo', {{}}),
        'products': result.get('products', [])
    }}
```

CRITIQUE - PAGINATION:
1. ANALYSE le HTML fourni pour identifier le pattern de pagination exact
2. IMPL√âMENTE une boucle qui trouve TOUTES les pages
3. UTILISE Gemini pour extraire (comme scraper.py) - ne pas essayer d'extraire manuellement avec BeautifulSoup
4. LOGS: Affiche chaque page visit√©e et le nombre de produits trouv√©s

IMPORTANT - SIGNATURE DE LA FONCTION:
La fonction scrape() DOIT avoir UN SEUL param√®tre: base_url
NE PAS ajouter gemini_client ou session comme param√®tres - ils sont d√©j√† disponibles globalement.

Exemple correct:
```python
def scrape(base_url):
    # gemini_client et session sont disponibles globalement
    # Utiliser directement sans les passer en param√®tres
    response = session.get(base_url)
    result = gemini_client.call(prompt, EXTRACTION_SCHEMA)
```

Exemple INCORRECT (ne pas faire):
```python
def scrape(base_url, gemini_client, session):  # ‚ùå NE PAS FAIRE
```

G√âN√àRE LE CODE COMPLET ET FONCTIONNEL avec la bonne signature.
"""

        try:
            result = self.gemini_client.call(
                prompt=prompt,
                schema=SCRAPER_GENERATION_SCHEMA,
                show_prompt=True
            )
            return result
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration du scraper: {e}")
            raise

    def analyze_and_generate_scraper(self, url: str, html_content: str,
                                     force_refresh: bool = False) -> Dict:
        r"""Analyse le HTML et g√©n√®re un scraper sp√©cifique pour le site

        Nouveau flux en 4 √©tapes :
        1. ExplorationAgent : D√©couvre URLs et extrait infos via Gemini
        2. SiteDataStorage : Sauvegarde donn√©es structur√©es
        3. ScraperGenerator : G√©n√®re script depuis template (sans Gemini)
        4. Retourne r√©sultat pour ex√©cution
        """
        # V√©rifier le cache du scraper (format ancien ou nouveau)
        if not force_refresh:
            cached = self._load_cached_scraper(url)
            if cached:
                print(f"\nüíæ Scraper charg√© depuis le cache")
                print(
                    f"   ‚úÖ Le scraper en cache sera utilis√© pour acc√©l√©rer l'extraction")
                print(
                    f"   üìù Code scraper pr√©sent: {len(cached.get('scraperCode', ''))} caract√®res")
                return cached

        # V√©rifier si les donn√©es d'exploration existent dans le cache
        storage = SiteDataStorage()
        cached_site_data = storage.load_site_data(url)

        if cached_site_data and not force_refresh:
            print(f"\n{'='*60}")
            print(f"‚úÖ DONN√âES D'EXPLORATION TROUV√âES DANS LE CACHE")
            print(f"{'='*60}")
            print(f"üåê URL: {url}")
            print(
                f"üìÖ Date d'exploration: {cached_site_data.get('exploration_date', 'N/A')}")
            print(
                f"üìã URLs de produits: {len(cached_site_data.get('product_urls', []))}")
            print(
                f"üéØ S√©lecteurs d√©tect√©s: {len(cached_site_data.get('detected_selectors', {}))}")
            print(
                f"\n   ‚ö° R√©utilisation des donn√©es existantes (pas de re-exploration Gemini)")
            print(f"   üîß G√©n√©ration du scraper depuis les donn√©es en cache...\n")

            # Convertir les donn√©es au format attendu par ScraperGenerator
            site_data = {
                'site_url': cached_site_data.get('site_url', url),
                'product_urls': cached_site_data.get('product_urls', []),
                'detected_selectors': cached_site_data.get('detected_selectors', {}),
                'site_structure': cached_site_data.get('site_structure', {}),
                'metadata': cached_site_data.get('metadata', {})
            }
        else:
            # Les donn√©es n'existent pas, faire l'exploration compl√®te
            print(f"\n{'='*60}")
            print(f"üîç NOUVEAU FLUX : 4 √âTAPES DISTINCTES")
            print(f"{'='*60}")
            print(f"üåê URL: {url}")
            print(f"üìÑ Taille HTML: {len(html_content)} caract√®res\n")

            # √âTAPE 1 : ExplorationAgent (Gemini)
            print(f"\nüìç √âTAPE 1 : EXPLORATION ET EXTRACTION GEMINI")
            exploration_agent = ExplorationAgent()
            exploration_data = exploration_agent.explore_and_extract(
                url, html_content)

            # √âTAPE 2 : SiteDataStorage (Sauvegarde structur√©e)
            print(f"\nüíæ √âTAPE 2 : STOCKAGE STRUCTUR√â")
            site_data = {
                'site_url': url,
                'product_urls': exploration_data['product_urls'],
                'html_samples': exploration_data['html_samples'],
                'extracted_products': exploration_data['extracted_products'],
                'detected_selectors': exploration_data['detected_selectors'],
                'site_structure': exploration_data['site_structure'],
                'metadata': {
                    'data_version': '1.0',
                    'exploration_date': exploration_data.get('exploration_date')
                }
            }
            storage.save_site_data(
                url=url,
                product_urls=site_data['product_urls'],
                html_samples=site_data['html_samples'],
                extracted_products=site_data['extracted_products'],
                detected_selectors=site_data['detected_selectors'],
                site_structure=site_data['site_structure'],
                metadata=site_data['metadata']
            )

        # √âTAPE 3 : ScraperGenerator (Template, sans Gemini)
        print(f"\nüîß √âTAPE 3 : G√âN√âRATION DU SCRAPER (SANS GEMINI)")
        generator = ScraperGenerator()
        scraper_code = generator.generate_scraper(site_data)

        # Construire le r√©sultat au format attendu
        site_structure = site_data.get('site_structure', {})
        result = {
            'scraperCode': scraper_code,
            'siteAnalysis': {
                'siteName': site_structure.get('domain', ''),
                'siteUrl': url,
                'structureType': site_structure.get('structure_type', 'unknown')
            },
            'fieldMappings': {
                'products': site_data['detected_selectors']
            },
            'metadata': {
                'url': url,
                'cache_key': self._get_cache_key(url),
                'data_version': '1.0',
                'prompt_version': PROMPT_VERSION  # Garder pour compatibilit√©
            }
        }

        # Sauvegarder dans le cache (format ancien pour compatibilit√©)
        print(f"\nüíæ Sauvegarde du scraper dans le cache...")
        cache_path = self._save_scraper_to_cache(url, result)
        print(f"   ‚úÖ Scraper sauvegard√©: {cache_path}")
        print(f"   üìù Version: {PROMPT_VERSION}")
        print(f"\nüöÄ D√©marrage imm√©diat de l'extraction avec le scraper g√©n√©r√©...")

        return result

    def _explore_site_with_ai_tools(self, url: str, initial_html: str) -> Dict:
        """Explore le site en utilisant les outils AI de mani√®re flexible

        L'agent explore le site de mani√®re exhaustive pour maximiser la flexibilit√©.
        """

        # Utiliser les outils pour explorer
        tools = self.ai_tools

        # 1. Extraire tous les liens
        print(f"   üìç Extraction des liens...")
        all_links = tools.get_all_links(initial_html, url)
        print(f"      ‚úÖ {len(all_links)} liens trouv√©s")

        # 2. D√©tecter les URLs de produits (avec plusieurs strat√©gies EXHAUSTIVES)
        print(f"   üîç D√©tection EXHAUSTIVE des pages de produits...")
        product_urls = tools.discover_product_urls(initial_html, url)

        # Essayer aussi avec des s√©lecteurs CSS sp√©cifiques (plus de s√©lecteurs)
        product_selectors = [
            'a[href*="product"]',
            'a[href*="inventory"]',
            'a[href*="inventaire"]',
            'a[href*="moto"]',
            'a[href*="vehicle"]',
            'a[href*="vehicule"]',
            'a[href*="quad"]',
            'a[href*="atv"]',
            'a[href*="snowmobile"]',
            'a[href*="motoneige"]',
            '.product-link',
            '.product-card a',
            '[class*="product"] a',
            '[class*="item"] a',
            '[data-product-id]',
            'a[href*="/detail"]',
            'a[href*="/fiche"]'
        ]

        additional_product_urls = []
        for selector in product_selectors:
            links = tools.parse_html(initial_html, selector)
            for link in links:
                normalized = tools.normalize_url(url, link)
                if normalized and normalized not in product_urls:
                    additional_product_urls.append(normalized)

        product_urls.extend(additional_product_urls)
        product_urls = list(set(product_urls))  # D√©dupliquer
        print(
            f"      ‚úÖ {len(product_urls)} URLs de produits potentielles d√©tect√©es")
        print(
            f"      üéØ Ces URLs serviront de point de d√©part pour trouver TOUS les produits")

        # 3. D√©tecter la pagination (avec plusieurs m√©thodes) - CRITIQUE pour trouver TOUS les produits
        print(f"   üìë D√©tection de la pagination (CRITIQUE pour exhaustivit√©)...")
        pagination = tools.detect_pagination(initial_html, url)
        if pagination:
            print(
                f"      ‚úÖ Pagination d√©tect√©e: {pagination.get('type', 'unknown')}")
            print(f"      üéØ Pattern: {pagination.get('pattern', 'N/A')}")
            print(f"      ‚ö†Ô∏è IMPORTANT: Le scraper devra boucler sur TOUTES les pages")
        else:
            print(
                f"      ‚ö†Ô∏è Aucune pagination d√©tect√©e - devra explorer tous les liens manuellement")

        # 4. Essayer de r√©cup√©rer le sitemap (plusieurs emplacements) - PRIORIT√â ABSOLUE
        print(f"   üó∫Ô∏è Recherche du sitemap (PRIORIT√â pour trouver TOUS les produits)...")
        sitemap_urls = tools.get_sitemap_urls(url)
        if sitemap_urls:
            print(f"      ‚úÖ {len(sitemap_urls)} URLs trouv√©es dans le sitemap")
            print(f"      üéØ Le sitemap contient probablement TOUS les produits du site")
        else:
            print(
                f"      ‚ö†Ô∏è Aucun sitemap trouv√© - devra utiliser pagination/navigation exhaustive")

        # 5. R√©cup√©rer plusieurs types de pages pour analyse compl√®te
        pages_to_analyze = [url]  # Commencer par la page d'accueil

        # Ajouter des URLs de produits (jusqu'√† 10 pour avoir une bonne vari√©t√©)
        for product_url in product_urls[:10]:
            if product_url not in pages_to_analyze:
                pages_to_analyze.append(product_url)

        # Ajouter des pages de cat√©gories si trouv√©es (plusieurs cat√©gories)
        category_keywords = ['category', 'categorie', 'catalog',
                             'catalogue', 'shop', 'boutique', 'inventory', 'inventaire']
        category_pages_found = 0
        for link in all_links[:100]:  # Examiner plus de liens
            link_lower = link.lower()
            if any(keyword in link_lower for keyword in category_keywords):
                if link not in pages_to_analyze and category_pages_found < 5:
                    pages_to_analyze.append(link)
                    category_pages_found += 1

        # Si sitemap disponible, analyser quelques URLs du sitemap pour comprendre la structure
        if sitemap_urls:
            for sitemap_url in sitemap_urls[:5]:
                if sitemap_url not in pages_to_analyze and len(pages_to_analyze) < 15:
                    pages_to_analyze.append(sitemap_url)

        # R√©cup√©rer le HTML de ces pages
        pages_content = {}
        print(
            f"\n   üì• R√©cup√©ration de {len(pages_to_analyze)} pages pour analyse...")
        for page_url in pages_to_analyze:
            print(f"      R√©cup√©ration: {page_url}")
            # Essayer d'abord avec requests, puis avec Selenium si n√©cessaire
            page_html = tools.get(page_url, use_selenium=False)
            if not page_html or len(page_html) < 1000:
                # Si le HTML est trop court, essayer avec Selenium
                print(f"         ‚ö†Ô∏è HTML court, essai avec Selenium...")
                page_html = tools.browser_get(page_url)

            if page_html:
                pages_content[page_url] = page_html
                print(f"      ‚úÖ {len(page_html)} caract√®res")

        # 6. Analyser la structure HTML pour identifier les patterns
        print(f"   üî¨ Analyse de la structure HTML...")
        structure_info = {}
        # Analyser les 3 premi√®res pages
        for page_url, html in list(pages_content.items())[:3]:
            # D√©tecter les s√©lecteurs communs pour les produits
            product_containers = tools.parse_html(
                html, '.product, .item, .card, [class*="product"], [class*="item"]')
            if product_containers:
                structure_info[page_url] = {
                    'has_product_containers': True,
                    'container_count': len(product_containers)
                }

        # 7. D√âCOUVRIR TOUTES LES URLs DE PRODUITS
        print(f"\n   üîç D√©couverte compl√®te de toutes les URLs de produits...")
        all_product_urls_list = []

        # Utiliser le sitemap si disponible (priorit√© absolue)
        if sitemap_urls:
            print(f"      üìã Utilisation du sitemap: {len(sitemap_urls)} URLs")
            all_product_urls_list.extend(sitemap_urls)

        # Parcourir toutes les pages de pagination si pagination d√©tect√©e
        if pagination:
            print(f"      üìë Parcours de la pagination...")
            page = 1
            consecutive_empty = 0
            max_pages = 200
            max_urls = 500  # Limite de s√©curit√©

            while page <= max_pages and len(all_product_urls_list) < max_urls:
                try:
                    page_url = tools.build_pagination_url(
                        url, pagination, page)
                    print(f"         Page {page}: {page_url[:80]}...")

                    page_html = tools.get(page_url, use_selenium=False)
                    if not page_html or len(page_html) < 1000:
                        consecutive_empty += 1
                        if consecutive_empty >= 3:
                            print(
                                f"         ‚ö†Ô∏è 3 pages vides cons√©cutives, arr√™t de la pagination")
                            break
                        page += 1
                        continue

                    # Extraire les URLs de produits de cette page
                    page_product_urls = tools.discover_product_urls(
                        page_html, page_url)
                    if page_product_urls:
                        all_product_urls_list.extend(page_product_urls)
                        consecutive_empty = 0
                        print(
                            f"         ‚úÖ {len(page_product_urls)} URLs trouv√©es (total: {len(all_product_urls_list)})")
                    else:
                        consecutive_empty += 1
                        if consecutive_empty >= 3:
                            print(
                                f"         ‚ö†Ô∏è 3 pages sans produits cons√©cutives, arr√™t")
                            break

                    page += 1
                    time.sleep(0.3)  # Rate limiting
                except Exception as e:
                    print(f"         ‚ö†Ô∏è Erreur page {page}: {e}")
                    consecutive_empty += 1
                    if consecutive_empty >= 3:
                        break
                    page += 1

        # 8. D√âDUPLIQUER LES URLs
        print(f"\n   üîÑ D√©duplication des URLs...")
        normalized_urls_dict = {}
        for url in all_product_urls_list:
            normalized = tools.normalize_url_for_dedup(url)
            # Garder l'URL originale la plus courte
            if normalized not in normalized_urls_dict or len(url) < len(normalized_urls_dict[normalized]):
                normalized_urls_dict[normalized] = url

        all_product_urls = list(normalized_urls_dict.values())
        print(
            f"      ‚úÖ {len(all_product_urls)} URLs uniques apr√®s d√©duplication (sur {len(all_product_urls_list)} totales)")

        # 8.5. D√âDUPLIQUER PAR MOD√àLE+ANN√âE (ignorer les couleurs)
        print(f"\n   üîÑ D√©duplication par mod√®le+ann√©e (ignorer les couleurs)...")
        model_year_urls_dict = {}
        for url in all_product_urls:
            model_year_key = tools.normalize_url_by_model_year(url)
            # Garder la premi√®re URL trouv√©e pour chaque combinaison mod√®le+ann√©e
            if model_year_key not in model_year_urls_dict:
                model_year_urls_dict[model_year_key] = url

        all_product_urls = list(model_year_urls_dict.values())
        print(
            f"      ‚úÖ {len(all_product_urls)} URLs uniques apr√®s d√©duplication par mod√®le+ann√©e (une URL par mod√®le+ann√©e, couleurs ignor√©es)")

        # 9. FILTRER POUR NE GARDER QUE LES PAGES DE PRODUITS
        print(f"\n   üéØ Filtrage pour ne garder que les pages de produits...")
        filtered_product_urls = []

        # Mots-cl√©s indicateurs de pages de produits
        product_keywords = ['product', 'inventory', 'inventaire', 'moto', 'vehicle', 'vehicule',
                            'quad', 'atv', 'snowmobile', 'motoneige', 'detail', 'fiche']
        exclude_keywords = ['contact', 'about', 'policy', 'privacy', 'terms', 'blog', 'news',
                            'service', 'appointment', 'financing', 'home', 'index', 'login',
                            'register', 'account', 'cart', 'checkout', 'wishlist', 'search', 'faq']

        # V√©rifier un √©chantillon pour identifier les patterns
        sample_size = min(50, len(all_product_urls))
        for url in all_product_urls[:sample_size]:
            url_lower = url.lower()
            # Exclure si contient un mot-cl√© d'exclusion
            if any(keyword in url_lower for keyword in exclude_keywords):
                continue
            # Inclure si contient un mot-cl√© de produit
            if any(keyword in url_lower for keyword in product_keywords):
                filtered_product_urls.append(url)

        # Si filtrage trop strict, utiliser toutes les URLs
        if len(filtered_product_urls) < len(all_product_urls) * 0.1 and len(all_product_urls) > 50:
            print(
                f"      ‚ö†Ô∏è Filtrage trop strict ({len(filtered_product_urls)}/{len(all_product_urls)}), utilisation de toutes les URLs")
            filtered_product_urls = all_product_urls
        else:
            # Appliquer le m√™me filtrage au reste
            for url in all_product_urls[sample_size:]:
                url_lower = url.lower()
                if any(keyword in url_lower for keyword in exclude_keywords):
                    continue
                if any(keyword in url_lower for keyword in product_keywords):
                    filtered_product_urls.append(url)

        print(
            f"      ‚úÖ {len(filtered_product_urls)} URLs de produits filtr√©es")

        return {
            # Beaucoup plus de liens pour trouver TOUS les produits
            'all_links': all_links[:500],
            # Plus d'URLs de produits pour analyse
            'product_urls': product_urls[:200],
            'pagination': pagination,
            # TOUTES les URLs du sitemap si disponible
            'sitemap_urls': sitemap_urls[:500] if sitemap_urls else [],
            # NOUVEAU: Toutes les URLs de produits d√©couvertes et d√©dupliqu√©es
            'all_product_urls': filtered_product_urls,
            'discovered_pages': list(pages_content.keys()),
            # Plus de contenu HTML
            'pages_content': {k: v[:100000] for k, v in pages_content.items()},
            'structure_info': structure_info,
            'exploration_metadata': {
                'total_links_found': len(all_links),
                'total_product_urls': len(product_urls),
                'total_sitemap_urls': len(sitemap_urls),
                'total_all_product_urls': len(filtered_product_urls),
                'pages_analyzed': len(pages_content),
                'has_sitemap': len(sitemap_urls) > 0,
                'has_pagination': pagination is not None,
                # Indique si le sitemap semble complet
                'sitemap_is_complete': len(sitemap_urls) > 100
            }
        }

    def _generate_scraper_with_ai_exploration(self, url: str, exploration_result: Dict) -> Dict:
        """G√©n√®re le scraper bas√© sur l'exploration avec les outils AI"""

        schema_str = json.dumps(EXTRACTION_SCHEMA, indent=2)

        # Pr√©parer le contexte d'exploration
        exploration_context = f"""
EXPLORATION DU SITE AVEC LES OUTILS AI:

1. LIENS D√âCOUVERTS: {len(exploration_result.get('all_links', []))} liens internes
   Exemples: {exploration_result.get('all_links', [])[:10]}

2. PAGES DE PRODUITS D√âTECT√âES: {len(exploration_result.get('product_urls', []))} URLs
   Exemples: {exploration_result.get('product_urls', [])[:5]}

3. PAGINATION:
   {json.dumps(exploration_result.get('pagination'), indent=2) if exploration_result.get(
            'pagination') else 'Aucune pagination d√©tect√©e'}

4. SITEMAP:
   {len(exploration_result.get('sitemap_urls', []))} URLs trouv√©es dans le sitemap

5. URLs DE PRODUITS PR√â-D√âCOUVERTES (D√âJ√Ä D√âDUPLIQU√âES):
   {len(exploration_result.get('all_product_urls', []))} URLs de produits d√©couvertes par l'AI Agent
   ‚ö†Ô∏è CRITIQUE: Ces URLs sont D√âJ√Ä d√©dupliqu√©es et filtr√©es - utiliser directement dans le script
   Exemples: {exploration_result.get('all_product_urls', [])[:5]}

6. PAGES ANALYS√âES:
"""

        for page_url, html_content in exploration_result.get('pages_content', {}).items():
            truncated = html_content[:30000]  # Limiter la taille
            exploration_context += f"\n   PAGE: {page_url}\n   HTML (tronqu√©):\n{truncated[:1000]}...\n"

        # Pr√©parer les m√©tadonn√©es d'exploration
        exploration_meta = exploration_result.get('exploration_metadata', {})

        prompt = rf"""Tu es un expert en scraping web. G√©n√®re un scraper Python robuste et exhaustif pour ce site de vente de v√©hicules motoris√©s.

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
1. CONTEXTE DU SITE
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

URL DE BASE: {url}

{exploration_context}

M√âTADONN√âES D'EXPLORATION:
- Total liens trouv√©s: {exploration_meta.get('total_links_found', 0)}
- URLs de produits d√©tect√©es: {exploration_meta.get('total_product_urls', 0)}
- URLs dans sitemap: {exploration_meta.get('total_sitemap_urls', 0)}
- Pages analys√©es: {exploration_meta.get('pages_analyzed', 0)}
- Sitemap disponible: {'Oui (COMPLET)' if exploration_meta.get('has_sitemap') and exploration_meta.get('sitemap_is_complete') else 'Oui (partiel)' if exploration_meta.get('has_sitemap') else 'Non'}
- Pagination d√©tect√©e: {'Oui' if exploration_meta.get('has_pagination') else 'Non'}

‚ö†Ô∏è CRITIQUE - URLs PR√â-D√âCOUVERTES:
L'AI Agent a d√©j√† d√©couvert TOUTES les URLs de produits du site et les a d√©dupliqu√©es.
Ces URLs sont disponibles dans exploration_result['all_product_urls'].
Le script g√©n√©r√© DOIT utiliser ces URLs directement - NE PAS les red√©couvrir.

Total URLs de produits pr√©-d√©couvertes: {exploration_meta.get('total_all_product_urls', 0)}
Exemples: {exploration_result.get('all_product_urls', [])[:5]}

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
2. OBJECTIF
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚ö†Ô∏è CRITIQUE: Trouver TOUS les produits du site, pas seulement un √©chantillon.

Le scraper DOIT ABSOLUMENT:
- ‚ö†Ô∏è OBLIGATOIRE ET NON-N√âGOCIABLE: Utiliser DIRECTEMENT les URLs pr√©-d√©couvertes (exploration_result['all_product_urls'])
- ‚ö†Ô∏è INTERDIT ABSOLUMENT: NE JAMAIS red√©couvrir les URLs (d√©j√† fait par l'AI Agent avant g√©n√©ration du script)
- ‚ö†Ô∏è INTERDIT ABSOLUMENT: NE JAMAIS appeler get_sitemap_urls(), detect_pagination(), ou discover_product_urls() dans l'√âTAPE 1
- ‚ö†Ô∏è INTERDIT ABSOLUMENT: NE JAMAIS utiliser optimized_path, load_json('optimized_path'), ou faire de d√©couverte compl√®te
- ‚ö†Ô∏è INTERDIT ABSOLUMENT: NE JAMAIS cr√©er normalized_urls_dict ou add_url_with_dedup() - les URLs sont D√âJ√Ä d√©dupliqu√©es
- Respecter le sch√©ma JSON fourni
- ‚ö†Ô∏è CRITIQUE: Faire l'extraction LOCALEMENT avec BeautifulSoup (SANS Gemini)
- Utiliser les fieldMappings d√©tect√©s pour extraction CSS/XPath
- Fallback sur patterns g√©n√©riques si fieldMappings √©chouent
- G√©rer les erreurs et cas limites

‚ö†Ô∏è R√àGLE ABSOLUE: L'√âTAPE 1 doit COMMENCER par:
   all_product_urls = exploration_result.get('all_product_urls', [])
   
   Et rien d'autre. Pas de sitemap, pas de pagination, pas de d√©couverte.

SCH√âMA JSON √Ä RESPECTER:
{schema_str}

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
3. OUTILS DISPONIBLES
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Tous ces outils sont disponibles dans le namespace d'ex√©cution:

REQU√äTES WEB:
- get(url, use_selenium=False): HTML rapide (requests)
- browser_get(url): HTML rendu (Selenium pour JavaScript)
   - session: Session requests r√©utilisable

PARSING HTML:
- parse_html(html, selector): Extraire √©l√©ments avec CSS
- get_text_content(html, selector=None): Extraire texte brut
   - BeautifulSoup: Parser HTML complet

LIENS & URLS:
- get_all_links(html, base_url): Tous les liens normalis√©s
- discover_product_urls(html, base_url): URLs de produits
   - normalize_url(base, link): Normaliser liens relatifs
   - urljoin, urlparse: Manipulation d'URLs

EXPLORATION AVANC√âE:
- get_sitemap_urls(url): R√©cup√©rer TOUTES les URLs du sitemap (AM√âLIOR√â: cherche aussi dans robots.txt)
  - Cherche dans robots.txt pour directives Sitemap:
  - Supporte sitemaps multiples et sitemap index
  - D√©tection automatique de tous les sitemaps disponibles
- detect_pagination(html, url): D√©tecter pattern de pagination AUTOMATIQUEMENT
  - Cherche d'abord dans les liens HTML
  - Cherche ensuite dans l'URL actuelle
  - Si rien trouv√©, TESTE automatiquement les patterns standards (page=, paged=, fwp_paged=, p=, offset=, start=)
  - Retourne: type, pattern, exemple, current_page, detected_by ('html', 'url', ou 'test')
- build_pagination_url(base_url, pagination_info, page_number): Construire URL de pagination (pr√©serve les filtres existants)
- extract_url_filters(url): Extraire filtres depuis URL (ex: {{'v1': 'Motocyclette'}} depuis ?v1=Motocyclette)
- build_url_with_filters(base_url, filters, pagination=None, page_number=1): Construire URL avec filtres + pagination
- discover_product_urls(html, base_url): D√©couvrir URLs produits via heuristiques (mots-cl√©s)
- get_all_links(html, base_url): Tous les liens normalis√©s du m√™me domaine

D√âTECTION INTELLIGENTE:
- analyze_url_patterns(urls): Analyser patterns d'URL pour identifier produits, cat√©gories (/product/*, /item/*, etc.)
- detect_important_sections(html, base_url): D√©tecter sections importantes (navigation, cat√©gories, product listings, breadcrumbs)
- detect_ajax_data_layer(html): D√©tecter AJAX calls et data layer (dataLayer, window.__INITIAL_STATE__, etc.)
- detect_internal_apis(html, base_url): D√©tecter APIs internes (wp-json, /api/products, Shopify Storefront, etc.)

R√âCUP√âRATION INTELLIGENTE:
- smart_get(url, max_retries=3): GET intelligent avec fallback (requests ‚Üí Selenium ‚Üí API detection)
  - Retourne: html, method_used, api_detected, blocked, requires_javascript
- detect_blocking(html, status_code): D√©tecter si page bloqu√©e (Cloudflare, bot detection, CAPTCHA)

EXTRACTION HYBRIDE:
- extract_with_hybrid_method(html, field_name, selectors): Extraction hybride (CSS/XPath ‚Üí Gemini fallback)
  - Essaie d'abord s√©lecteurs CSS fournis
  - Si √©chec, essaie JSON-LD
  - Fallback Gemini si n√©cessaire

DONN√âES STRUCTUR√âES:
- extract_json_ld(html): Extraire donn√©es JSON-LD (peut contenir listes de produits)
- extract_opengraph(html): Extraire m√©tadonn√©es Open Graph (peut contenir URLs produits)
- extract_microdata(html): Extraire microdata (schema.org) depuis HTML
- extract_script_data(html): Extraire donn√©es depuis variables JavaScript (window.__INITIAL_STATE__, etc.)

FORMULAIRES & RECHERCHE:
- find_search_form(html): Trouver formulaires de recherche (action, method, inputs)
- find_filters(html): Trouver filtres (selects, checkboxes) avec leurs options

APIS & ENDPOINTS:
- detect_api_endpoints(html): D√©tecter endpoints API depuis JavaScript (fetch, axios, etc.)

GESTION AVANC√âE:
- retry_get(url, max_retries=3, backoff=1.0, use_selenium=False): Retry avec backoff exponentiel
- detect_rate_limit(response_text, status_code): D√©tecter rate limiting (429, etc.)
- wait_between_requests(seconds=1.0): Attendre entre requ√™tes pour √©viter rate limiting
- validate_url(url): Valider qu'une URL est bien form√©e

D√âTECTION AVANC√âE:
- detect_infinite_scroll(html): D√©tecter infinite scroll / lazy loading
- detect_captcha(html): D√©tecter pr√©sence de CAPTCHA
- find_iframes(html): Trouver toutes les iframes et leurs sources

UTILITAIRES:
- clean_text(text): Nettoyer texte (espaces, caract√®res sp√©ciaux)
- clean_html(html): Nettoyer HTML des caract√®res Unicode invalides (surrogates) - ‚ö†Ô∏è CRITIQUE pour √©viter UnicodeEncodeError
- prepare_html_for_prompt(html): ‚ö†Ô∏è CRITIQUE - Pr√©pare HTML pour insertion s√©curis√©e dans prompts
  - Nettoie les surrogates Unicode
  - √âchappe les accolades {{ }} pour √©viter erreurs dans f-strings
  - Remplace les triple backticks ``` qui peuvent casser le formatage
  - TOUJOURS utiliser avant d'ins√©rer HTML dans un prompt (surtout dans f-strings)
- extract_price(text): Extraire prix depuis texte
- extract_number(text): Extraire n'importe quel nombre depuis texte
- check_robots_txt(url): V√©rifier robots.txt

STANDARDISATION & VALIDATION:
- standardize_field(field_name, value): Standardiser champ (prix ‚Üí float, disponibilit√© ‚Üí enum, images ‚Üí liste)
- validate_product_data(product): Valider donn√©es produit et d√©tecter anomalies
  - D√©tecte champs manquants
  - D√©tecte anomalies (prix suspect, image invalide)
  - Auto-corrige les valeurs
- structural_preview(urls, sample_size=10): Pr√©lecture structurelle - analyser √©chantillon pages pour patterns globaux
  - D√©tecte s√©lecteurs communs
  - Recommande s√©lecteurs les plus fiables
  - V√©rifie coh√©rence structure

EXTRACTION LOCALE:
- BeautifulSoup: Parser HTML pour extraction locale
- extract_price(text): Extraire prix depuis texte
- extract_year(text): Extraire ann√©e depuis texte
- Patterns g√©n√©riques: S√©lecteurs CSS communs pour produits (h1, .price, .description, etc.)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
4. WORKFLOW √Ä SUIVRE
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚ö†Ô∏è ORDRE CRITIQUE DES √âTAPES:
1. D'abord: Utiliser les URLs pr√©-d√©couvertes par l'AI Agent (√âTAPE 1)
2. Ensuite: R√©cup√©rer le HTML de chaque URL (√âTAPE 2)
3. Puis: Extraire les donn√©es de chaque produit LOCALEMENT (√âTAPE 3) - SANS Gemini
4. Enfin: Valider et retourner les r√©sultats (√âTAPE 4)

√âTAPE 1: UTILISER LES URLs PR√â-D√âCOUVERTES
   ‚ö†Ô∏è CRITIQUE: Les URLs ont D√âJ√Ä √©t√© d√©couvertes par l'AI Agent et sont d√©dupliqu√©es.
   ‚ö†Ô∏è NE PAS red√©couvrir les URLs - utiliser directement exploration_result['all_product_urls']
   
   Code OBLIGATOIRE (avec URLs HARDCOD√âES):
   ```python
   # √âTAPE 1: URLs hardcod√©es (d√©j√† d√©couvertes par l'AI Agent)
   print(f"\\n{{'='*60}}")
   print(f"üìç √âTAPE 1: UTILISATION DES URLs PR√â-D√âCOUVERTES")
   print(f"{{'='*60}}")
   
   # ‚ö†Ô∏è CRITIQUE: URLs HARDCOD√âES directement dans le script
   # Utiliser exploration_result['all_product_urls'] pour remplir cette liste lors de la g√©n√©ration
   PRODUCT_URLS = [
       "https://site.com/product1",
       "https://site.com/product2",
       # ... TOUTES les URLs de exploration_result['all_product_urls'] doivent √™tre ici
   ]
   
   if not PRODUCT_URLS:
       print("‚ùå Aucune URL de produit pr√©-d√©couverte par l'AI Agent")
       return {{'companyInfo': {{}}, 'products': []}}
   
   print(f"‚úÖ {{len(PRODUCT_URLS)}} URLs de produits pr√©-d√©couvertes (hardcod√©es dans le script)")
   print(f"   Exemples: {{PRODUCT_URLS[:3]}}")
   ```
   
   ‚ö†Ô∏è IMPORTANT: 
   - Les URLs sont D√âJ√Ä d√©dupliqu√©es (pas besoin de red√©dupliquer)
   - Les URLs sont D√âJ√Ä filtr√©es pour ne garder que les pages de produits
   - Passer directement √† l'√âTAPE 2 (r√©cup√©ration HTML)

√âTAPE 2: R√âCUP√âRER LE HTML (M√âTHODE INTELLIGENTE)
      if sitemap_urls and len(sitemap_urls) > 10:
          for url in sitemap_urls:
              add_url_with_dedup(url)  # ‚ö†Ô∏è D√©duplication imm√©diate
          print(f"‚úÖ {{len(sitemap_urls)}} URLs depuis sitemap (COMPLET)")
          print(f"   Apr√®s d√©duplication: {{len(normalized_urls_dict)}} URLs uniques")
          
          # NOUVEAU: Analyser les patterns d'URL pour identifier structure
          url_patterns = analyze_url_patterns(list(sitemap_urls)[:100])  # Analyser √©chantillon
          print(f"üìä Patterns d√©tect√©s: {{url_patterns.get('product_patterns', [])}}")
          
          # Si sitemap complet, tu peux skip la pagination, mais v√©rifie quand m√™me
      else:
          print(f"‚ö†Ô∏è Sitemap vide ou incomplet ({{len(sitemap_urls) if sitemap_urls else 0}} URLs), utiliser pagination")
      
      # NOUVEAU: Pr√©lecture structurelle pour d√©tecter patterns globaux
      if len(all_product_urls) > 0:
          sample_urls = list(all_product_urls)[:10]
          structural_info = structural_preview(sample_urls, sample_size=10)
          print(f"üîç S√©lecteurs recommand√©s: {{structural_info.get('recommended_selectors', {{}})}}")

   2. Pagination EXHAUSTIVE (TOUJOURS essayer, m√™me si sitemap existe - pour v√©rification)
      D√âCISION: Si pagination_info est None, essaie quand m√™me de construire des URLs avec ?page=1, ?paged=1, etc.
      
      # √âTAPE A: Extraire les filtres de l'URL (IMPORTANT pour pr√©server les filtres)
      url_filters = extract_url_filters(base_url)
      print(f"üìã Filtres d√©tect√©s dans l'URL: {{url_filters}}")
      
      # √âTAPE B: D√©tecter la pagination (d√©tecte automatiquement les patterns standards si n√©cessaire)
      pagination_info = detect_pagination(html, base_url)
      
      if pagination_info:
          detected_by = pagination_info.get('detected_by', 'html_or_url')
          print(f"‚úÖ Pagination d√©tect√©e: {{pagination_info.get('pattern')}} (type: {{pagination_info.get('type')}}, m√©thode: {{detected_by}})")
          
          # Utiliser build_pagination_url pour construire les URLs correctement
          # (pr√©serve automatiquement les filtres existants dans l'URL)
   page = 1
          consecutive_empty_pages = 0
          consecutive_no_new_products = 0
          previous_total = 0
          
          max_pages = 200  # Limite de s√©curit√© stricte pour √©viter boucles infinies
          while page <= max_pages:  # Limite s√©curit√© r√©duite
              # ‚ö†Ô∏è LIMITE: Arr√™ter √† 500 URLs pour passer √† l'√©tape suivante
              if len(normalized_urls_dict) >= 500:
                  print(f"   ‚úÖ Limite de 500 URLs atteinte, passage √† l'√©tape suivante")
                  break
              
              page_url = build_pagination_url(base_url, pagination_info, page)
              print(f"   üîç Test page {{page}}: {{page_url}}")
              
       html = get(page_url)
              if not html or len(html) < 1000:
                  print(f"   ‚ö†Ô∏è Page {{page}} vide ou erreur, arr√™t")
           break
              
              products = discover_product_urls(html, base_url)
              
              # V√©rifier si nouveaux produits trouv√©s (avec d√©duplication imm√©diate)
              current_total = len(normalized_urls_dict)
              for url in products:
                  add_url_with_dedup(url)  # ‚ö†Ô∏è D√©duplication imm√©diate
              new_total = len(normalized_urls_dict)
              new_products_count = new_total - current_total
              
              # ‚ö†Ô∏è LIMITE: V√©rifier √† nouveau apr√®s ajout
              if len(normalized_urls_dict) >= 500:
                  print(f"   ‚úÖ Limite de 500 URLs atteinte, passage √† l'√©tape suivante")
                  break
              
              # Log d√©taill√© pour d√©boguer
              print(f"   üìä Page {{page}}: {{len(products)}} produits trouv√©s, {{new_products_count}} nouveaux (Total: {{new_total}})")
              
              if not products:
                  consecutive_empty_pages += 1
                  # V√©rifier si message "Aucun produit" ou similaire
                  if "aucun produit" in html.lower() or "no products" in html.lower() or consecutive_empty_pages >= 3:
                      print(f"   ‚ö†Ô∏è Plus de produits trouv√©s apr√®s {{consecutive_empty_pages}} pages vides")
                      break
              elif new_products_count == 0:
                  # Page avec produits mais tous d√©j√† connus (d√©j√† dans all_product_urls = DOUBLONS)
                  consecutive_no_new_products += 1
                  duplicates = len(products) - new_products_count
                  print(f"   ‚ö†Ô∏è Page {{page}}: {{len(products)}} produits trouv√©s mais {{duplicates}} doublons (Total: {{new_total}}, consecutive: {{consecutive_no_new_products}}/3)")
                  
                  # Arr√™ter si 3 pages cons√©cutives sans nouveaux produits
                  if consecutive_no_new_products >= 3:
                      print(f"   ‚úÖ Arr√™t: {{consecutive_no_new_products}} pages cons√©cutives sans nouveaux produits")
                      print(f"   ‚úÖ Toutes les pages ont √©t√© filtr√©es. Total unique: {{new_total}} URLs")
                      break
              else:
                  # Nouveaux produits trouv√©s
                  consecutive_empty_pages = 0
                  consecutive_no_new_products = 0
                  print(f"   ‚úÖ Page {{page}}: {{new_products_count}} nouveaux produits ({{len(products)}} trouv√©s, Total: {{new_total}})")
              
              previous_total = new_total
       page += 1
              wait_between_requests(0.5)  # Attendre entre pages pour √©viter rate limiting
          
          if page > max_pages:
              print(f"   ‚ö†Ô∏è Limite de s√©curit√© atteinte ({{max_pages}} pages)")
              print(f"   ‚ö†Ô∏è Si le site a plus de pages, augmentez max_pages ou v√©rifiez la logique de pagination")
          
          print(f"‚úÖ Pagination termin√©e: {{len(normalized_urls_dict)}} URLs uniques trouv√©es sur {{page-1}} pages")
      else:
          print("‚ùå Aucune pagination d√©tect√©e m√™me apr√®s tests des patterns standards")
          print("   Le site n'utilise peut-√™tre pas de pagination, ou utilise un syst√®me non standard")
          print("   Essayer d'explorer les cat√©gories ou utiliser browser_get() pour JavaScript")

   3. Exploration de Cat√©gories AM√âLIOR√âE (si n√©cessaire)
      # NOUVEAU: D√©tection automatique des sections importantes
      sections = detect_important_sections(html, base_url)
      print(f"üìÇ Sections d√©tect√©es:")
      print(f"   - Navigation: {{len(sections.get('navigation', []))}} liens")
      print(f"   - Cat√©gories: {{len(sections.get('categories', []))}} cat√©gories")
      print(f"   - Product listings: {{len(sections.get('product_listings', []))}} pages")
      
      # Utiliser les cat√©gories d√©tect√©es automatiquement
      categories = sections.get('categories', [])
      if not categories:
          # Fallback: chercher manuellement
          all_links = get_all_links(html, base_url)
          categories = [link for link in all_links if 'category' in link.lower() or 'categorie' in link.lower()]
      
      # Pour chaque cat√©gorie, explorer et trouver produits (avec pagination si n√©cessaire)
      for category_url in categories:
          print(f"   üìÅ Exploration cat√©gorie: {{category_url}}")
          cat_html = get(category_url)
          cat_products = discover_product_urls(cat_html, base_url)
          for url in cat_products:
              add_url_with_dedup(url)  # ‚ö†Ô∏è D√©duplication imm√©diate

   4. D√©couverte Heuristique (toujours utiliser)
      product_urls = discover_product_urls(html, base_url)
      for url in product_urls:
          add_url_with_dedup(url)  # ‚ö†Ô∏è D√©duplication imm√©diate

   5. Parsing avec S√©lecteurs CSS (si structure connue)
      # Utiliser parse_html avec s√©lecteurs sp√©cifiques
      product_links = parse_html(html, "a.product-link[href]")
      product_links = parse_html(html, ".product-card a[href]")
      # Normaliser et ajouter

   6. Donn√©es Structur√©es (JSON-LD, Open Graph)
      json_ld = extract_json_ld(html)
      # Extraire URLs de produits depuis JSON-LD
      og_data = extract_opengraph(html)
      # Extraire URLs depuis Open Graph

   7. Exploration R√©cursive (si autres m√©thodes √©chouent)
      # Explorer les liens prometteurs r√©cursivement (avec limite de profondeur)

   8. Sites avec API AM√âLIOR√âE (si d√©tect√©)
      # NOUVEAU: D√©tection automatique d'APIs internes (wp-json, Shopify, etc.)
      internal_apis = detect_internal_apis(html, base_url)
      if internal_apis:
          print(f"‚úÖ APIs internes d√©tect√©es: {{[api['name'] for api in internal_apis]}}")
          for api in internal_apis:
              print(f"   üîå API: {{api['name']}} - {{api['endpoint']}}")
              # Essayer d'appeler l'API
              try:
                  response = session.get(api['endpoint'], timeout=10)
                  if response.status_code == 200:
                      api_data = response.json()
                      # Extraire produits depuis r√©ponse API
                      # Structure d√©pend de l'API (adapter selon le type)
              except Exception as e:
                  print(f"   ‚ö†Ô∏è Erreur API {{api['name']}}: {{e}}")
      
      # Aussi chercher dans AJAX/data layer
      ajax_data = detect_ajax_data_layer(html)
      if ajax_data.get('ajax_endpoints'):
          print(f"‚úÖ Endpoints AJAX d√©tect√©s: {{len(ajax_data['ajax_endpoints'])}}")
          # Essayer endpoints AJAX trouv√©s

   9. Donn√©es dans JavaScript (SPA - Single Page Apps)
      script_data = extract_script_data(html)
      if script_data:
          # Extraire URLs produits depuis window.__INITIAL_STATE__ ou similaire
          # Beaucoup de sites modernes chargent les donn√©es ainsi

   10. Formulaires de Recherche (si n√©cessaire)
       search_form = find_search_form(html)
       if search_form:
           # Utiliser le formulaire pour rechercher des produits
           # Essayer diff√©rentes requ√™tes de recherche

   11. Filtres (pour explorer diff√©rentes combinaisons)
       filters = find_filters(html)
       if filters:
           # Explorer diff√©rentes combinaisons de filtres
           # Pour trouver tous les produits dans chaque cat√©gorie/filtre

   ‚ö†Ô∏è IMPORTANT: 
   - COMBINER plusieurs strat√©gies en parall√®le pour √™tre s√ªr de ne rien manquer
   - ‚ö†Ô∏è LIMITE CRITIQUE: Arr√™ter la d√©couverte √† 500 URLs et passer √† l'√©tape suivante (r√©cup√©ration HTML)
   - V√©rifier `len(normalized_urls_dict) >= 500` dans TOUTES les boucles de d√©couverte
   - D√®s que 500 URLs sont atteintes, BREAK imm√©diatement et passe √† l'√âTAPE 1.5 (filtrage)
   - Toujours d√©dupliquer: all_product_urls = list(set(all_product_urls))
   - Logger le nombre total trouv√©: print(f"‚úÖ TOTAL: {{len(all_product_urls)}} URLs")
   - Si aucune strat√©gie ne fonctionne, utiliser browser_get() pour JavaScript
   - Utiliser retry_get() et wait_between_requests() pour √©viter rate limiting
   - Si CAPTCHA d√©tect√©: utiliser browser_get() et attendre plus longtemps
   
   ‚ö†Ô∏è CRITIQUE - FILTRAGE DES URLs (NOUVEAU):
   Apr√®s avoir trouv√© TOUTES les URLs, tu DOIS filtrer pour ne garder QUE les pages de produits.
   √âlimine les URLs qui m√®nent √† des cat√©gories, pages d'accueil, pages de service, etc.
   
   Code OBLIGATOIRE apr√®s la d√©couverte:
   ```python
   # √âTAPE 1.5: FILTRAGE DES URLs - Ne garder que les pages de produits
   print(f"\\n{{'='*60}}")
   print(f"üîç √âTAPE 1.5: FILTRAGE DES URLs")
   print(f"{{'='*60}}")
   
   # Normaliser et d√©dupliquer les URLs (supprimer param√®tres de tracking, etc.)
   def normalize_url_for_dedup(url):
       \"\"\"Normalise une URL pour la d√©duplication (supprime param√®tres inutiles)\"\"\"
       from urllib.parse import urlparse, urlunparse, parse_qs, urlencode
       parsed = urlparse(url)
       
       # Param√®tres √† conserver (pagination, filtres importants)
       keep_params = ['page', 'paged', 'fwp_paged', 'p', 'offset', 'start', 'id', 'product_id']
       
       # Param√®tres √† supprimer (tracking, analytics, etc.)
       remove_params = ['utm_source', 'utm_medium', 'utm_campaign', 'ref', 'source', 
                        'fbclid', 'gclid', '_ga', 'tracking', 'affiliate']
       
       query_params = parse_qs(parsed.query)
       filtered_params = {{}}
       
       for key, values in query_params.items():
           if key.lower() in keep_params:
               filtered_params[key] = values
           elif key.lower() not in remove_params:
               # Garder les autres param√®tres (filtres, etc.)
               filtered_params[key] = values
       
       # Reconstruire l'URL sans les param√®tres de tracking
       new_query = urlencode(filtered_params, doseq=True)
       normalized = urlunparse((
           parsed.scheme, parsed.netloc, parsed.path,
           parsed.params, new_query, ''  # Supprimer le fragment
       ))
       return normalized
   
   # Normaliser toutes les URLs pour d√©duplication
   normalized_urls = {{}}
   for url in all_product_urls:
       normalized = normalize_url_for_dedup(url)
       # Garder l'URL originale la plus courte (sans param√®tres de tracking)
       if normalized not in normalized_urls or len(url) < len(normalized_urls[normalized]):
           normalized_urls[normalized] = url
   
   all_product_urls = list(normalized_urls.values())
   print(f"‚úÖ Apr√®s normalisation: {{len(all_product_urls)}} URLs uniques")
   
   # Filtrer pour ne garder QUE les pages de produits
   # V√©rifier chaque URL pour confirmer qu'elle m√®ne √† un produit
   filtered_product_urls = []
   sample_size = min(50, len(all_product_urls))  # V√©rifier un √©chantillon pour identifier patterns
   
   print(f"üîç V√©rification d'un √©chantillon de {{sample_size}} URLs pour identifier les patterns...")
   sample_urls = list(all_product_urls)[:sample_size]
   product_patterns = []
   non_product_patterns = []
   
   for url in sample_urls:
       try:
           html = get(url)
           if not html or len(html) < 1000:
               continue
           
           # V√©rifier si c'est une page de produit
           is_product = False
           
           # Indicateurs positifs (page de produit)
           product_indicators = [
               'prix' in html.lower() or 'price' in html.lower(),
               'ajouter au panier' in html.lower() or 'add to cart' in html.lower(),
               'disponible' in html.lower() or 'available' in html.lower(),
               'product-detail' in html.lower() or 'product_detail' in html.lower(),
               'inventory-item' in html.lower() or 'inventaire' in html.lower(),
               'fiche technique' in html.lower() or 'specifications' in html.lower(),
               'marque' in html.lower() and 'mod√®le' in html.lower(),
               'brand' in html.lower() and 'model' in html.lower(),
           ]
           
           # Indicateurs n√©gatifs (PAS une page de produit)
           non_product_indicators = [
               'liste' in html.lower() and 'produit' not in html.lower(),
               'category' in html.lower() and 'product' not in html.lower(),
               'categorie' in html.lower() and 'produit' not in html.lower(),
               ("page d'accueil" in html.lower() or 'homepage' in html.lower()),
               'contact' in html.lower() and 'product' not in html.lower(),
               'about' in html.lower() and 'product' not in html.lower(),
           ]
           
           # Si au moins 1 indicateur positif ET pas d'indicateurs n√©gatifs forts
           # (Assouplir les crit√®res pour ne pas √©liminer trop de pages)
           has_positive = sum(product_indicators) >= 1
           has_strong_negative = any([
               'liste' in html.lower() and 'produit' not in html.lower() and 'product' not in html.lower(),
               'category' in html.lower() and 'product' not in html.lower() and 'produit' not in html.lower(),
               ("page d'accueil" in html.lower() or 'homepage' in html.lower()),
           ])
           
           if has_positive and not has_strong_negative:
               is_product = True
               # Analyser le pattern de l'URL
               from urllib.parse import urlparse
               parsed = urlparse(url)
               path_parts = [p for p in parsed.path.split('/') if p]
               if path_parts:
                   product_patterns.append('/'.join(path_parts[-2:]))  # Derniers segments
           else:
               # Analyser le pattern pour exclusion
               from urllib.parse import urlparse
               parsed = urlparse(url)
               path_parts = [p for p in parsed.path.split('/') if p]
               if path_parts:
                   non_product_patterns.append('/'.join(path_parts[-2:]))
           
           if is_product:
               filtered_product_urls.append(url)
           
           wait_between_requests(0.2)  # Attendre entre v√©rifications
       except Exception as e:
           print(f"   ‚ö†Ô∏è Erreur lors de la v√©rification de {{url}}: {{e}}")
           continue
   
   # Identifier les patterns les plus fr√©quents
   from collections import Counter
   product_pattern_counter = Counter(product_patterns)
   non_product_pattern_counter = Counter(non_product_patterns)
   
   print(f"üìä Patterns de produits identifi√©s: {{dict(product_pattern_counter.most_common(5))}}")
   print(f"üìä Patterns NON-produits identifi√©s: {{dict(non_product_pattern_counter.most_common(5))}}")
   
   # Filtrer le reste des URLs bas√© sur les patterns identifi√©s
   if product_pattern_counter:
       # Utiliser les patterns pour filtrer rapidement
       common_product_patterns = [p for p, count in product_pattern_counter.most_common(3) if count >= 2]
       common_non_product_patterns = [p for p, count in non_product_pattern_counter.most_common(3) if count >= 2]
       
       print(f"üîç Application des patterns sur les {{len(all_product_urls) - sample_size}} URLs restantes...")
       
       for url in all_product_urls[sample_size:]:
           from urllib.parse import urlparse
           parsed = urlparse(url)
           path_parts = [p for p in parsed.path.split('/') if p]
           if path_parts:
               url_pattern = '/'.join(path_parts[-2:])
               
               # Exclure si pattern non-produit identifi√©
               if any(non_p in url_pattern for non_p in common_non_product_patterns):
                   continue
               
               # Inclure si pattern produit identifi√©
               if any(p in url_pattern for p in common_product_patterns):
                   filtered_product_urls.append(url)
               else:
                   # Si pattern inconnu, utiliser discover_product_urls pour v√©rifier
                   # (mais seulement si on a peu d'URLs restantes pour √©viter trop de requ√™tes)
                   if len(all_product_urls) - sample_size < 100:
                       try:
                           html = get(url)
                           if html and len(html) > 1000:
                               discovered = discover_product_urls(html, base_url)
                               if url in discovered or any(url in d for d in discovered):
                                   filtered_product_urls.append(url)
                           wait_between_requests(0.2)
                       except:
                           pass
   else:
       # Si pas de patterns clairs, v√©rifier toutes les URLs avec discover_product_urls
       print(f"‚ö†Ô∏è Pas de patterns clairs, v√©rification compl√®te de toutes les URLs...")
       for url in all_product_urls[sample_size:]:
           try:
               html = get(url)
               if html and len(html) > 1000:
                   discovered = discover_product_urls(html, base_url)
                   if url in discovered or any(url in d for d in discovered):
                       filtered_product_urls.append(url)
               wait_between_requests(0.2)
           except:
               continue
   
   # D√©dupliquer final
   all_product_urls = list(set(filtered_product_urls))
   print(f"\\n‚úÖ FILTRAGE TERMIN√â: {{len(all_product_urls)}} URLs de produits confirm√©es (sur {{len(normalized_urls)}} URLs initiales)")
   
   # ‚ö†Ô∏è IMPORTANT: Si le filtrage a √©limin√© trop d'URLs, utiliser les URLs originales
   # (le filtrage peut √™tre trop strict sur certains sites)
   if len(all_product_urls) < len(normalized_urls) * 0.1:  # Si moins de 10% des URLs passent
       print(f"‚ö†Ô∏è Filtrage trop strict ({{len(all_product_urls)}}/{{len(normalized_urls)}}), utilisation des URLs originales")
       all_product_urls = list(normalized_urls.values())
       print(f"‚úÖ Utilisation de {{len(all_product_urls)}} URLs (filtrage assoupli)")
   
   if not all_product_urls:
       print("‚ùå Aucune URL de produit valide trouv√©e apr√®s filtrage!")
       return {{'companyInfo': {{}}, 'products': []}}
   ```
   
   ‚ö†Ô∏è CRITIQUE - NE PAS G√âN√âRER DE CHEMIN OPTIMIS√â:
   Les URLs sont D√âJ√Ä dans exploration_result['all_product_urls'].
   NE PAS g√©n√©rer ou sauvegarder de chemin optimis√©.
   NE PAS appeler save_json('optimized_path', ...).
   Les URLs sont d√©j√† d√©couvertes, d√©dupliqu√©es et filtr√©es par l'AI Agent.
   
√âTAPE 2: R√âCUP√âRATION DU HTML
   ‚ö†Ô∏è CRITIQUE: Cette √©tape r√©cup√®re le HTML de TOUTES les URLs pr√©-d√©couvertes dans exploration_result.
   La m√©thode de r√©cup√©ration est standardis√©e et toujours la m√™me.
   
   ‚ö†Ô∏è EXIGENCE D'EXPLICITE (0% AMBIVALENT):
   - Utiliser 'get' par d√©faut (les URLs sont d√©j√† filtr√©es, pas besoin de smart_get)
   - Montrer exactement comment r√©cup√©rer le HTML pour chaque URL
   
   Code OBLIGATOIRE:
```python
   print(f"\\n{'='*60}")
   print(f"üì• R√âCUP√âRATION DU HTML")
   print(f"{'='*60}")
   print(f"‚úÖ {{len(all_product_urls)}} URLs √† traiter")
   
   # R√©cup√©rer le HTML de chaque URL
   pages_html_dict = {{}}  # Dictionnaire URL -> HTML
   
   # ‚ö†Ô∏è EXPLICITE: Utiliser 'get' par d√©faut (les URLs sont d√©j√† filtr√©es)
   html_retrieval_method = 'get'
   print(f"   M√©thode de r√©cup√©ration: {{html_retrieval_method}} (par d√©faut)")
   print(f"   Raison: Les URLs sont d√©j√† filtr√©es par l'AI Agent, utilisation de get() standard")
   
   for idx, url in enumerate(all_product_urls, 1):
       print(f"   üì• {{idx}}/{{len(all_product_urls)}}: {{url[:80]}}...")
       
       # ‚ö†Ô∏è EXPLICITE: Utiliser la m√©thode sp√©cifi√©e avec explication
       if html_retrieval_method == 'browser_get':
           # M√©thode browser_get: n√©cessaire si le site utilise JavaScript pour charger le contenu
           html = browser_get(url)
       elif html_retrieval_method == 'smart_get':
           # M√©thode smart_get: essaie get() d'abord, puis browser_get() si n√©cessaire
           result = smart_get(url, max_retries=3)
           html = result.get('html', '')
       else:  # 'get' par d√©faut
           # M√©thode get(): pour sites statiques sans JavaScript
           html = get(url)
       
       if html:
           # ‚ö†Ô∏è CRITIQUE: Pr√©parer le HTML pour insertion s√©curis√©e dans le prompt
           # prepare_html_for_prompt() nettoie les surrogates, √©chappe les accolades, remplace triple backticks
           html = ai_tools.prepare_html_for_prompt(html)
           pages_html_dict[url] = html
       
       wait_between_requests(0.3)  # Attendre entre requ√™tes
   
   print(f"‚úÖ {{len(pages_html_dict)}} pages HTML r√©cup√©r√©es et nettoy√©es")
   ```

√âTAPE 3: EXTRACTION LOCALE (SANS GEMINI)
   ‚ö†Ô∏è CRITIQUE: Cette √©tape fait l'extraction LOCALEMENT avec BeautifulSoup - SANS utiliser Gemini.
   Utilise les s√©lecteurs CSS HARDCOD√âS comme m√©thode principale, avec fallback sur patterns g√©n√©riques.
   
   ‚ö†Ô∏è IMPORTANT: Le scraper g√©n√©r√© DOIT utiliser les s√©lecteurs HARDCOD√âS dans le dictionnaire SELECTORS.
   Les s√©lecteurs doivent √™tre hardcod√©s directement dans le script, pas r√©cup√©r√©s depuis field_mappings au runtime.
   Si les s√©lecteurs hardcod√©s √©chouent, utiliser des patterns g√©n√©riques (comme dans extract.py).
   
   ‚ö†Ô∏è EXIGENCE D'EXPLICITE (0% AMBIVALENT):
   Chaque √©tape d'extraction doit √™tre EXPLICITE et d√©taill√©e:
   - Indiquer clairement quelle m√©thode utiliser pour chaque champ (JSON-LD, fieldMappings, ou patterns g√©n√©riques)
   - Montrer exactement quel s√©lecteur CSS utiliser pour chaque champ
   - Montrer exactement comment extraire la valeur (get_text_content, parse_html, BeautifulSoup.select_one, etc.)
   - Ne pas utiliser de fonctions g√©n√©riques comme "extract_product_data" - montrer le code d'extraction complet
   - Pour chaque URL, montrer exactement o√π aller chercher les donn√©es dans le HTML
   
   Code OBLIGATOIRE (√† ex√©cuter apr√®s √âTAPE 2) avec s√©lecteurs HARDCOD√âS:
```python
   print(f"\\n{{'='*60}}")
   print(f"üîç √âTAPE 3: EXTRACTION AVEC S√âLECTEURS HARDCOD√âS")
   print(f"{'='*60}")
   print(f"‚úÖ {{len(pages_html_dict)}} pages HTML √† extraire")
   
   all_products = []
   
   # ‚ö†Ô∏è CRITIQUE: S√©lecteurs HARDCOD√âS directement dans le script
   # Utiliser field_mappings['products'] pour remplir ce dictionnaire lors de la g√©n√©ration
   SELECTORS = {{
       'name': 'h1.product-title',
       'prix': '.price',
       'image': 'img.product-image::attr(src)',
       # ... TOUS les s√©lecteurs de field_mappings['products'] doivent √™tre ici
   }}
   
   # Pour chaque page, extraire avec les s√©lecteurs CSS d√©tect√©s
   # ‚ö†Ô∏è EXPLICITE: Montrer exactement comment extraire chaque champ pour chaque URL
   for url, html in pages_html_dict.items():
       print(f"   üîç Extraction: {{url[:60]}}...")
       
       product = {{}}
       
       # 3.1: Essayer JSON-LD d'abord (le plus fiable)
       # ‚ö†Ô∏è EXPLICITE: Montrer exactement comment extraire depuis JSON-LD
       json_ld_data = extract_json_ld(html)
       product_extracted = False
       
       if json_ld_data and isinstance(json_ld_data, list):
           for item in json_ld_data:
               if item.get('@type') in ['Product', 'Vehicle', 'Motorcycle', 'Car']:
                   # Extraction EXPLICITE de chaque champ depuis JSON-LD
                   product['name'] = item.get('name', '')
                   product['description'] = item.get('description', '')
                   if 'offers' in item:
                       product['prix'] = item['offers'].get('price', '')
                   product['image'] = item.get('image', '')
                   product['marque'] = item.get('brand', {{}}).get('name', '') if isinstance(item.get('brand'), dict) else item.get('brand', '')
                   product['sourceUrl'] = url
                   
                   if product.get('name'):
                       all_products.append(product)
                       product_extracted = True
                       print(f"      ‚úÖ Produit extrait via JSON-LD: {{product.get('name', 'Unknown')[:50]}}")
                       break
       
       # 3.2: Si JSON-LD √©choue, utiliser fieldMappings pour extraction CSS directe
       # ‚ö†Ô∏è EXPLICITE: Montrer exactement quel s√©lecteur CSS utiliser pour chaque champ
       if not product_extracted:
           extraction_success = False
           
           # Extraire chaque champ avec les s√©lecteurs CSS HARDCOD√âS
           # ‚ö†Ô∏è EXPLICITE: Pour chaque champ, montrer le s√©lecteur exact et la m√©thode d'extraction
           for field, selector in SELECTORS.items():
               if selector:
                   # Utiliser parse_html pour extraire avec le s√©lecteur CSS
                   elements = parse_html(html, selector)
                   if elements:
                       # Si le s√©lecteur contient ::attr(), extraire l'attribut
                       # Utiliser find() pour √©viter les probl√®mes de syntaxe avec les parenth√®ses
                       attr_marker = '::attr'
                       if attr_marker in selector:
                           # Extraire le nom de l'attribut entre ::attr( et )
                           start_idx = selector.find(attr_marker) + len(attr_marker) + 1
                           end_idx = selector.find(')', start_idx)
                           if end_idx > start_idx:
                               attr_name = selector[start_idx:end_idx]
                               value = elements[0].get(attr_name, '') if hasattr(elements[0], 'get') else ''
                           else:
                               value = get_text_content(html, selector)
                       else:
                           value = get_text_content(html, selector)
                       
                       if value:
                           product[field] = value
                           extraction_success = True
           
           # Si extraction CSS r√©ussie, ajouter le produit
           if extraction_success and product.get('name'):
               product['sourceUrl'] = url
               all_products.append(product)
               print(f"      ‚úÖ Produit extrait via CSS (fieldMappings): {{product.get('name', 'Unknown')[:50]}}")
           else:
               # 3.3: Fallback: Utiliser patterns g√©n√©riques (extraction locale sans Gemini)
               # ‚ö†Ô∏è EXPLICITE: Montrer exactement quels s√©lecteurs CSS g√©n√©riques utiliser
               print(f"      ‚ö†Ô∏è Extraction CSS √©chou√©e, fallback patterns g√©n√©riques...")
               
               # Extraction avec patterns g√©n√©riques (comme dans extract.py)
               soup = BeautifulSoup(html, 'html.parser')
               
               # Chercher le nom - EXPLICITE: s√©lecteurs CSS exacts
               name_elem = soup.select_one('h1, h2, h3, .title, .name, [class*="title"], [class*="name"]')
               if name_elem:
                   product['name'] = name_elem.get_text(strip=True)
               
               # Chercher le prix - EXPLICITE: s√©lecteurs CSS exacts
               price_elem = soup.select_one('.price, .prix, [class*="price"], [class*="prix"]')
               if price_elem:
                   price_text = price_elem.get_text(strip=True)
                   price = extract_price(price_text)
                   if price:
                       product['prix'] = price
               
               # Chercher la description - EXPLICITE: s√©lecteurs CSS exacts
               desc_elem = soup.select_one('.description, .desc, [class*="description"], [class*="desc"]')
               if desc_elem:
                   product['description'] = desc_elem.get_text(strip=True)[:500]
               
               # Chercher l'image - EXPLICITE: s√©lecteur CSS exact
               img = soup.select_one('img')
               if img and img.get('src'):
                   product['image'] = urljoin(base_url, img['src'])
               
               # Extraire ann√©e depuis le nom/description - EXPLICITE: regex exact
               name_desc = (product.get('name', '') + ' ' + product.get('description', '')).lower()
               year_match = re.search(r'\\b(19|20)\\d{{2}}\\b', name_desc)
               if year_match:
                   try:
                       year = int(year_match.group(0))
                       if 1900 <= year <= 2100:
                           product['annee'] = year
                   except:
                       pass
               
               # Extraire marque et mod√®le depuis le nom - EXPLICITE: patterns regex exacts
               name = product.get('name', '')
               if name:
                   brand_patterns = [
                       r'^(Kawasaki|Honda|Yamaha|Suzuki|Arctic Cat|Polaris|Can-Am|BRP|KTM|Ducati|BMW|Harley-Davidson)',
                       r'\\b(Kawasaki|Honda|Yamaha|Suzuki|Arctic Cat|Polaris|Can-Am|BRP|KTM|Ducati|BMW|Harley-Davidson)\\b'
                   ]
                   for pattern in brand_patterns:
                       match = re.search(pattern, name, re.I)
                       if match:
                           product['marque'] = match.group(1)
                           model = name.replace(match.group(1), '').strip()
                           if model:
                               product['modele'] = model.split()[0] if model.split() else model[:50]
                           break
               
               # Ajouter le produit si au moins le nom est pr√©sent
               if product.get('name') and len(product.get('name', '')) >= 3:
                   product['sourceUrl'] = url
                   all_products.append(product)
                   print(f"      ‚úÖ Produit extrait via patterns g√©n√©riques: {{product.get('name', 'Unknown')[:50]}}")
               else:
                   print(f"      ‚ùå Aucun produit extrait (nom manquant ou trop court)")
       
       wait_between_requests(0.2)
   
   print(f"\\n‚úÖ {{len(all_products)}} produits extraits au total (extraction locale sans Gemini)")
   ```

√âTAPE 4: VALIDATION, STANDARDISATION ET RETOUR
   ‚ö†Ô∏è NOUVEAU: Validation automatique avec d√©tection d'anomalies et auto-correction
   
   Code OBLIGATOIRE:
   ```python
   # Valider et standardiser tous les produits
   validated_products = []
   anomalies_found = []
   
   for product in all_products:
       # Standardiser tous les champs
       for field, value in product.items():
           product[field] = standardize_field(field, value)
       
       # Valider et d√©tecter anomalies
       validation = validate_product_data(product)
       
       if validation['is_valid']:
           # Appliquer corrections automatiques
           product.update(validation.get('corrected', {{}}))
           validated_products.append(product)
           
           # Logger anomalies si pr√©sentes
           if validation.get('anomalies'):
               anomalies_found.extend(validation['anomalies'])
               print(f"‚ö†Ô∏è Anomalies d√©tect√©es pour {{product.get('name', 'Unknown')}}: {{validation['anomalies']}}")
       else:
           print(f"‚ùå Produit rejet√© (champs manquants: {{validation['missing_fields']}}): {{product.get('name', 'Unknown')}}")
   
   # Rapport final
   print(f"‚úÖ {{len(validated_products)}} produits valid√©s sur {{len(all_products)}}")
   if anomalies_found:
       print(f"‚ö†Ô∏è {{len(anomalies_found)}} anomalies d√©tect√©es (v√©rifier manuellement)")
   
   # Retourner au format EXTRACTION_SCHEMA
   return {{
       'companyInfo': {{}},
       'products': validated_products
   }}
   ```

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
5. STRUCTURE DU CODE
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

SIGNATURE OBLIGATOIRE:
```python
def scrape(base_url):
    \"\"\"
    Scraper g√©n√©r√© pour {url}
    IMPORTANT: gemini_client et session sont disponibles globalement.
    NE PAS les passer en param√®tres.
    \"\"\"
    # Code ici
```

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
6. EXEMPLE COMPLET DE R√âF√âRENCE (CHECKLIST DE V√âRIFICATION)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚ö†Ô∏è IMPORTANT: Utilise cet exemple comme REFERENCE et CHECKLIST pour v√©rifier que ton scraper est complet.

STRUCTURE ATTENDUE DU SCRAPER G√âN√âR√â:

```python
def scrape(base_url):
    \"\"\"
    Scraper g√©n√©r√© pour {url}
    IMPORTANT: gemini_client et session sont disponibles globalement.
    NE PAS les passer en param√®tres.
    \"\"\"
    
    # ============================================================
    # √âTAPE 0: INITIALISATION
    # ============================================================
    print(f"\\n{{'='*60}}")
    print(f"üöÄ D√âMARRAGE DU SCRAPER")
    print(f"{{'='*60}}")
    print(f"üåê URL: {{base_url}}")
    
    # ============================================================
    # √âTAPE 1: UTILISATION DES URLs PR√â-D√âCOUVERTES PAR L'AI AGENT
    # ============================================================
    print(f"\\n{{'='*60}}")
    print(f"üìç √âTAPE 1: UTILISATION DES URLs PR√â-D√âCOUVERTES")
    print(f"{{'='*60}}")
    
    # ‚ö†Ô∏è CRITIQUE: URLs HARDCOD√âES directement dans le script
    # L'AI Agent a explor√© le site et d√©couvert toutes les URLs - elles sont maintenant hardcod√©es ici
    # NE PAS utiliser exploration_result au runtime - les URLs sont dans PRODUCT_URLS
    PRODUCT_URLS = [
        "https://site.com/product1",
        "https://site.com/product2",
        # ... TOUTES les URLs de exploration_result['all_product_urls'] doivent √™tre hardcod√©es ici
    ]
    
    if not PRODUCT_URLS:
        print("‚ùå Aucune URL de produit pr√©-d√©couverte par l'AI Agent")
        return {{'companyInfo': {{}}, 'products': []}}
    
    print(f"‚úÖ {{len(PRODUCT_URLS)}} URLs de produits pr√©-d√©couvertes (hardcod√©es dans le script)")
    print(f"   Exemples: {{PRODUCT_URLS[:3]}}")
    
    # ‚ö†Ô∏è CRITIQUE: S√©lecteurs CSS HARDCOD√âS directement dans le script
    # L'AI Agent a d√©tect√© les s√©lecteurs CSS - ils sont maintenant hardcod√©s ici
    # NE PAS utiliser field_mappings au runtime - les s√©lecteurs sont dans SELECTORS
    SELECTORS = {{
        'name': 'h1.product-title',
        'prix': '.price',
        'image': 'img.product-image::attr(src)',
        # ... TOUS les s√©lecteurs de field_mappings['products'] doivent √™tre hardcod√©s ici
    }}
    
    # ============================================================
    # √âTAPE 2: R√âCUP√âRATION DU HTML (SEULEMENT PAGES DE PRODUITS)
    # ============================================================
    print(f"\\n{{'='*60}}")
    print(f"üì• √âTAPE 2: R√âCUP√âRATION HTML")
    print(f"{{'='*60}}")
    
    pages_html_dict = {{}}
    # Utiliser 'get' par d√©faut (les URLs sont d√©j√† filtr√©es, pas besoin de smart_get)
    html_retrieval_method = 'get'
    
    for idx, url in enumerate(all_product_urls, 1):
        print(f"   üì• {{idx}}/{{len(all_product_urls)}}: {{url[:80]}}...")
        
        if html_retrieval_method == 'browser_get':
            html = browser_get(url)
        elif html_retrieval_method == 'smart_get':
            result = smart_get(url, max_retries=3)
            html = result.get('html', '')
        else:
            html = get(url)
        
        if html:
            html = clean_html(html)  # ‚ö†Ô∏è CRITIQUE: Nettoyer Unicode invalide
            pages_html_dict[url] = html
        
        wait_between_requests(0.3)
    
    print(f"‚úÖ {{len(pages_html_dict)}} pages HTML r√©cup√©r√©es et nettoy√©es")
    
    # ============================================================
    # √âTAPE 3: EXTRACTION LOCALE (SANS GEMINI)
    # ============================================================
    print(f"\\n{{'='*60}}")
    print(f"üîç √âTAPE 3: EXTRACTION LOCALE")
    print(f"{{'='*60}}")
    
    all_products = []
    
    # Utiliser fieldMappings pour extraction CSS directe
    product_mappings = field_mappings.get('products', {{}}) if 'field_mappings' in locals() else {{}}
    
    # Pour chaque page, extraire avec les s√©lecteurs CSS d√©tect√©s
    for url, html in pages_html_dict.items():
        print(f"   üîç Extraction: {{url[:60]}}...")
        
        # Essayer JSON-LD d'abord (le plus fiable)
        json_ld_data = extract_json_ld(html)
        product_extracted = False
        
        if json_ld_data and isinstance(json_ld_data, list):
            for item in json_ld_data:
                if item.get('@type') in ['Product', 'Vehicle', 'Motorcycle', 'Car']:
                    product = {{}}
                    product['name'] = item.get('name', '')
                    product['description'] = item.get('description', '')
                    if 'offers' in item:
                        product['prix'] = item['offers'].get('price', '')
                    product['image'] = item.get('image', '')
                    product['marque'] = item.get('brand', {{}}).get('name', '') if isinstance(item.get('brand'), dict) else item.get('brand', '')
                    product['sourceUrl'] = url
                    
                    if product.get('name'):
                        all_products.append(product)
                        product_extracted = True
                        print(f"      ‚úÖ Produit extrait via JSON-LD: {{product.get('name', 'Unknown')[:50]}}")
                        break
        
        # Si JSON-LD √©choue, utiliser fieldMappings pour extraction CSS directe
        if not product_extracted:
            product = {{}}
            extraction_success = False
            
            # Extraire chaque champ avec les s√©lecteurs CSS d√©tect√©s
            for field, selector in product_mappings.items():
                if selector:
                    elements = parse_html(html, selector)
                    if elements:
                        # Si le s√©lecteur contient ::attr(), extraire l'attribut
                        # Utiliser find() pour √©viter les probl√®mes de syntaxe avec les parenth√®ses
                        attr_marker = '::attr'
                        if attr_marker in selector:
                            # Extraire le nom de l'attribut entre ::attr( et )
                            start_idx = selector.find(attr_marker) + len(attr_marker) + 1
                            end_idx = selector.find(')', start_idx)
                            if end_idx > start_idx:
                                attr_name = selector[start_idx:end_idx]
                                value = elements[0].get(attr_name, '') if hasattr(elements[0], 'get') else ''
                            else:
                                value = get_text_content(html, selector)
                        else:
                            value = get_text_content(html, selector)
                        
                        if value:
                            product[field] = value
                            extraction_success = True
            
            # Si extraction CSS r√©ussie, ajouter le produit
            if extraction_success and product.get('name'):
                product['sourceUrl'] = url
                all_products.append(product)
                print(f"      ‚úÖ Produit extrait via CSS (fieldMappings): {{product.get('name', 'Unknown')[:50]}}")
            else:
                # Fallback: Utiliser patterns g√©n√©riques (extraction locale sans Gemini)
                soup = BeautifulSoup(html, 'html.parser')
                
                # Chercher le nom
                name_elem = soup.select_one('h1, h2, h3, .title, .name, [class*="title"], [class*="name"]')
                if name_elem:
                    product['name'] = name_elem.get_text(strip=True)
                
                # Chercher le prix
                price_elem = soup.select_one('.price, .prix, [class*="price"], [class*="prix"]')
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    price = extract_price(price_text)
                    if price:
                        product['prix'] = price
                
                # Chercher la description
                desc_elem = soup.select_one('.description, .desc, [class*="description"], [class*="desc"]')
                if desc_elem:
                    product['description'] = desc_elem.get_text(strip=True)[:500]
                
                # Chercher l'image
                img = soup.select_one('img')
                if img and img.get('src'):
                    product['image'] = urljoin(base_url, img['src'])
                
                # Extraire ann√©e depuis le nom/description
                name_desc = (product.get('name', '') + ' ' + product.get('description', '')).lower()
                year_match = re.search(r'\\b(19|20)\\d{{2}}\\b', name_desc)
                if year_match:
                    try:
                        year = int(year_match.group(0))
                        if 1900 <= year <= 2100:
                            product['annee'] = year
                    except:
                        pass
                
                # Extraire marque et mod√®le depuis le nom
                name = product.get('name', '')
                if name:
                    brand_patterns = [
                        r'^(Kawasaki|Honda|Yamaha|Suzuki|Arctic Cat|Polaris|Can-Am|BRP|KTM|Ducati|BMW|Harley-Davidson)',
                        r'\\b(Kawasaki|Honda|Yamaha|Suzuki|Arctic Cat|Polaris|Can-Am|BRP|KTM|Ducati|BMW|Harley-Davidson)\\b'
                    ]
                    for pattern in brand_patterns:
                        match = re.search(pattern, name, re.I)
                        if match:
                            product['marque'] = match.group(1)
                            model = name.replace(match.group(1), '').strip()
                            if model:
                                product['modele'] = model.split()[0] if model.split() else model[:50]
                            break
                
                # Ajouter le produit si au moins le nom est pr√©sent
                if product.get('name') and len(product.get('name', '')) >= 3:
                    product['sourceUrl'] = url
                    all_products.append(product)
                    print(f"      ‚úÖ Produit extrait via patterns g√©n√©riques: {{product.get('name', 'Unknown')[:50]}}")
                else:
                    print(f"      ‚ùå Aucun produit extrait (nom manquant ou trop court)")
    
    print(f"‚úÖ {{len(all_products)}} produits extraits au total (extraction locale sans Gemini)")
    
    # ============================================================
    # √âTAPE 4: VALIDATION ET STANDARDISATION
    # ============================================================
    print(f"\\n{{'='*60}}")
    print(f"‚úÖ √âTAPE 4: VALIDATION")
    print(f"{{'='*60}}")
    
    validated_products = []
    anomalies_found = []
    
    for product in all_products:
        # Standardiser tous les champs
        for field, value in product.items():
            product[field] = standardize_field(field, value)
        
        # Valider et d√©tecter anomalies
        validation = validate_product_data(product)
        
        if validation['is_valid']:
            product.update(validation.get('corrected', {{}}))
            validated_products.append(product)
            
            if validation.get('anomalies'):
                anomalies_found.extend(validation['anomalies'])
        else:
            print(f"‚ùå Produit rejet√©: {{product.get('name', 'Unknown')}}")
    
    print(f"‚úÖ {{len(validated_products)}} produits valid√©s sur {{len(all_products)}}")
    
    # ============================================================
    # √âTAPE 5: RETOUR DES R√âSULTATS
    # ============================================================
    return {{
        'companyInfo': {{}},
        'products': validated_products
    }}
```

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
7. CHECKLIST DE V√âRIFICATION (√Ä UTILISER APR√àS G√âN√âRATION)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚ö†Ô∏è AVANT DE RETOURNER LE SCRAPER, V√âRIFIE QUE:

‚úÖ STRUCTURE:
   [ ] La fonction s'appelle bien `scrape(base_url)`
   [ ] Pas de param√®tres suppl√©mentaires (gemini_client, session)
   [ ] Docstring pr√©sente avec URL du site

‚úÖ √âTAPE 1 - UTILISATION URLs PR√â-D√âCOUVERTES:
   [ ] Utilise `exploration_result['all_product_urls']` directement
   [ ] NE PAS red√©couvrir les URLs (d√©j√† fait par l'AI Agent)
   [ ] V√©rifie si aucune URL pr√©-d√©couverte (retourne vide)
   [ ] Logs pour indiquer le nombre d'URLs pr√©-d√©couvertes

‚úÖ √âTAPE 2 - R√âCUP√âRATION HTML:
   [ ] R√©cup√®re HTML SEULEMENT sur les URLs filtr√©es (pages de produits)
   [ ] Utilise `html_retrieval_method` du chemin optimis√© (si disponible)
   [ ] Appelle `clean_html()` sur chaque HTML r√©cup√©r√©
   [ ] Utilise `wait_between_requests(0.3)` entre requ√™tes
   [ ] G√®re les erreurs (si html vide, skip)
   [ ] Ne r√©cup√®re PAS les URLs de cat√©gories ou pages d'accueil

‚úÖ √âTAPE 3 - EXTRACTION LOCALE:
   [ ] Utilise fieldMappings pour extraction CSS directe
   [ ] Fallback sur patterns g√©n√©riques si fieldMappings √©chouent
   [ ] Utilise BeautifulSoup pour parsing HTML
   [ ] Extrait nom, prix, description, image, ann√©e, marque, mod√®le
   [ ] N'utilise JAMAIS gemini_client.call() (extraction locale uniquement)

‚úÖ √âTAPE 4 - VALIDATION:
   [ ] Appelle `standardize_field()` pour chaque champ
   [ ] Appelle `validate_product_data()` pour chaque produit
   [ ] Applique les corrections automatiques
   [ ] Rejette les produits invalides avec log
   [ ] Compte les anomalies d√©tect√©es

‚úÖ √âTAPE 5 - RETOUR:
   [ ] Retourne au format EXTRACTION_SCHEMA
   [ ] Structure: {{'companyInfo': {{}}, 'products': [...]}}
   [ ] Tous les produits sont valid√©s

‚úÖ GESTION ERREURS:
   [ ] Try/except pour les op√©rations critiques
   [ ] V√©rifie si HTML vide avant traitement
   [ ] V√©rifie si aucune URL trouv√©e (retourne vide)
   [ ] Logs avec emojis (‚úÖ ‚ö†Ô∏è ‚ùå) pour clart√©

‚úÖ ANTI-HALLUCINATIONS:
   [ ] N'invente JAMAIS de donn√©es qui ne sont pas dans le HTML
   [ ] Si un champ n'est pas trouv√©, laisse-le vide ou None
   [ ] N'extrait que ce qui est r√©ellement pr√©sent dans le HTML
   [ ] Utilise les outils (extract_price, etc.) au lieu d'inventer

‚úÖ OPTIMISATIONS:
   [ ] Utilise directement exploration_result['all_product_urls'] (d√©j√† optimis√© par l'AI Agent)
   [ ] Traite par lots si contenu volumineux
   [ ] Nettoie le HTML avant extraction locale
   [ ] √âvite les requ√™tes inutiles

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
8. EXEMPLE COMPLET (VERSION SIMPLIFI√âE POUR R√âF√âRENCE)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚ö†Ô∏è CRITIQUE: Cet exemple montre la BONNE approche - utiliser exploration_result directement.

EXEMPLE COMPLET:
```python
def scrape(base_url):
    \"\"\"
    Scraper g√©n√©r√© pour {url}
    IMPORTANT: gemini_client et session sont disponibles globalement.
    \"\"\"
    
    # ============================================================
    # √âTAPE 1: UTILISATION DES URLs PR√â-D√âCOUVERTES PAR L'AI AGENT
    # ============================================================
    print(f"\\n{{'='*60}}")
    print(f"üìç √âTAPE 1: UTILISATION DES URLs PR√â-D√âCOUVERTES")
    print(f"{{'='*60}}")
    
    # ‚ö†Ô∏è CRITIQUE: Les URLs ont D√âJ√Ä √©t√© d√©couvertes par l'AI Agent avant la g√©n√©ration de ce script
    # L'AI Agent a explor√© le site, trouv√© le sitemap, parcouru la pagination, et d√©dupliqu√© toutes les URLs
    # NE PAS red√©couvrir les URLs - utiliser directement exploration_result['all_product_urls']
    
    # R√©cup√©rer les URLs d√©j√† d√©couvertes par l'AI Agent
    all_product_urls = exploration_result.get('all_product_urls', [])
    
    if not all_product_urls:
        print("‚ùå Aucune URL de produit pr√©-d√©couverte par l'AI Agent")
        print("   Le scraper ne peut pas fonctionner sans URLs pr√©-d√©couvertes")
        return {{'companyInfo': {{}}, 'products': []}}
    
    print(f"‚úÖ {{len(all_product_urls)}} URLs de produits pr√©-d√©couvertes (d√©j√† d√©dupliqu√©es)")
    print(f"   Exemples: {{all_product_urls[:3]}}")
    print(f"   ‚ö†Ô∏è IMPORTANT: Ces URLs sont D√âJ√Ä d√©dupliqu√©es et filtr√©es par l'AI Agent")
    print(f"   ‚ö†Ô∏è NE PAS appeler get_sitemap_urls(), detect_pagination(), ou discover_product_urls()")
    print(f"   ‚ö†Ô∏è Passer directement √† l'√âTAPE 2 (r√©cup√©ration HTML)")
    
    # ============================================================
    # √âTAPE 2: R√âCUP√âRATION DU HTML (SEULEMENT PAGES DE PRODUITS)
    # ============================================================
    print(f"\\n{{'='*60}}")
    print(f"üì• √âTAPE 2: R√âCUP√âRATION HTML")
    print(f"{{'='*60}}")
    
    pages_html_dict = {{}}
    html_retrieval_method = 'get'
    
    for idx, url in enumerate(all_product_urls, 1):
        print(f"   üì• {{idx}}/{{len(all_product_urls)}}: {{url[:80]}}...")
        
        if html_retrieval_method == 'browser_get':
            html = browser_get(url)
        elif html_retrieval_method == 'smart_get':
            result = smart_get(url, max_retries=3)
            html = result.get('html', '')
        else:
            html = get(url)
        
        if html:
            html = clean_html(html)  # ‚ö†Ô∏è CRITIQUE: Nettoyer Unicode invalide
            pages_html_dict[url] = html
        
        wait_between_requests(0.3)
    
    print(f"‚úÖ {{len(pages_html_dict)}} pages HTML r√©cup√©r√©es et nettoy√©es")
    
    # ============================================================
    # √âTAPE 3: EXTRACTION LOCALE (SANS GEMINI)
    # ============================================================
    print(f"\\n{{'='*60}}")
    print(f"üîç √âTAPE 3: EXTRACTION LOCALE")
    print(f"{{'='*60}}")
    
    all_products = []
    product_mappings = field_mappings.get('products', {{}}) if 'field_mappings' in locals() else {{}}
    
    for url, html in pages_html_dict.items():
        print(f"   üîç Extraction: {{url[:60]}}...")
        
        # Essayer JSON-LD d'abord
        json_ld_data = extract_json_ld(html)
        product_extracted = False
        
        if json_ld_data and isinstance(json_ld_data, list):
            for item in json_ld_data:
                if item.get('@type') in ['Product', 'Vehicle', 'Motorcycle', 'Car']:
                    product = {{}}
                    product['name'] = item.get('name', '')
                    product['description'] = item.get('description', '')
                    if 'offers' in item:
                        product['prix'] = item['offers'].get('price', '')
                    product['image'] = item.get('image', '')
                    product['marque'] = item.get('brand', {{}}).get('name', '') if isinstance(item.get('brand'), dict) else item.get('brand', '')
                    product['sourceUrl'] = url
                    
                    if product.get('name'):
                        all_products.append(product)
                        product_extracted = True
                        break
        
        # Si JSON-LD √©choue, utiliser fieldMappings
        if not product_extracted:
            product = {{}}
            extraction_success = False
            
            for field, selector in product_mappings.items():
                if selector:
                    elements = parse_html(html, selector)
                    if elements:
                        value = get_text_content(html, selector)
                        if value:
                            product[field] = value
                            extraction_success = True
            
            if extraction_success and product.get('name'):
                product['sourceUrl'] = url
                all_products.append(product)
            else:
                # Fallback: patterns g√©n√©riques
                soup = BeautifulSoup(html, 'html.parser')
                name_elem = soup.select_one('h1, h2, h3, .title, .name')
                if name_elem:
                    product['name'] = name_elem.get_text(strip=True)
                price_elem = soup.select_one('.price, .prix, [class*="price"]')
                if price_elem:
                    price = extract_price(price_elem.get_text(strip=True))
                    if price:
                        product['prix'] = price
                if product.get('name'):
                    product['sourceUrl'] = url
                    all_products.append(product)
    
    # ============================================================
    # √âTAPE 4: VALIDATION
    # ============================================================
    print(f"\\n{{'='*60}}")
    print(f"‚úÖ √âTAPE 4: VALIDATION")
    print(f"{{'='*60}}")
    
    validated_products = []
    for product in all_products:
        for field, value in product.items():
            product[field] = standardize_field(field, value)
        validation = validate_product_data(product)
        if validation['is_valid']:
            product.update(validation.get('corrected', {{}}))
            validated_products.append(product)
    
    print(f"‚úÖ {{len(validated_products)}} produits valid√©s sur {{len(all_products)}}")
    
    return {{'companyInfo': {{}}, 'products': validated_products}}
```
            if not html or len(html) < 1000:
                break
            product_links = discover_product_urls(html, base_url)
            if not product_links:
                break
            for url in product_links:
                add_url_with_dedup(url)  # ‚ö†Ô∏è D√©duplication imm√©diate
            print(f"   Page {{page}}: {{len(product_links)}} produits (Total unique: {{len(normalized_urls_dict)}})")
            page += 1
    
    # Les URLs sont d√©j√† d√©dupliqu√©es dans normalized_urls_dict
    all_product_urls = list(normalized_urls_dict.values())
    print(f"‚úÖ TOTAL: {{len(all_product_urls)}} URLs trouv√©es")
    
    # Si d√©couverte compl√®te, g√©n√©rer et sauvegarder le chemin optimis√©
    all_product_urls = list(normalized_urls_dict.values())
    if not optimized_path and all_product_urls:
        # G√©n√©rer le chemin optimis√© (SIMPLIFI√â: seulement chemin vers produits + m√©thode HTML)
        if sitemap_urls and len(sitemap_urls) > 10:
            optimized_path = {{
                'sitemap_url': base_url,  # Chemin pour trouver les URLs
                'html_retrieval_method': 'get'  # Chemin pour r√©cup√©rer les infos
            }}
        elif pagination_info:
            optimized_path = {{
                'pagination_info': pagination_info,  # Chemin pour trouver les URLs
                'html_retrieval_method': 'get'  # Chemin pour r√©cup√©rer les infos
            }}
        else:
            optimized_path = {{
                'sitemap_url': base_url,
                'html_retrieval_method': 'get'
            }}
        save_json('optimized_path', optimized_path)
        print(f"‚úÖ Chemin optimis√© sauvegard√© pour les prochains scrapes")

    if not all_product_urls:
        return {{'companyInfo': {{}}, 'products': []}}

    # √âTAPE 2: R√©cup√©rer HTML (utiliser m√©thode du chemin optimis√© si disponible)
    print(f"\\nüì• R√©cup√©ration du HTML...")
    pages_html_dict = {{}}
    # Utiliser 'get' par d√©faut (les URLs sont d√©j√† filtr√©es, pas besoin de smart_get)
    html_retrieval_method = 'get'
    
    for idx, url in enumerate(all_product_urls, 1):
        print(f"   üì• {{idx}}/{{len(all_product_urls)}}: {{url[:80]}}...")
        html = get(url) if html_retrieval_method == 'get' else browser_get(url) if html_retrieval_method == 'browser_get' else smart_get(url, max_retries=3).get('html', '')
        if html:
            html = clean_html(html)  # Nettoyer caract√®res invalides
            pages_html_dict[url] = html
        wait_between_requests(0.3)
    
    print(f"‚úÖ {{len(pages_html_dict)}} pages HTML r√©cup√©r√©es")

    # √âTAPE 3: Extraction avec Gemini (m√©thode standardis√©e)
    print(f"\\nü§ñ Extraction avec Gemini...")
    pages_html = ""
    separator = "‚îÄ" * 60
    for url, html in pages_html_dict.items():
        # Pr√©parer le HTML avant insertion (s√©curise contre accolades, surrogates, etc.)
        html_prepared = ai_tools.prepare_html_for_prompt(html)
        pages_html += f"\\n{{separator}}\\nPAGE: {{url}}\\n{{separator}}\\n{{html_prepared}}\\n"
    
    prompt = f\"\"\"Extrais TOUS les v√©hicules motoris√©s depuis ces pages HTML.
    IMPORTANT: Extrais UNIQUEMENT les V√âHICULES INDIVIDUELS avec marque et mod√®le sp√©cifiques.
    Ignore les cat√©gories, les pages d'accueil, les pages de service.
    
    {{pages_html}}
    \"\"\"

    result = gemini_client.call(prompt, EXTRACTION_SCHEMA)
    products_count = len(result.get('products', []))
    print(f"‚úÖ {{products_count}} produits extraits")

    # √âTAPE 4: Retourner r√©sultats
    return result
```

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
6. EXEMPLE DE R√âF√âRENCE ET CHECKLIST DE V√âRIFICATION
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚ö†Ô∏è CRITIQUE: Utilise cet exemple comme REFERENCE et CHECKLIST pour v√©rifier que ton scraper est complet et correct.

STRUCTURE ATTENDUE DU SCRAPER G√âN√âR√â:

```python
def scrape(base_url):
    \"\"\"
    Scraper g√©n√©r√© pour {url}
    IMPORTANT: gemini_client et session sont disponibles globalement.
    NE PAS les passer en param√®tres.
    \"\"\"
    
    # ============================================================
    # √âTAPE 0: INITIALISATION
    # ============================================================
    print(f"\\n{{'='*60}}")
    print(f"üöÄ D√âMARRAGE DU SCRAPER")
    print(f"{{'='*60}}")
    print(f"üåê URL: {{base_url}}")
    
    # ============================================================
    # √âTAPE 1: UTILISATION DES URLs PR√â-D√âCOUVERTES PAR L'AI AGENT
    # ============================================================
    print(f"\\n{{'='*60}}")
    print(f"üìç √âTAPE 1: UTILISATION DES URLs PR√â-D√âCOUVERTES")
    print(f"{{'='*60}}")
    
    # ‚ö†Ô∏è CRITIQUE: Les URLs ont D√âJ√Ä √©t√© d√©couvertes par l'AI Agent avant la g√©n√©ration de ce script
    # L'AI Agent a explor√© le site, trouv√© le sitemap, parcouru la pagination, et d√©dupliqu√© toutes les URLs
    # NE PAS red√©couvrir les URLs - utiliser directement exploration_result['all_product_urls']
    
    # R√©cup√©rer les URLs d√©j√† d√©couvertes par l'AI Agent
    all_product_urls = exploration_result.get('all_product_urls', [])
    
    if not all_product_urls:
        print("‚ùå Aucune URL de produit pr√©-d√©couverte par l'AI Agent")
        print("   Le scraper ne peut pas fonctionner sans URLs pr√©-d√©couvertes")
        return {{'companyInfo': {{}}, 'products': []}}
    
    print(f"‚úÖ {{len(all_product_urls)}} URLs de produits pr√©-d√©couvertes (d√©j√† d√©dupliqu√©es)")
    print(f"   Exemples: {{all_product_urls[:3]}}")
    print(f"   ‚ö†Ô∏è IMPORTANT: Ces URLs sont D√âJ√Ä d√©dupliqu√©es et filtr√©es par l'AI Agent")
    print(f"   ‚ö†Ô∏è NE PAS appeler get_sitemap_urls(), detect_pagination(), ou discover_product_urls()")
    print(f"   ‚ö†Ô∏è Passer directement √† l'√âTAPE 2 (r√©cup√©ration HTML)")
    
    # ============================================================
    # √âTAPE 2: R√âCUP√âRATION DU HTML (SEULEMENT PAGES DE PRODUITS)
    # ============================================================
    print(f"\\n{{'='*60}}")
    print(f"üì• √âTAPE 2: R√âCUP√âRATION HTML")
    print(f"{{'='*60}}")
    
    pages_html_dict = {{}}
    # Utiliser 'get' par d√©faut (les URLs sont d√©j√† filtr√©es, pas besoin de smart_get)
    html_retrieval_method = 'get'
    
    for idx, url in enumerate(all_product_urls, 1):
        print(f"   üì• {{idx}}/{{len(all_product_urls)}}: {{url[:80]}}...")
        
        if html_retrieval_method == 'browser_get':
            html = browser_get(url)
        elif html_retrieval_method == 'smart_get':
            result = smart_get(url, max_retries=3)
            html = result.get('html', '')
        else:
            html = get(url)
        
        if html:
            html = clean_html(html)  # ‚ö†Ô∏è CRITIQUE: Nettoyer Unicode invalide
            pages_html_dict[url] = html
        
        wait_between_requests(0.3)
    
    print(f"‚úÖ {{len(pages_html_dict)}} pages HTML r√©cup√©r√©es et nettoy√©es")
    
    # ============================================================
    # √âTAPE 3: EXTRACTION LOCALE (SANS GEMINI)
    # ============================================================
    print(f"\\n{{'='*60}}")
    print(f"üîç √âTAPE 3: EXTRACTION LOCALE")
    print(f"{{'='*60}}")
    
    all_products = []
    
    # Utiliser fieldMappings pour extraction CSS directe
    product_mappings = field_mappings.get('products', {{}}) if 'field_mappings' in locals() else {{}}
    
    # Pour chaque page, extraire avec les s√©lecteurs CSS d√©tect√©s
    for url, html in pages_html_dict.items():
        print(f"   üîç Extraction: {{url[:60]}}...")
        
        # Essayer JSON-LD d'abord (le plus fiable)
        json_ld_data = extract_json_ld(html)
        product_extracted = False
        
        if json_ld_data and isinstance(json_ld_data, list):
            for item in json_ld_data:
                if item.get('@type') in ['Product', 'Vehicle', 'Motorcycle', 'Car']:
                    product = {{}}
                    product['name'] = item.get('name', '')
                    product['description'] = item.get('description', '')
                    if 'offers' in item:
                        product['prix'] = item['offers'].get('price', '')
                    product['image'] = item.get('image', '')
                    product['marque'] = item.get('brand', {{}}).get('name', '') if isinstance(item.get('brand'), dict) else item.get('brand', '')
                    product['sourceUrl'] = url
                    
                    if product.get('name'):
                        all_products.append(product)
                        product_extracted = True
                        print(f"      ‚úÖ Produit extrait via JSON-LD: {{product.get('name', 'Unknown')[:50]}}")
                        break
        
        # Si JSON-LD √©choue, utiliser fieldMappings pour extraction CSS directe
        if not product_extracted:
            product = {{}}
            extraction_success = False
            
            # Extraire chaque champ avec les s√©lecteurs CSS d√©tect√©s
            for field, selector in product_mappings.items():
                if selector:
                    elements = parse_html(html, selector)
                    if elements:
                        # Si le s√©lecteur contient ::attr(), extraire l'attribut
                        # Utiliser find() pour √©viter les probl√®mes de syntaxe avec les parenth√®ses
                        attr_marker = '::attr'
                        if attr_marker in selector:
                            # Extraire le nom de l'attribut entre ::attr( et )
                            start_idx = selector.find(attr_marker) + len(attr_marker) + 1
                            end_idx = selector.find(')', start_idx)
                            if end_idx > start_idx:
                                attr_name = selector[start_idx:end_idx]
                                value = elements[0].get(attr_name, '') if hasattr(elements[0], 'get') else ''
                            else:
                                value = get_text_content(html, selector)
                        else:
                            value = get_text_content(html, selector)
                        
                        if value:
                            product[field] = value
                            extraction_success = True
            
            # Si extraction CSS r√©ussie, ajouter le produit
            if extraction_success and product.get('name'):
                product['sourceUrl'] = url
                all_products.append(product)
                print(f"      ‚úÖ Produit extrait via CSS (fieldMappings): {{product.get('name', 'Unknown')[:50]}}")
            else:
                # Fallback: Utiliser patterns g√©n√©riques (extraction locale sans Gemini)
                soup = BeautifulSoup(html, 'html.parser')
                
                # Chercher le nom
                name_elem = soup.select_one('h1, h2, h3, .title, .name, [class*="title"], [class*="name"]')
                if name_elem:
                    product['name'] = name_elem.get_text(strip=True)
                
                # Chercher le prix
                price_elem = soup.select_one('.price, .prix, [class*="price"], [class*="prix"]')
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    price = extract_price(price_text)
                    if price:
                        product['prix'] = price
                
                # Chercher la description
                desc_elem = soup.select_one('.description, .desc, [class*="description"], [class*="desc"]')
                if desc_elem:
                    product['description'] = desc_elem.get_text(strip=True)[:500]
                
                # Chercher l'image
                img = soup.select_one('img')
                if img and img.get('src'):
                    product['image'] = urljoin(base_url, img['src'])
                
                # Extraire ann√©e depuis le nom/description
                name_desc = (product.get('name', '') + ' ' + product.get('description', '')).lower()
                year_match = re.search(r'\\b(19|20)\\d{{2}}\\b', name_desc)
                if year_match:
                    try:
                        year = int(year_match.group(0))
                        if 1900 <= year <= 2100:
                            product['annee'] = year
                    except:
                        pass
                
                # Extraire marque et mod√®le depuis le nom
                name = product.get('name', '')
                if name:
                    brand_patterns = [
                        r'^(Kawasaki|Honda|Yamaha|Suzuki|Arctic Cat|Polaris|Can-Am|BRP|KTM|Ducati|BMW|Harley-Davidson)',
                        r'\\b(Kawasaki|Honda|Yamaha|Suzuki|Arctic Cat|Polaris|Can-Am|BRP|KTM|Ducati|BMW|Harley-Davidson)\\b'
                    ]
                    for pattern in brand_patterns:
                        match = re.search(pattern, name, re.I)
                        if match:
                            product['marque'] = match.group(1)
                            model = name.replace(match.group(1), '').strip()
                            if model:
                                product['modele'] = model.split()[0] if model.split() else model[:50]
                            break
                
                # Ajouter le produit si au moins le nom est pr√©sent
                if product.get('name') and len(product.get('name', '')) >= 3:
                    product['sourceUrl'] = url
                    all_products.append(product)
                    print(f"      ‚úÖ Produit extrait via patterns g√©n√©riques: {{product.get('name', 'Unknown')[:50]}}")
                else:
                    print(f"      ‚ùå Aucun produit extrait (nom manquant ou trop court)")
    
    print(f"‚úÖ {{len(all_products)}} produits extraits au total (extraction locale sans Gemini)")
    
    # ============================================================
    # √âTAPE 4: VALIDATION ET STANDARDISATION
    # ============================================================
    print(f"\\n{{'='*60}}")
    print(f"‚úÖ √âTAPE 4: VALIDATION")
    print(f"{{'='*60}}")
    
    validated_products = []
    anomalies_found = []
    
    for product in all_products:
        # Standardiser tous les champs
        for field, value in product.items():
            product[field] = standardize_field(field, value)
        
        # Valider et d√©tecter anomalies
        validation = validate_product_data(product)
        
        if validation['is_valid']:
            product.update(validation.get('corrected', {{}}))
            validated_products.append(product)
            
            if validation.get('anomalies'):
                anomalies_found.extend(validation['anomalies'])
        else:
            print(f"‚ùå Produit rejet√©: {{product.get('name', 'Unknown')}}")
    
    print(f"‚úÖ {{len(validated_products)}} produits valid√©s sur {{len(all_products)}}")
    
    # ============================================================
    # √âTAPE 5: RETOUR DES R√âSULTATS
    # ============================================================
    return {{
        'companyInfo': {{}},
        'products': validated_products
    }}
```

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
7. CHECKLIST DE V√âRIFICATION (√Ä UTILISER APR√àS G√âN√âRATION)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚ö†Ô∏è AVANT DE RETOURNER LE SCRAPER, V√âRIFIE QUE:

‚úÖ STRUCTURE:
   [ ] La fonction s'appelle bien `scrape(base_url)`
   [ ] Pas de param√®tres suppl√©mentaires (gemini_client, session)
   [ ] Docstring pr√©sente avec URL du site

‚úÖ √âTAPE 1 - UTILISATION URLs PR√â-D√âCOUVERTES:
   [ ] Utilise `exploration_result['all_product_urls']` directement
   [ ] NE PAS red√©couvrir les URLs (d√©j√† fait par l'AI Agent)
   [ ] V√©rifie si aucune URL pr√©-d√©couverte (retourne vide)
   [ ] Logs pour indiquer le nombre d'URLs pr√©-d√©couvertes

‚úÖ √âTAPE 2 - R√âCUP√âRATION HTML:
   [ ] Utilise `html_retrieval_method = 'get'` par d√©faut (les URLs sont d√©j√† filtr√©es)
   [ ] Appelle `clean_html()` sur chaque HTML r√©cup√©r√©
   [ ] Utilise `wait_between_requests(0.3)` entre requ√™tes
   [ ] G√®re les erreurs (si html vide, skip)

‚úÖ √âTAPE 3 - EXTRACTION LOCALE:
   [ ] Utilise fieldMappings pour extraction CSS directe
   [ ] Fallback sur patterns g√©n√©riques si fieldMappings √©chouent
   [ ] Utilise BeautifulSoup pour parsing HTML
   [ ] Extrait nom, prix, description, image, ann√©e, marque, mod√®le
   [ ] N'utilise JAMAIS gemini_client.call() (extraction locale uniquement)

‚úÖ √âTAPE 4 - VALIDATION:
   [ ] Appelle `standardize_field()` pour chaque champ
   [ ] Appelle `validate_product_data()` pour chaque produit
   [ ] Applique les corrections automatiques
   [ ] Rejette les produits invalides avec log
   [ ] Compte les anomalies d√©tect√©es

‚úÖ √âTAPE 5 - RETOUR:
   [ ] Retourne au format EXTRACTION_SCHEMA
   [ ] Structure: {{'companyInfo': {{}}, 'products': [...]}}
   [ ] Tous les produits sont valid√©s

‚úÖ GESTION ERREURS:
   [ ] Try/except pour les op√©rations critiques
   [ ] V√©rifie si HTML vide avant traitement
   [ ] V√©rifie si aucune URL trouv√©e (retourne vide)
   [ ] Logs avec emojis (‚úÖ ‚ö†Ô∏è ‚ùå) pour clart√©

‚úÖ ANTI-HALLUCINATIONS:
   [ ] N'invente JAMAIS de donn√©es qui ne sont pas dans le HTML
   [ ] Si un champ n'est pas trouv√©, laisse-le vide ou None
   [ ] N'extrait que ce qui est r√©ellement pr√©sent dans le HTML
   [ ] Utilise les outils (extract_price, etc.) au lieu d'inventer

‚úÖ OPTIMISATIONS:
   [ ] Utilise directement exploration_result['all_product_urls'] (d√©j√† optimis√© par l'AI Agent)
   [ ] Traite par lots si contenu volumineux
   [ ] Nettoie le HTML avant extraction locale
   [ ] √âvite les requ√™tes inutiles

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

G√©n√®re un code COMPLET, FONCTIONNEL et PR√äT √Ä EX√âCUTER.
Utilise cet exemple comme r√©f√©rence et v√©rifie chaque point de la checklist.
"""

        try:
            result = self.gemini_client.call(
                prompt=prompt,
                schema=SCRAPER_GENERATION_SCHEMA,
                show_prompt=True
            )
            return result
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration du scraper: {e}")
            raise

    def get_scraper_for_site(self, url: str) -> Optional[Dict]:
        """R√©cup√®re un scraper depuis le cache"""
        return self._load_cached_scraper(url)

    def invalidate_cache(self, url: str) -> bool:
        """Invalide le cache pour un site donn√©"""
        cache_path = self._get_cache_path(url)
        if cache_path.exists():
            try:
                cache_path.unlink()
                print(f"üóëÔ∏è Cache invalid√© pour: {url}")
                return True
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors de l'invalidation du cache: {e}")
        return False
