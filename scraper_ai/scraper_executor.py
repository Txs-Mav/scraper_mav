"""
Module pour ex√©cuter les scrapers g√©n√©r√©s par Gemini
"""
import json
import re
from typing import Dict, List, Optional, Any
from pathlib import Path
from urllib.parse import urljoin, urlparse
import requests
from bs4 import BeautifulSoup
import time

import os

try:
    from .html_analyzer import HTMLAnalyzer
    from .config import EXTRACTION_SCHEMA
    from .gemini_client import GeminiClient
except ImportError:
    from html_analyzer import HTMLAnalyzer
    from config import EXTRACTION_SCHEMA
    from gemini_client import GeminiClient


class ScraperExecutor:
    """Ex√©cute les scrapers g√©n√©r√©s par Gemini"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.html_analyzer = HTMLAnalyzer()

    def fetch_html(self, url: str) -> str:
        """R√©cup√®re le contenu HTML d'une URL"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"‚ùå Erreur lors de la r√©cup√©ration de {url}: {e}")
            raise

    def execute_scraper(self, url: str, scraper_data: Dict,
                        reference_url: Optional[str] = None) -> Dict:
        """Ex√©cute un scraper g√©n√©r√© pour extraire les donn√©es

        Args:
            url: URL de base du site √† scraper
            scraper_data: Donn√©es du scraper g√©n√©r√© (depuis HTMLAnalyzer)
            reference_url: URL du site de r√©f√©rence pour comparaison de prix

        Returns:
            Dict au format EXTRACTION_SCHEMA avec companyInfo et products
        """
        print(f"\n{'='*60}")
        print(f"üöÄ EX√âCUTION DU SCRAPER G√âN√âR√â")
        print(f"{'='*60}")
        print(f"üåê URL: {url}\n")

        try:
            # Charger le code du scraper
            scraper_code = scraper_data.get('scraperCode', '')
            site_analysis = scraper_data.get('siteAnalysis', {})
            field_mappings = scraper_data.get('fieldMappings', {})
            # R√©cup√©rer exploration_result pour passer les URLs pr√©-d√©couvertes
            exploration_result = scraper_data.get(
                'metadata', {}).get('exploration_data', {})
            # Si pas dans metadata, chercher directement
            if not exploration_result:
                exploration_result = scraper_data.get('exploration_data', {})

            if not scraper_code:
                raise ValueError("Le scraper g√©n√©r√© ne contient pas de code")

            print(f"üìã Site analys√©: {site_analysis.get('siteName', 'N/A')}")
            print(
                f"üìÑ Type de structure: {site_analysis.get('structureType', 'N/A')}")

            pagination = site_analysis.get('paginationStrategy', {})
            if pagination:
                print(f"üìë Pagination: {pagination.get('type', 'none')}")
                if pagination.get('pattern'):
                    print(f"   Pattern: {pagination.get('pattern')}")

            # Cr√©er un namespace pour ex√©cuter le code du scraper
            # NOUVEAU FLUX : Scripts autonomes sans d√©pendance Gemini
            # Le script g√©n√©r√© est compl√®tement ind√©pendant et n'utilise pas Gemini

            # Imports de base (sans Gemini)
            from concurrent.futures import ThreadPoolExecutor, as_completed

            # Cr√©er le namespace avec les outils de base uniquement
            namespace = {
                'requests': requests,
                'BeautifulSoup': BeautifulSoup,
                'urljoin': urljoin,
                'urlparse': urlparse,
                're': re,
                'json': json,
                'time': time,
                'os': os,
                'url': url,
                'base_url': url,  # Alias pour compatibilit√©
                'session': self.session,
                'EXTRACTION_SCHEMA': EXTRACTION_SCHEMA,
                # Pour compatibilit√© avec anciens scrapers
                'field_mappings': field_mappings,
                'site_analysis': site_analysis,
                'exploration_result': exploration_result,
                # Pour compatibilit√©
                'Path': Path,
                'ThreadPoolExecutor': ThreadPoolExecutor,
                'as_completed': as_completed,
                'print': print  # S'assurer que print fonctionne
            }

            # NOUVEAU FLUX : Les scripts g√©n√©r√©s sont autonomes et n'ont pas besoin d'outils AI
            # Les URLs et s√©lecteurs sont hardcod√©s dans le script
            print(f"   ‚úÖ Namespace configur√© pour script autonome (sans Gemini)")

            print(f"\nüîß V√©rification du namespace d'ex√©cution...")
            # V√©rifier que les imports de base sont pr√©sents
            required_namespace_items = [
                'BeautifulSoup', 'requests', 'urljoin', 'urlparse', 're', 'json', 'time'
            ]
            missing_items = [
                item for item in required_namespace_items if item not in namespace]
            if missing_items:
                print(
                    f"   ‚ö†Ô∏è  ATTENTION: √âl√©ments manquants dans le namespace: {missing_items}")
            else:
                print(
                    f"   ‚úÖ Tous les imports de base sont pr√©sents dans le namespace")

            print(f"\nüîß Ex√©cution du code du scraper...")
            print(f"   Longueur du code: {len(scraper_code)} caract√®res")

            # V√©rifier que le code contient bien une fonction scrape
            if 'def scrape' not in scraper_code and 'def main' not in scraper_code:
                print(
                    f"   ‚ö†Ô∏è  ATTENTION: Le code g√©n√©r√© ne contient pas de fonction 'scrape' ou 'main'")

            # VALIDATION: V√©rifier que le scraper est autonome (sans Gemini)
            print(f"\nüîç Validation du scraper autonome...")
            workflow_checks = {
                'URLs hardcod√©es (PRODUCT_URLS)': 'PRODUCT_URLS' in scraper_code,
                'S√©lecteurs hardcod√©s (SELECTORS)': 'SELECTORS' in scraper_code,
                'Extraction locale (BeautifulSoup)': any(keyword in scraper_code.lower() for keyword in [
                    'beautifulsoup', 'soup.select', 'soup.find'
                ]),
                'Pas de d√©pendance Gemini': 'gemini_client' not in scraper_code.lower() and 'GeminiClient' not in scraper_code,
                'Fonction scrape()': 'def scrape' in scraper_code
            }

            all_checks_passed = all(workflow_checks.values())
            for step, passed in workflow_checks.items():
                status = "‚úÖ" if passed else "‚ö†Ô∏è"
                print(f"   {status} {step}: {'OK' if passed else 'MANQUANT'}")

            if not all_checks_passed:
                print(
                    f"\n   ‚ö†Ô∏è  ATTENTION: Le scraper g√©n√©r√© ne semble pas √™tre compl√®tement autonome.")
                print(
                    f"   Certaines fonctionnalit√©s peuvent √™tre manquantes.")
            else:
                print(
                    f"\n   ‚úÖ Le scraper est autonome et pr√™t √† √™tre ex√©cut√© (sans Gemini)")

            # Nettoyer et valider le code Python avant ex√©cution
            print(f"   üîç Validation du code Python g√©n√©r√©...")

            # Nettoyer le code (enlever markdown si pr√©sent)
            cleaned_code = scraper_code
            if '```python' in cleaned_code:
                # Extraire le code entre ```python et ```
                match = re.search(r'```python\s*\n(.*?)\n```',
                                  cleaned_code, re.DOTALL)
                if match:
                    cleaned_code = match.group(1)
                    print(f"   ‚ö†Ô∏è  Markdown d√©tect√© et retir√© du code")
                else:
                    # Essayer avec ``` seul
                    match = re.search(r'```\s*\n(.*?)\n```',
                                      cleaned_code, re.DOTALL)
                    if match:
                        cleaned_code = match.group(1)
                        print(f"   ‚ö†Ô∏è  Markdown d√©tect√© et retir√© du code")

            # Valider la syntaxe Python
            try:
                compile(cleaned_code, '<string>', 'exec')
                print(f"   ‚úÖ Syntaxe Python valide")
            except SyntaxError as e:
                print(f"   ‚ùå ERREUR DE SYNTAXE dans le code g√©n√©r√©:")
                print(f"      Ligne {e.lineno}: {e.text}")
                print(f"      Message: {e.msg}")
                raise ValueError(f"Code g√©n√©r√© invalide (syntaxe Python): {e}")

            # Utiliser le code nettoy√©
            scraper_code = cleaned_code

            # Ex√©cuter le code du scraper
            try:
                print(f"   üîÑ Ex√©cution du code du scraper...")
                exec(scraper_code, namespace)
                print(f"   ‚úÖ Code ex√©cut√© sans erreur de syntaxe")
            except SyntaxError as e:
                print(f"   ‚ùå ERREUR DE SYNTAXE dans le code g√©n√©r√©:")
                print(f"      Ligne {e.lineno}: {e.text}")
                raise
            except Exception as e:
                print(f"   ‚ùå ERREUR lors de l'ex√©cution du code:")
                print(f"      {type(e).__name__}: {e}")
                import traceback
                traceback.print_exc()
                raise

            # Appeler la fonction principale du scraper
            # Essayer diff√©rentes signatures de fonction
            if 'scrape' in namespace:
                print(f"   Appel de la fonction 'scrape'...")
                scrape_func = namespace['scrape']
                import inspect
                sig = inspect.signature(scrape_func)
                params = list(sig.parameters.keys())

                # Adapter l'appel selon les param√®tres de la fonction
                if len(params) == 1:
                    result = scrape_func(url)
                elif len(params) == 2:
                    if 'session' in params:
                        result = scrape_func(url, self.session)
                    elif 'gemini_client' in params:
                        result = scrape_func(url, gemini_client)
                    else:
                        result = scrape_func(url, namespace.get(params[1]))
                elif len(params) == 3:
                    result = scrape_func(url, gemini_client, self.session)
                else:
                    # Essayer avec tous les param√®tres du namespace
                    kwargs = {p: namespace.get(p)
                              for p in params if p in namespace}
                    result = scrape_func(url, **kwargs)

            elif 'main' in namespace:
                print(f"   Appel de la fonction 'main'...")
                main_func = namespace['main']
                import inspect
                sig = inspect.signature(main_func)
                params = list(sig.parameters.keys())

                if len(params) == 1:
                    result = main_func(url)
                elif len(params) == 2:
                    if 'session' in params:
                        result = main_func(url, self.session)
                    elif 'gemini_client' in params:
                        result = main_func(url, gemini_client)
                    else:
                        result = main_func(url, namespace.get(params[1]))
                elif len(params) == 3:
                    result = main_func(url, gemini_client, self.session)
                else:
                    kwargs = {p: namespace.get(p)
                              for p in params if p in namespace}
                    result = main_func(url, **kwargs)
            else:
                print(
                    f"   ‚ùå ERREUR: Aucune fonction 'scrape' ou 'main' trouv√©e dans le code g√©n√©r√©")
                print(
                    f"   Fonctions disponibles: {[k for k in namespace.keys() if callable(namespace[k]) and not k.startswith('_')]}")
                raise ValueError(
                    "Le scraper g√©n√©r√© doit contenir une fonction 'scrape' ou 'main'")

            # Valider le format du r√©sultat
            if not isinstance(result, dict):
                raise ValueError("Le scraper doit retourner un dictionnaire")

            if 'companyInfo' not in result:
                result['companyInfo'] = {}
            if 'products' not in result:
                result['products'] = []

            print(f"\nüìä R√©sultat du scraper:")
            print(f"   - Produits trouv√©s: {len(result.get('products', []))}")
            print(f"   - CompanyInfo: {bool(result.get('companyInfo', {}))}")

            # NOUVEAU FLUX : Pas de v√©rification Gemini (script autonome)
            print(f"\n‚úÖ Script autonome ex√©cut√© (sans d√©pendance Gemini)")

            # Ajouter sourceSite √† chaque produit
            for product in result.get('products', []):
                if 'sourceSite' not in product:
                    product['sourceSite'] = url
                if 'sourceUrl' not in product:
                    product['sourceUrl'] = url

            products_count = len(result.get('products', []))

            if products_count == 0:
                print(f"\n‚ùå PROBL√àME: Aucun produit extrait!")
                print(f"   Le scraper n'a pas r√©ussi √† extraire de produits.")
                print(f"\n   üîç Diagnostic:")
                print(f"   ‚ö†Ô∏è  Le scraper autonome n'a pas trouv√© de produits")
                print(f"\n   Raisons possibles:")
                print(f"   - Les URLs hardcod√©es sont incorrectes ou obsol√®tes")
                print(f"   - Les s√©lecteurs CSS hardcod√©s ne correspondent plus au HTML")
                print(f"   - Le site a chang√© sa structure")
                print(f"   - Le site n√©cessite JavaScript (Selenium requis)")
                print(f"\n   üí° Solutions:")
                print(f"   1. Utilisez --force-refresh pour r√©g√©n√©rer le scraper")
                print(
                    f"   2. V√©rifiez les logs ci-dessus pour voir o√π le scraper a √©chou√©")
                print(
                    f"   3. V√©rifiez que le site est toujours accessible et n'a pas chang√©")
            else:
                print(
                    f"\n‚úÖ Scraping termin√©: {products_count} produits extraits")

            # Avertissement si tr√®s peu de produits (possible probl√®me de pagination)
            if 0 < products_count < 10:
                print(
                    f"\n‚ö†Ô∏è  ATTENTION: Seulement {products_count} produits trouv√©s.")
                print(f"   V√©rifiez si la pagination fonctionne correctement.")
                print(f"   Le site pourrait avoir plus de produits sur d'autres pages.")

            return result

        except SyntaxError as e:
            print(f"‚ùå Erreur de syntaxe dans le scraper g√©n√©r√©: {e}")
            print(f"   Ligne: {e.lineno}")
            print(f"   Code probl√©matique: {e.text}")
            import traceback
            traceback.print_exc()
            return {"companyInfo": {}, "products": []}
        except Exception as e:
            print(f"‚ùå Erreur lors de l'ex√©cution du scraper: {e}")
            import traceback
            traceback.print_exc()
            return {"companyInfo": {}, "products": []}

    def scrape_site(self, url: str, reference_url: Optional[str] = None,
                    force_refresh: bool = False) -> Dict:
        """Scrape un site complet: analyse + g√©n√©ration + ex√©cution

        Args:
            url: URL du site √† scraper
            reference_url: URL du site de r√©f√©rence pour comparaison de prix
            force_refresh: Si True, ignore le cache et r√©g√©n√®re le scraper

        Returns:
            Dict au format EXTRACTION_SCHEMA
        """
        print(f"\n{'='*60}")
        print(f"üîç D√âMARRAGE DU SCRAPING AI")
        print(f"{'='*60}")
        print(f"üåê Site: {url}")
        if force_refresh:
            print(f"üîÑ Mode: Force refresh (ignore le cache)")

        # √âtape 1: R√©cup√©rer le HTML de la page d'accueil
        print(f"\nüì• R√©cup√©ration du HTML de la page d'accueil...")
        html_content = self.fetch_html(url)
        print(f"   ‚úÖ {len(html_content)} caract√®res r√©cup√©r√©s")

        # √âtape 2: Analyser et g√©n√©rer le scraper
        # Gemini peut demander des pages suppl√©mentaires si n√©cessaire
        print(f"\nüîç Analyse du site (Gemini peut demander plus de pages)...")
        scraper_data = self.html_analyzer.analyze_and_generate_scraper(
            url=url,
            html_content=html_content,
            force_refresh=force_refresh
        )

        # Afficher les pages analys√©es
        metadata = scraper_data.get('metadata', {})
        analyzed_pages = metadata.get('analyzed_pages', [url])
        print(f"\nüìä Pages analys√©es: {len(analyzed_pages)}")
        for page in analyzed_pages:
            print(f"   - {page}")

        # Afficher les informations du cache si c'est un nouveau scraper
        cache_key = metadata.get('cache_key', '')
        if cache_key:
            print(f"\nüíæ Scraper disponible dans le cache")
            print(f"   üìÅ Cl√© de cache: {cache_key}")
            print(
                f"   üìù Version du prompt: {metadata.get('prompt_version', 'N/A')}")

        # √âtape 3: Ex√©cuter le scraper imm√©diatement apr√®s la g√©n√©ration
        print(f"\n{'='*60}")
        print(f"üöÄ EX√âCUTION DU SCRAPER G√âN√âR√â")
        print(f"{'='*60}")
        print(f"üîÑ D√©marrage de l'extraction avec le scraper sauvegard√©...\n")
        result = self.execute_scraper(
            url=url,
            scraper_data=scraper_data,
            reference_url=reference_url
        )

        return result
