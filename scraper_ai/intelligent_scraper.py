"""
Module de scraping intelligent avec cache Supabase
Orchestre le workflow complet: cache ‚Üí exploration ‚Üí d√©tection ‚Üí extraction
"""
import json
import time
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from urllib.parse import urlparse, urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
from bs4 import BeautifulSoup

try:
    from .supabase_storage import SupabaseStorage, get_storage
    from .selector_detector import SelectorDetector
    from .exploration_agent import ExplorationAgent
    from .scraper_generator import ScraperGenerator
    from .config import CACHE_DIR, PROMPT_VERSION
except ImportError:
    from supabase_storage import SupabaseStorage, get_storage
    from selector_detector import SelectorDetector
    from exploration_agent import ExplorationAgent
    from scraper_generator import ScraperGenerator
    from config import CACHE_DIR, PROMPT_VERSION


class IntelligentScraper:
    """Scraper intelligent avec gestion de cache et s√©lecteurs dynamiques"""

    def __init__(self, user_id: str):
        """
        Initialise le scraper intelligent.

        Args:
            user_id: ID de l'utilisateur connect√© (OBLIGATOIRE)

        Raises:
            ValueError: Si user_id n'est pas fourni
        """
        if not user_id:
            raise ValueError(
                "‚ùå Authentification requise: vous devez √™tre connect√© pour utiliser le scraper.")

        self.user_id = user_id
        self.storage = SupabaseStorage(user_id)
        self.selector_detector = SelectorDetector()
        self.exploration_agent = ExplorationAgent()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def scrape(
        self,
        url: str,
        force_refresh: bool = False,
        categories: List[str] = None,
        inventory_only: bool = False
    ) -> Dict[str, Any]:
        """Scrape un site avec le workflow intelligent

        Args:
            url: URL du site √† scraper
            force_refresh: Forcer la r√©g√©n√©ration m√™me si cache valide
            categories: Cat√©gories √† scraper (inventaire, occasion, catalogue)
            inventory_only: Si True, exclut les pages catalogue/showroom

        Returns:
            Dict avec products, metadata, scraper_info
        """
        self._inventory_only = inventory_only
        start_time = time.time()

        print(f"\n{'='*70}")
        print(f"üöÄ SCRAPER INTELLIGENT v{PROMPT_VERSION}")
        print(f"{'='*70}")
        print(f"üåê Site: {url}")
        print(f"üë§ User ID: {self.user_id or 'Non connect√© (local)'}")
        print(f"üîÑ Force refresh: {force_refresh}")
        print(f"üì¶ Inventaire seulement: {'Oui' if inventory_only else 'Non'}")

        # Normaliser l'URL
        url = self._normalize_url(url)

        # Cat√©gories par d√©faut: TOUTES les cat√©gories pour extraction compl√®te
        # L'√©tat (neuf/usag√©/catalogue) est d√©tect√© automatiquement par produit
        if categories is None:
            categories = ['inventaire', 'occasion', 'catalogue']

        # =====================================================
        # PR√â-CHECK: CONNECTIVIT√â DU SITE
        # =====================================================
        if not self._check_site_connectivity(url):
            print(f"\n{'='*60}")
            print(f"‚ùå SITE INACCESSIBLE: {url}")
            print(f"{'='*60}")
            print(f"   Le site ne r√©pond pas apr√®s plusieurs tentatives.")
            print(f"   V√©rifiez l'URL et votre connexion r√©seau.")
            return self._create_empty_result(url, start_time, "site_unreachable")

        # =====================================================
        # √âTAPE 1: V√âRIFICATION DU CACHE
        # =====================================================
        print(f"\n{'='*50}")
        print(f"üì¶ √âTAPE 1: V√âRIFICATION DU CACHE")
        print(f"{'='*50}")

        cached_scraper = None
        cache_status = "miss"

        if not force_refresh and self.storage:
            is_valid, cached_scraper = self.storage.is_cache_valid(url)

            if is_valid and cached_scraper:
                cache_status = "hit"
                print(f"‚úÖ CACHE VALIDE trouv√©!")
                print(f"   Expire: {cached_scraper.get('expires_at', 'N/A')}")
                print(
                    f"   S√©lecteurs: {len(cached_scraper.get('selectors', {}))} d√©tect√©s")
                print(
                    f"   URLs produits: {len(cached_scraper.get('product_urls', []))} en cache")
            elif cached_scraper:
                cache_status = "expired"
                print(f"‚ö†Ô∏è  CACHE EXPIR√â - Mise √† jour des URLs n√©cessaire")
            else:
                print(f"‚ùå Aucun cache trouv√©")
        else:
            print(
                f"‚è≠Ô∏è  Cache ignor√© (force_refresh={force_refresh}, storage={bool(self.storage)})")

        # =====================================================
        # √âTAPE 2: EXPLORATION (si n√©cessaire)
        # =====================================================
        selectors = {}
        product_urls = []

        if cache_status == "hit":
            # Utiliser les donn√©es du cache
            selectors = cached_scraper.get('selectors', {})
            product_urls = cached_scraper.get('product_urls', [])

            # Optionnel: rafra√Æchir les URLs si le cache est proche de l'expiration
            # (comment√© pour l'instant, √† activer si n√©cessaire)
            # product_urls = self._refresh_product_urls(url, selectors, categories)

        elif cache_status == "expired" and cached_scraper:
            # Cache expir√©: r√©utiliser les s√©lecteurs, mais rafra√Æchir les URLs
            print(f"\n{'='*50}")
            print(f"üîÑ √âTAPE 2: RAFRA√éCHISSEMENT DES URLs")
            print(f"{'='*50}")

            selectors = cached_scraper.get('selectors', {})
            print(f"   R√©utilisation des s√©lecteurs existants")

            # D√©couvrir les nouvelles URLs
            product_urls = self._discover_product_urls(url, categories)

            # Mettre √† jour le cache avec les nouvelles URLs
            if self.storage and product_urls:
                self.storage.update_scraper_urls(url, product_urls)
                self.storage.refresh_cache_expiry(url)
                print(f"   ‚úÖ Cache mis √† jour avec {len(product_urls)} URLs")

        else:
            # Pas de cache: exploration compl√®te
            print(f"\n{'='*50}")
            print(f"üîç √âTAPE 2: EXPLORATION COMPL√àTE")
            print(f"{'='*50}")

            # 2.1 D√©couvrir les URLs de produits
            product_urls = self._discover_product_urls(url, categories)

            if not product_urls:
                print(f"‚ùå Aucune URL de produit trouv√©e!")
                return self._create_empty_result(url, start_time, "no_urls_found")

            # 2.2 R√©cup√©rer des √©chantillons HTML
            html_samples = self._fetch_html_samples(product_urls[:5])

            if not html_samples:
                print(f"‚ùå Impossible de r√©cup√©rer le HTML!")
                return self._create_empty_result(url, start_time, "html_fetch_failed")

            # 2.3 D√©tecter les s√©lecteurs CSS
            print(f"\nüéØ D√©tection des s√©lecteurs CSS...")
            detection_result = self.selector_detector.detect_selectors(
                html_samples=html_samples,
                base_url=url
            )

            selectors = detection_result.get('selectors', {})

            # 2.4 Sauvegarder dans le cache
            if self.storage and selectors:
                scraper_code = self._generate_scraper_code(
                    url, selectors, product_urls)
                self.storage.save_scraper(
                    site_url=url,
                    scraper_code=scraper_code,
                    selectors=selectors,
                    product_urls=product_urls,
                    metadata={
                        'site_name': self._extract_site_name(url),
                        'detection_result': detection_result,
                        'prompt_version': PROMPT_VERSION,
                        'categories': categories
                    }
                )

        # =====================================================
        # √âTAPE 3: EXTRACTION DES PRODUITS
        # =====================================================
        print(f"\n{'='*50}")
        print(f"üì• √âTAPE 3: EXTRACTION DES PRODUITS")
        print(f"{'='*50}")
        print(f"   URLs √† traiter: {len(product_urls)}")

        products = self._extract_products(product_urls, selectors, url)

        print(f"\n‚úÖ {len(products)} produits extraits")

        # =====================================================
        # PROTECTION CACHE: Invalider si 0 produits extraits
        # =====================================================
        # Un scraper qui ne trouve aucun produit est probablement cass√©.
        # Supprimer le cache pour forcer une nouvelle d√©tection au prochain essai.
        if len(products) == 0 and cache_status in ("miss", "expired"):
            if self.storage:
                print(
                    f"‚ö†Ô∏è  0 produits extraits ‚Üí invalidation du cache pour √©viter de r√©utiliser un scraper cass√©")
                try:
                    self.storage.delete_scraper(url)
                except Exception as e:
                    print(
                        f"   ‚ö†Ô∏è  Erreur lors de l'invalidation du cache: {e}")
        elif len(products) == 0 and cache_status == "hit":
            # Le cache existait d√©j√† mais n'a rien extrait ‚Üí invalider aussi
            if self.storage:
                print(f"‚ö†Ô∏è  Cache existant mais 0 produits ‚Üí invalidation du cache")
                try:
                    self.storage.delete_scraper(url)
                except Exception as e:
                    print(
                        f"   ‚ö†Ô∏è  Erreur lors de l'invalidation du cache: {e}")

        # =====================================================
        # √âTAPE 4: SAUVEGARDE DES R√âSULTATS
        # =====================================================
        elapsed_time = time.time() - start_time

        if self.storage:
            self.storage.save_scraping_result(
                site_url=url,
                products=products,
                execution_time=elapsed_time,
                metadata={
                    'cache_status': cache_status,
                    'urls_processed': len(product_urls),
                    'categories': categories
                }
            )

        # R√©sum√© final
        print(f"\n{'='*70}")
        print(f"‚úÖ SCRAPING TERMIN√â!")
        print(f"{'='*70}")
        print(f"üì¶ Produits extraits: {len(products)}")
        print(f"‚è±Ô∏è  Temps total: {elapsed_time:.1f}s")
        print(f"üìä Cache: {cache_status}")

        return {
            'products': products,
            'metadata': {
                'site_url': url,
                'products_count': len(products),
                'urls_processed': len(product_urls),
                'execution_time_seconds': round(elapsed_time, 2),
                'cache_status': cache_status,
                'categories': categories,
                'prompt_version': PROMPT_VERSION
            },
            'scraper_info': {
                'selectors': selectors,
                'product_urls_count': len(product_urls)
            }
        }

    def _check_site_connectivity(self, url: str, max_retries: int = 4, initial_wait: float = 3.0) -> bool:
        """V√©rifie que le site est accessible avant de lancer l'exploration.

        Effectue un HEAD request avec retry et exponential backoff.
        G√®re sp√©cifiquement les erreurs DNS transitoires.

        Args:
            url: URL du site √† v√©rifier
            max_retries: Nombre maximum de tentatives (d√©faut: 4)
            initial_wait: D√©lai initial en secondes (d√©faut: 3s)

        Returns:
            True si le site est accessible, False sinon
        """
        import socket

        parsed = urlparse(url)
        hostname = parsed.netloc or parsed.hostname

        print(f"\nüîå V√©rification de la connectivit√©: {hostname}...")

        for attempt in range(max_retries):
            try:
                # 1. V√©rifier la r√©solution DNS
                socket.getaddrinfo(
                    hostname, 443, socket.AF_UNSPEC, socket.SOCK_STREAM)

                # 2. V√©rifier l'acc√®s HTTP (HEAD request rapide)
                response = self.session.head(
                    url, timeout=15, allow_redirects=True)
                # Accepter tout code < 500 (m√™me 403/404 = site accessible)
                if response.status_code < 500:
                    print(
                        f"   ‚úÖ Site accessible (HTTP {response.status_code})")
                    return True
                else:
                    raise requests.exceptions.HTTPError(
                        f"HTTP {response.status_code}"
                    )

            except Exception as e:
                error_str = str(e).lower()
                is_dns = any(kw in error_str for kw in [
                    'nameresolution', 'name resolution', 'nodename nor servname',
                    'temporary failure', 'getaddrinfo', 'newconnectionerror',
                ])
                is_transient = is_dns or any(kw in error_str for kw in [
                    'timeout', 'timed out', 'connectionerror', 'connection refused',
                    'connectionreset', 'remotedisconnected', 'max retries',
                    '502', '503', '504',
                ])

                if attempt < max_retries - 1 and is_transient:
                    wait_time = initial_wait * \
                        (2 ** attempt)  # 3s, 6s, 12s, 24s
                    error_type = "DNS" if is_dns else "connexion"
                    print(
                        f"   ‚ö†Ô∏è Tentative {attempt + 1}/{max_retries}: Erreur {error_type} ‚Üí {e}")
                    print(f"   üîÑ Nouvelle tentative dans {wait_time:.0f}s...")
                    time.sleep(wait_time)
                else:
                    print(
                        f"   ‚ùå Site inaccessible apr√®s {attempt + 1} tentative(s): {e}")
                    return False

        return False

    def _normalize_url(self, url: str) -> str:
        """Normalise une URL"""
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        # Supprimer le trailing slash
        url = url.rstrip('/')

        return url

    def _extract_site_name(self, url: str) -> str:
        """Extrait le nom du site depuis l'URL"""
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '')
        return domain.split('.')[0].title()

    def _discover_product_urls(self, base_url: str, categories: List[str]) -> List[str]:
        """D√©couvre les URLs de produits via l'agent d'exploration"""
        print(f"\n   üîç D√©couverte des URLs de produits...")
        print(f"   Cat√©gories: {categories}")

        try:
            # Utiliser l'agent d'exploration
            inventory_only = getattr(self, '_inventory_only', False)
            result = self.exploration_agent.explore_and_extract(
                base_url, inventory_only=inventory_only)

            all_urls = result.get('product_urls', [])

            # Filtrer par cat√©gorie si possible
            filtered_urls = self._filter_urls_by_category(
                all_urls, categories, base_url)

            print(
                f"   ‚úÖ {len(filtered_urls)} URLs d√©couvertes (filtr√© de {len(all_urls)})")

            return filtered_urls

        except Exception as e:
            print(f"   ‚ùå Erreur exploration: {e}")
            return []

    def _filter_urls_by_category(
        self,
        urls: List[str],
        categories: List[str],
        base_url: str
    ) -> List[str]:
        """Filtre les URLs par cat√©gorie (inventaire, occasion, catalogue)

        IMPORTANT: Quand toutes les cat√©gories sont incluses, cette m√©thode
        retourne TOUTES les URLs pour une extraction compl√®te.
        L'√©tat (neuf/usag√©/catalogue) est ensuite d√©tect√© par produit.
        """
        if not categories:
            return urls

        # Si toutes les cat√©gories sont incluses, pas de filtrage
        all_categories = {'inventaire', 'occasion', 'catalogue'}
        if all_categories.issubset(set(categories)):
            print(f"      ‚ÑπÔ∏è  Toutes les cat√©gories actives - pas de filtrage URL")
            return urls

        # Mots-cl√©s pour chaque cat√©gorie
        category_keywords = {
            # NOTE: "neuf" est ambigu (peut √™tre catalogue/showroom). On √©vite de l'utiliser comme indicateur inventaire.
            'inventaire': ['inventaire', 'inventory', 'stock', 'en-stock', 'disponible', 'a-vendre', 'for-sale'],
            'occasion': ['occasion', 'used', 'pre-owned', 'usag', 'seconde-main', 'd-occasion'],
            'catalogue': ['catalogue', 'catalog', 'modele', 'model', 'gamme', 'range']
        }

        # Mots-cl√©s √† exclure
        exclude_keywords = []
        if 'catalogue' not in categories:
            exclude_keywords.extend(
                ['catalogue', 'catalog', 'modele', 'model', 'gamme', 'range'])

        filtered = []
        for url in urls:
            url_lower = url.lower()

            # V√©rifier si l'URL contient des mots-cl√©s √† exclure
            if any(kw in url_lower for kw in exclude_keywords):
                continue

            # V√©rifier si l'URL contient des mots-cl√©s de cat√©gorie
            for cat in categories:
                if cat in category_keywords:
                    if any(kw in url_lower for kw in category_keywords[cat]):
                        filtered.append(url)
                        break
            else:
                # Si aucune cat√©gorie sp√©cifique, inclure par d√©faut
                # (sauf si explicitement exclu)
                if not exclude_keywords or not any(kw in url_lower for kw in exclude_keywords):
                    filtered.append(url)

        return filtered if filtered else urls

    def _fetch_html_samples(self, urls: List[str], max_samples: int = 5) -> Dict[str, str]:
        """R√©cup√®re des √©chantillons HTML de plusieurs URLs"""
        samples = {}

        for url in urls[:max_samples]:
            try:
                response = self.session.get(url, timeout=15)
                if response.status_code == 200:
                    samples[url] = response.text
                    print(f"      ‚úÖ {url[:60]}...")
            except Exception as e:
                print(f"      ‚ùå {url[:60]}... ({e})")

        return samples

    def _extract_products(
        self,
        urls: List[str],
        selectors: Dict[str, str],
        base_url: str
    ) -> List[Dict]:
        """Extrait les produits de toutes les URLs"""
        all_products = []

        # Utiliser le multithreading pour acc√©l√©rer
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = {
                executor.submit(self._extract_from_url, url, selectors, base_url): url
                for url in urls
            }

            for future in as_completed(futures):
                url = futures[future]
                try:
                    products = future.result()
                    if products:
                        all_products.extend(products)
                        print(
                            f"      ‚úÖ {len(products)} produits de {url[:50]}...")
                except Exception as e:
                    print(f"      ‚ùå Erreur {url[:50]}...: {e}")

        # D√©dupliquer les produits
        unique_products = self._deduplicate_products(all_products)

        return unique_products

    def _extract_from_url(
        self,
        url: str,
        selectors: Dict[str, str],
        base_url: str
    ) -> List[Dict]:
        """Extrait les produits d'une seule URL

        STRAT√âGIE D'EXTRACTION (ordre de priorit√©):
        1. Donn√©es structur√©es (JSON-LD, Open Graph) - pour les pages de d√©tail
        2. S√©lecteurs CSS - pour les pages de listing

        Apr√®s extraction, enrichit chaque produit avec:
        - sourceCategorie (inventaire, catalogue, vehicules_occasion)
        - etat (neuf, occasion, demonstrateur)
        """
        try:
            response = self.session.get(url, timeout=15)
            if response.status_code != 200:
                return []

            html = response.text

            # ============================================================
            # PRIORIT√â 1: Extraction depuis donn√©es structur√©es (JSON-LD, OG)
            # C'est la m√©thode la plus fiable pour les pages de d√©tail produit
            # ============================================================
            product_from_structured = self._extract_structured_data(
                html, url, base_url)

            if product_from_structured and product_from_structured.get('name') and product_from_structured.get('prix'):
                # Extraction structur√©e r√©ussie AVEC un prix ‚Äî utiliser ce r√©sultat
                # IMPORTANT: Toujours forcer sourceSite au site en cours (pas conditionnel)
                product_from_structured['sourceSite'] = base_url
                product_from_structured['sourceUrl'] = url
                # D√©tecter l'√©tat/condition du produit
                self._detect_product_condition(
                    product_from_structured, url, html)
                return [product_from_structured]

            # ============================================================
            # PRIORIT√â 2: Extraction via s√©lecteurs CSS (pages listing)
            # ============================================================
            products = self.selector_detector.extract_with_selectors(
                html=html,
                selectors=selectors,
                base_url=base_url
            )

            # IMPORTANT: Toujours FORCER sourceUrl et sourceSite (pas conditionnel)
            # Cela garantit qu'un produit extrait d'un site ne sera jamais attribu√© √† un autre
            for product in products:
                product['sourceUrl'] = product.get('sourceUrl') or url
                # Toujours forcer le site source
                product['sourceSite'] = base_url
                # D√©tecter l'√©tat/condition du produit
                self._detect_product_condition(product, url, html)

            # ============================================================
            # HYBRIDE: Si les donn√©es structur√©es avaient un nom mais pas de prix,
            # essayer de trouver le prix, puis retourner les donn√©es structur√©es
            # (M√äME SANS PRIX ‚Äî c'est mieux que des donn√©es CSS garbage)
            # ============================================================
            if product_from_structured and product_from_structured.get('name') and not product_from_structured.get('prix'):
                # Chercher un prix dans les produits CSS
                css_price = None
                for p in products:
                    if p.get('prix') and p['prix'] > 0:
                        css_price = p['prix']
                        break

                if css_price:
                    product_from_structured['prix'] = css_price

                # Essayer aussi le fallback regex sur le HTML brut
                if not product_from_structured.get('prix'):
                    from scraper_ai.templates.scraper_template import extract_price
                    import re
                    price_patterns = [
                        r'class="[^"]*(?:price|prix)[^"]*"[^>]*>([^<]+)',
                        r'itemprop="price"[^>]*content="([^"]+)"',
                        r'data-price="([^"]+)"',
                        r'<span[^>]*class="[^"]*amount[^"]*"[^>]*>([^<]+)',
                    ]
                    for pattern in price_patterns:
                        matches = re.findall(pattern, html, re.I)
                        for match_text in matches:
                            price = extract_price(match_text)
                            if price > 0:
                                product_from_structured['prix'] = price
                                break
                        if product_from_structured.get('prix'):
                            break

                # IMPORTANT: Retourner les donn√©es structur√©es M√äME SANS PRIX
                # Un produit avec nom+marque+mod√®le sans prix est bien plus utile
                # qu'un produit CSS garbage sans nom ni mod√®le
                product_from_structured['sourceSite'] = base_url
                product_from_structured['sourceUrl'] = url
                self._detect_product_condition(
                    product_from_structured, url, html)
                return [product_from_structured]

            return products

        except Exception as e:
            return []

    def _extract_structured_data(self, html: str, url: str, base_url: str) -> Dict:
        """Extrait les donn√©es produit depuis JSON-LD, Open Graph, et microdata

        Cette m√©thode est essentielle pour les pages de d√©tail produit
        qui utilisent des donn√©es structur√©es standardis√©es.
        """
        import json
        import re
        from urllib.parse import urljoin

        soup = BeautifulSoup(html, 'html.parser')
        product = {}

        # ========================================================
        # STRAT√âGIE 1: JSON-LD (la plus fiable)
        # ========================================================
        for script in soup.find_all('script', type='application/ld+json'):
            try:
                if not script.string:
                    continue
                data = json.loads(script.string)

                items_to_check = []
                if isinstance(data, list):
                    items_to_check.extend(data)
                elif isinstance(data, dict):
                    items_to_check.append(data)
                    if '@graph' in data:
                        items_to_check.extend(data['@graph'])

                for item in items_to_check:
                    if not isinstance(item, dict):
                        continue

                    item_type = item.get('@type', '')
                    if isinstance(item_type, list):
                        item_types = [t.lower() for t in item_type]
                    else:
                        item_types = [item_type.lower()]

                    # Types support√©s
                    if any(t in ' '.join(item_types) for t in ['product', 'vehicle', 'motorcycle', 'car']):
                        # Nom
                        if not product.get('name') and item.get('name'):
                            product['name'] = str(item['name']).strip()

                        # Prix depuis offers OU directement depuis l'item
                        if not product.get('prix'):
                            price = None
                            # D'abord chercher directement dans l'item (certains sites)
                            price = item.get('price') or item.get(
                                'lowPrice') or item.get('highPrice')
                            # Sinon chercher dans offers (standard schema.org)
                            if not price:
                                offers = item.get('offers', {})
                                if isinstance(offers, list) and offers:
                                    offers = offers[0]
                                if isinstance(offers, dict):
                                    price = offers.get('price') or offers.get(
                                        'lowPrice') or offers.get('highPrice')
                            if price:
                                try:
                                    product['prix'] = float(
                                        str(price).replace(',', '.').replace(' ', ''))
                                except (ValueError, TypeError):
                                    pass

                        # Image
                        if not product.get('image'):
                            img = item.get('image')
                            if img:
                                if isinstance(img, list):
                                    img = img[0]
                                if isinstance(img, dict):
                                    img = img.get('url')
                                if img and isinstance(img, str):
                                    product['image'] = urljoin(base_url, img)

                        # Marque
                        if not product.get('marque'):
                            brand = item.get('brand') or item.get(
                                'manufacturer')
                            if brand:
                                if isinstance(brand, dict):
                                    brand = brand.get('name')
                                if brand:
                                    product['marque'] = str(brand)

                        # Mod√®le (champ schema.org 'model')
                        if not product.get('modele'):
                            model = item.get('model')
                            if model:
                                if isinstance(model, dict):
                                    model = model.get(
                                        'name') or model.get('model')
                                if model and isinstance(model, str):
                                    product['modele'] = str(model).strip()

                        # Ann√©e
                        if not product.get('annee'):
                            year = item.get(
                                'vehicleModelDate') or item.get('modelYear')
                            if year:
                                try:
                                    product['annee'] = int(str(year)[:4])
                                except (ValueError, TypeError):
                                    pass

                        # ========================================================
                        # CONDITION / √âTAT du produit (schema.org itemCondition)
                        # ========================================================
                        if not product.get('etat'):
                            # Chercher directement dans l'item
                            condition = item.get('itemCondition', '')
                            # Chercher dans offers
                            if not condition:
                                offers = item.get('offers', {})
                                if isinstance(offers, list) and offers:
                                    offers = offers[0]
                                if isinstance(offers, dict):
                                    condition = offers.get('itemCondition', '')

                            if condition:
                                condition_str = str(condition).lower()
                                if 'new' in condition_str or 'neuf' in condition_str:
                                    product['etat'] = 'neuf'
                                elif 'used' in condition_str or 'occasion' in condition_str or 'refurbished' in condition_str:
                                    product['etat'] = 'occasion'
                                elif 'demo' in condition_str:
                                    product['etat'] = 'demonstrateur'

                        # Kilom√©trage (pour v√©hicules)
                        if not product.get('kilometrage'):
                            mileage = item.get('mileageFromOdometer')
                            if mileage:
                                if isinstance(mileage, dict):
                                    mileage = mileage.get('value')
                                if mileage:
                                    try:
                                        product['kilometrage'] = int(
                                            float(str(mileage).replace(',', '').replace(' ', '')))
                                    except (ValueError, TypeError):
                                        pass

                        if product.get('name'):
                            break
            except (json.JSONDecodeError, Exception):
                continue

        # ========================================================
        # STRAT√âGIE 2: Open Graph meta tags
        # ========================================================
        if not product.get('name'):
            og_title = soup.find('meta', property='og:title')
            if og_title and og_title.get('content'):
                title = og_title['content'].strip()
                if len(title) >= 5:
                    product['name'] = title

        if not product.get('image'):
            og_image = soup.find('meta', property='og:image')
            if og_image and og_image.get('content'):
                product['image'] = urljoin(base_url, og_image['content'])

        if not product.get('prix'):
            for price_prop in ['og:price:amount', 'product:price:amount']:
                og_price = soup.find('meta', property=price_prop)
                if og_price and og_price.get('content'):
                    try:
                        product['prix'] = float(
                            og_price['content'].replace(',', '.'))
                        break
                    except (ValueError, TypeError):
                        continue

        # ========================================================
        # STRAT√âGIE 3: Microdata (itemprop)
        # ========================================================
        if not product.get('name'):
            name_elem = soup.find(attrs={'itemprop': 'name'})
            if name_elem:
                product['name'] = name_elem.get_text(strip=True)

        if not product.get('prix'):
            price_elem = soup.find(attrs={'itemprop': 'price'})
            if price_elem:
                price_text = price_elem.get(
                    'content') or price_elem.get_text(strip=True)
                try:
                    product['prix'] = float(
                        re.sub(r'[^\d.]', '', str(price_text)))
                except (ValueError, TypeError):
                    pass

        if not product.get('image'):
            img_elem = soup.find(attrs={'itemprop': 'image'})
            if img_elem:
                img_src = img_elem.get('src') or img_elem.get('content')
                if img_src:
                    product['image'] = urljoin(base_url, img_src)

        # ========================================================
        # STRAT√âGIE 4: Title de la page (fallback)
        # ========================================================
        if not product.get('name'):
            title_elem = soup.find('title')
            if title_elem:
                title = title_elem.get_text(strip=True)
                # Extraire la partie avant | ou - (souvent le nom du produit)
                for sep in ['|', ' - ', ' ‚Äì ']:
                    if sep in title:
                        title = title.split(sep)[0].strip()
                        break
                if len(title) >= 5 and len(title) < 100:
                    product['name'] = title

        # Ajouter les m√©tadonn√©es
        if product.get('name'):
            product['sourceUrl'] = url
            product['sourceSite'] = base_url

        return product

    def _detect_product_condition(self, product: Dict, url: str, html: str = '') -> Dict:
        """D√©tecte l'√©tat/condition du produit et le sourceCategorie.

        Analyse TOUTES les URLs disponibles (page courante + sourceUrl du produit)
        car l'√©tat est souvent encod√© dans l'URL:
          - /usage/motocyclette/inventaire/... ‚Üí occasion
          - /neuf/motoneige/... ‚Üí neuf
          - /inventaire-occasion/... ‚Üí occasion

        Signaux utilis√©s (par priorit√©):
        1. Donn√©es structur√©es (d√©j√† extraites dans product['etat'] via JSON-LD)
        2. URL du produit (sourceUrl) - souvent le signal le plus fiable
        3. URL de la page courante (listing page)
        4. Contenu HTML de la page (badges, breadcrumbs, titre)
        5. Kilom√©trage (si > 100km, probablement occasion)
        6. Fallback: d√©duire depuis sourceCategorie

        Args:
            product: Le produit √† enrichir
            url: URL de la page courante (peut √™tre une page listing)
            html: Contenu HTML de la page (optionnel)

        Returns:
            Le produit enrichi avec sourceCategorie et etat
        """
        import re

        # Collecter TOUTES les URLs pertinentes pour l'analyse
        # L'URL du produit (sourceUrl) a priorit√© car plus sp√©cifique
        product_url = product.get('sourceUrl', '')
        urls_to_check = [product_url, url]  # sourceUrl en premier
        all_urls_lower = ' '.join(u.lower() for u in urls_to_check if u)

        # ‚îÄ‚îÄ D√©tection de sourceCategorie depuis les URLs ‚îÄ‚îÄ
        if not product.get('sourceCategorie'):
            if any(x in all_urls_lower for x in ['occasion', 'used', 'pre-owned', 'usag',
                                                 'd-occasion', 'pre-possede', 'pre_possede',
                                                 'seconde-main', 'vehicules-occasion',
                                                 'vehicule-occasion', 'inventaire-usage']):
                product['sourceCategorie'] = 'vehicules_occasion'
            elif any(x in all_urls_lower for x in ['catalogue', 'catalog', 'showroom', 'gamme',
                                                   '/models/', '/modeles/']):
                product['sourceCategorie'] = 'catalogue'
            elif any(x in all_urls_lower for x in ['inventaire', 'inventory', 'stock', 'en-stock',
                                                   'a-vendre', 'for-sale']):
                product['sourceCategorie'] = 'inventaire'
            else:
                product['sourceCategorie'] = 'inventaire'  # Par d√©faut

        # ‚îÄ‚îÄ D√©tection de l'√©tat (etat) ‚îÄ‚îÄ
        # Si d√©j√† d√©fini par donn√©es structur√©es ou selector_detector, ne pas √©craser
        if not product.get('etat'):
            etat = None

            # Signal 1: URLs (sourceUrl du produit + URL de la page)
            # Analyser chaque URL s√©par√©ment pour des patterns plus pr√©cis
            for check_url in urls_to_check:
                if not check_url or etat:
                    continue
                check_lower = check_url.lower()

                # Patterns occasion/usag√© (les plus importants √† d√©tecter)
                if any(x in check_lower for x in ['/usage/', '/used/', '/occasion/', '/pre-owned/',
                                                  '/usag', '/d-occasion/', '/pre-possede/',
                                                  '-usage-', '-used-', '-occasion-',
                                                  'vehicules-occasion', 'vehicule-occasion',
                                                  'inventaire-usage', 'inventaire-occasion',
                                                  '/pre_possede/']):
                    etat = 'occasion'
                # Patterns d√©monstrateur
                elif any(x in check_lower for x in ['/demo/', '/demonstrat/', '-demo-', '-demonstr-',
                                                    'demonstrateur']):
                    etat = 'demonstrateur'
                # Patterns neuf
                elif any(x in check_lower for x in ['/neuf/', '/new/', '-neuf-', '-new-',
                                                    'inventaire-neuf']):
                    etat = 'neuf'

            # Signal 2: Contenu HTML (titre, breadcrumbs, badges)
            if not etat and html:
                soup = BeautifulSoup(html, 'html.parser')

                # Chercher dans le titre de la page
                title_elem = soup.find('title')
                title_text = title_elem.get_text(
                    strip=True).lower() if title_elem else ''

                # Chercher dans les breadcrumbs et badges
                badge_texts = []
                for selector in ['[class*="badge"]', '[class*="label"]', '[class*="tag"]',
                                 '[class*="condition"]', '[class*="etat"]', '[class*="state"]',
                                 '[class*="stock"]', '[class*="status"]',
                                 '.breadcrumb', 'nav[aria-label*="breadcrumb"]',
                                 '[class*="breadcrumb"]', '[class*="type-vehicle"]',
                                 '[class*="vehicle-type"]']:
                    for elem in soup.select(selector):
                        badge_texts.append(elem.get_text(strip=True).lower())

                # Chercher dans les m√©tadonn√©es de la page
                meta_texts = []
                for meta in soup.find_all('meta', attrs={'name': True}):
                    meta_texts.append(str(meta.get('content', '')).lower())

                all_page_text = ' '.join(
                    [title_text] + badge_texts + meta_texts)

                # D√©tection dans le contenu avec regex pour mots entiers
                if re.search(r'\b(usag√©|usag[e√©]|occasion|used|pre-owned|pr√©-poss√©d√©)\b', all_page_text):
                    etat = 'occasion'
                elif re.search(r'\b(d√©monstrateur|demonstrateur|demo unit|d√©mo)\b', all_page_text):
                    etat = 'demonstrateur'
                elif re.search(r'\b(neuf|brand new)\b', all_page_text):
                    etat = 'neuf'

            # Signal 3: Kilom√©trage comme indicateur
            if not etat:
                km = product.get('kilometrage', 0) or 0
                if isinstance(km, str):
                    try:
                        km = int(re.sub(r'[^\d]', '', km))
                    except (ValueError, TypeError):
                        km = 0
                if km > 100:
                    etat = 'occasion'

            # Signal 4: D√©duire depuis sourceCategorie
            if not etat:
                src_cat = product.get('sourceCategorie', '')
                if src_cat == 'vehicules_occasion':
                    etat = 'occasion'
                elif src_cat == 'catalogue':
                    etat = 'neuf'  # Les catalogues sont des mod√®les neufs
                else:
                    etat = 'neuf'  # Par d√©faut l'inventaire est consid√©r√© neuf

            product['etat'] = etat

        return product

    def _deduplicate_products(self, products: List[Dict]) -> List[Dict]:
        """D√©duplique les produits bas√© sur le nom et le prix"""
        seen = set()
        unique = []

        for product in products:
            # Cr√©er une cl√© unique
            key = (
                product.get('name', '').lower().strip(),
                product.get('prix', 0),
                product.get('sourceUrl', '')
            )

            if key not in seen:
                seen.add(key)
                unique.append(product)

        return unique

    def _generate_scraper_code(
        self,
        url: str,
        selectors: Dict[str, str],
        product_urls: List[str]
    ) -> str:
        """G√©n√®re le code Python du scraper"""
        # Code simplifi√© pour le cache
        return f'''"""
Scraper g√©n√©r√© automatiquement pour {url}
Version: {PROMPT_VERSION}
Date: {datetime.now().isoformat()}
"""

SITE_URL = "{url}"

SELECTORS = {json.dumps(selectors, indent=4)}

PRODUCT_URLS = {json.dumps(product_urls[:100], indent=4)}  # Limit√© √† 100 URLs

def scrape():
    """Fonction principale de scraping"""
    from bs4 import BeautifulSoup
    import requests
    
    session = requests.Session()
    session.headers.update({{
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }})
    
    products = []
    
    for url in PRODUCT_URLS:
        try:
            response = session.get(url, timeout=15)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                # Extraction avec les s√©lecteurs
                containers = soup.select(SELECTORS.get('product_container', ''))
                for container in containers:
                    product = {{}}
                    for field, selector in SELECTORS.items():
                        if field != 'product_container' and selector:
                            element = container.select_one(selector)
                            if element:
                                product[field] = element.get_text(strip=True)
                    if product:
                        product['sourceUrl'] = url
                        products.append(product)
        except Exception as e:
            print(f"Erreur {{url}}: {{e}}")
    
    return products

if __name__ == "__main__":
    results = scrape()
    print(f"{{len(results)}} produits extraits")
'''

    def _create_empty_result(
        self,
        url: str,
        start_time: float,
        error: str
    ) -> Dict[str, Any]:
        """Cr√©e un r√©sultat vide en cas d'erreur"""
        elapsed_time = time.time() - start_time

        return {
            'products': [],
            'metadata': {
                'site_url': url,
                'products_count': 0,
                'urls_processed': 0,
                'execution_time_seconds': round(elapsed_time, 2),
                'error': error,
                'prompt_version': PROMPT_VERSION
            },
            'scraper_info': {
                'selectors': {},
                'product_urls_count': 0
            }
        }


# =====================================================
# FONCTION PRINCIPALE POUR UTILISATION EN CLI
# =====================================================

def scrape_site(
    url: str,
    user_id: str,
    force_refresh: bool = False,
    categories: List[str] = None
) -> Dict[str, Any]:
    """Fonction utilitaire pour scraper un site

    Args:
        url: URL du site
        user_id: ID utilisateur (OBLIGATOIRE - doit √™tre connect√©)
        force_refresh: Forcer la r√©g√©n√©ration
        categories: Cat√©gories √† scraper

    Returns:
        R√©sultats du scraping

    Raises:
        ValueError: Si user_id n'est pas fourni
    """
    if not user_id:
        raise ValueError(
            "‚ùå Authentification requise: vous devez √™tre connect√© pour utiliser le scraper.")

    scraper = IntelligentScraper(user_id=user_id)
    return scraper.scrape(url, force_refresh=force_refresh, categories=categories)
