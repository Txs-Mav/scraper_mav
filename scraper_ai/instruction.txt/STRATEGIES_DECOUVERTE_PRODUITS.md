# üéØ Strat√©gies de D√©couverte de Produits - Guide Complet

Ce document liste **TOUS** les outils et strat√©gies disponibles pour l'agent IA lorsqu'il a de la difficult√© √† trouver un chemin vers tous les produits d'un site.

## üìã Table des Mati√®res

1. [Strat√©gies Principales](#strat√©gies-principales)
2. [Outils de D√©couverte](#outils-de-d√©couverte)
3. [Strat√©gies de Secours](#strat√©gies-de-secours)
4. [Techniques Avanc√©es](#techniques-avanc√©es)
5. [Exemples de Code](#exemples-de-code)

---

## üöÄ Strat√©gies Principales

### 1. **Sitemap (PRIORIT√â ABSOLUE)**

**Outil** : `get_sitemap_urls(base_url)`

**Pourquoi** : Le sitemap contient g√©n√©ralement **TOUTES** les URLs du site, c'est la m√©thode la plus compl√®te.

**Utilisation** :
```python
sitemap_urls = get_sitemap_urls(base_url)
if sitemap_urls:
    all_product_urls.update(sitemap_urls)
    print(f"‚úÖ {len(sitemap_urls)} URLs depuis sitemap (COMPLET)")
```

**Avantages** :
- ‚úÖ Contient g√©n√©ralement tous les produits
- ‚úÖ Rapide (un seul fichier XML)
- ‚úÖ Pas besoin de pagination
- ‚úÖ URLs d√©j√† normalis√©es

**Limitations** :
- ‚ö†Ô∏è Pas tous les sites ont un sitemap
- ‚ö†Ô∏è Parfois incomplet ou obsol√®te

---

### 2. **Pagination EXHAUSTIVE**

**Outil** : `detect_pagination(html, url)` + boucles

**Pourquoi** : Beaucoup de sites utilisent la pagination pour afficher les produits.

**Utilisation** :
```python
# D√©tecter le pattern de pagination
pagination_info = detect_pagination(html, base_url)

if pagination_info:
    page = 1
    max_pages = 1000  # Limite de s√©curit√©
    while page <= max_pages:
        # Construire URL selon pattern d√©tect√©
        if pagination_info['type'] == 'url_params':
            page_url = f"{base_url}?{pagination_info['pattern']}{page}"
        elif pagination_info['type'] == 'path':
            page_url = f"{base_url}{pagination_info['pattern']}{page}"
        else:
            # Pattern personnalis√©
            page_url = f"{base_url}?page={page}"
        
        html = get(page_url)
        if not html or len(html) < 1000:
            break
        
        product_links = discover_product_urls(html, base_url)
        if not product_links:
            break
        
        all_product_urls.update(product_links)
        print(f"   Page {page}: {len(product_links)} produits trouv√©s")
        page += 1
```

**Patterns de pagination courants** :
- `?page=1`, `?page=2`, etc.
- `/page/1/`, `/page/2/`, etc.
- `?p=1`, `?p=2`, etc.
- Bouton "Suivant" avec URL dynamique

---

### 3. **Exploration de Cat√©gories**

**Outil** : `get_all_links(html, base_url)` + filtrage

**Pourquoi** : Les sites organisent souvent les produits par cat√©gories.

**Utilisation** :
```python
# 1. Trouver toutes les cat√©gories
html = get(base_url)
all_links = get_all_links(html, base_url)

# Filtrer pour trouver les cat√©gories
category_keywords = ['category', 'categorie', 'inventory', 'inventaire', 'catalog', 'catalogue']
category_urls = [
    link for link in all_links 
    if any(kw in link.lower() for kw in category_keywords)
    and 'product' not in link.lower()  # Exclure les pages produits individuelles
]

# 2. Pour chaque cat√©gorie, trouver tous les produits
for category_url in category_urls:
    print(f"üìÅ Exploration cat√©gorie: {category_url}")
    
    # G√©rer pagination dans la cat√©gorie
    page = 1
    while page <= 100:
        if '?' in category_url:
            cat_page_url = f"{category_url}&page={page}"
        else:
            cat_page_url = f"{category_url}?page={page}"
        
        html = get(cat_page_url)
        if not html or len(html) < 1000:
            break
        
        products = discover_product_urls(html, base_url)
        if not products:
            break
        
        all_product_urls.update(products)
        page += 1
```

---

### 4. **D√©couverte Heuristique de Produits**

**Outil** : `discover_product_urls(html, base_url)`

**Pourquoi** : Utilise des heuristiques pour identifier les pages de produits.

**Utilisation** :
```python
html = get(base_url)
product_urls = discover_product_urls(html, base_url)
all_product_urls.update(product_urls)
```

**Mots-cl√©s recherch√©s** :
- `product`, `produit`, `item`, `article`
- `inventory`, `inventaire`, `stock`
- `detail`, `details`, `fiche`
- `moto`, `vehicle`, `vehicule`
- `quad`, `atv`, `snowmobile`, `motoneige`

**Mots-cl√©s exclus** :
- `contact`, `about`, `policy`, `privacy`
- `blog`, `news`, `cart`, `checkout`
- `login`, `register`, `account`, `search`

---

## üîç Outils de D√©couverte

### 1. **Exploration de Tous les Liens**

**Outil** : `get_all_links(html, base_url)`

**Utilisation** :
```python
html = get(base_url)
all_links = get_all_links(html, base_url)

# Filtrer manuellement selon les besoins
product_candidates = [
    link for link in all_links
    if '/product/' in link or '/inventory/' in link or '/item/' in link
]
```

---

### 2. **Parsing HTML avec S√©lecteurs CSS**

**Outil** : `parse_html(html, selector)`

**Utilisation** :
```python
# Trouver tous les liens de produits avec un s√©lecteur sp√©cifique
product_links = parse_html(html, "a.product-link[href]")
product_links = parse_html(html, ".product-card a[href]")
product_links = parse_html(html, "[data-product-url]")

# Normaliser les URLs
for link in product_links:
    normalized = normalize_url(base_url, link)
    if normalized:
        all_product_urls.add(normalized)
```

**S√©lecteurs CSS courants** :
- `a[href*="/product/"]` : Liens contenant "/product/"
- `.product-card a` : Liens dans les cartes produits
- `[data-product-id]` : √âl√©ments avec attribut data-product-id
- `.inventory-item a` : Liens d'inventaire

---

### 3. **Extraction de Donn√©es Structur√©es**

**Outil** : `extract_json_ld(html)` et `extract_opengraph(html)`

**Utilisation** :
```python
html = get(base_url)

# JSON-LD peut contenir des listes de produits
json_ld_data = extract_json_ld(html)
for item in json_ld_data:
    if item.get('@type') == 'Product' and 'url' in item:
        all_product_urls.add(item['url'])

# Open Graph peut contenir des URLs de produits
og_data = extract_opengraph(html)
if 'url' in og_data:
    all_product_urls.add(og_data['url'])
```

---

## üÜò Strat√©gies de Secours

### 1. **Exploration R√©cursive**

Si les m√©thodes standards √©chouent, explorer r√©cursivement :

```python
def explore_recursive(url, max_depth=3, current_depth=0):
    if current_depth >= max_depth:
        return []
    
    html = get(url)
    if not html:
        return []
    
    # Trouver tous les liens
    links = get_all_links(html, base_url)
    
    # Filtrer les liens prometteurs
    promising_links = [
        link for link in links
        if any(kw in link.lower() for kw in ['product', 'inventory', 'item', 'detail'])
        and link not in visited_urls
    ]
    
    product_urls = []
    for link in promising_links:
        visited_urls.add(link)
        # V√©rifier si c'est une page produit
        if is_product_page(link):
            product_urls.append(link)
        else:
            # Explorer r√©cursivement
            product_urls.extend(explore_recursive(link, max_depth, current_depth + 1))
    
    return product_urls
```

---

### 2. **Analyse des Patterns d'URL**

Analyser les patterns d'URL pour d√©couvrir des produits :

```python
# R√©cup√©rer quelques URLs de produits connues
known_product_urls = exploration_result.get('product_urls', [])[:5]

if known_product_urls:
    # Analyser le pattern
    patterns = []
    for url in known_product_urls:
        # Extraire le pattern (ex: /product/123, /inventory/item-456)
        pattern = extract_url_pattern(url)
        patterns.append(pattern)
    
    # G√©n√©rer des URLs selon le pattern
    common_pattern = find_common_pattern(patterns)
    if common_pattern:
        # Essayer des IDs s√©quentiels
        for product_id in range(1, 10000):
            test_url = f"{base_url}{common_pattern.format(id=product_id)}"
            html = get(test_url)
            if html and len(html) > 1000:
                all_product_urls.add(test_url)
            else:
                # Arr√™ter si plusieurs URLs cons√©cutives √©chouent
                break
```

---

### 3. **Utilisation de Selenium pour JavaScript**

Si le site charge les produits dynamiquement :

```python
# Utiliser Selenium pour le rendu JavaScript
html = browser_get(base_url)

# Les produits peuvent √™tre charg√©s via AJAX
# Attendre que le contenu soit charg√©, puis extraire
product_links = discover_product_urls(html, base_url)

# Ou utiliser des s√©lecteurs sp√©cifiques
product_elements = parse_html(html, ".product-item[data-url]")
```

---

### 4. **V√©rification robots.txt**

**Outil** : `check_robots_txt(url)`

V√©rifier les restrictions avant d'explorer :

```python
robots_info = check_robots_txt(base_url)
if robots_info.get('exists'):
    # Analyser robots.txt pour voir ce qui est autoris√©
    # √âviter les chemins interdits
    pass
```

---

## üé® Techniques Avanc√©es

### 1. **Combinaison de Plusieurs Strat√©gies**

**IMPORTANT** : Utiliser plusieurs strat√©gies en parall√®le et combiner les r√©sultats :

```python
all_product_urls = set()

# Strat√©gie 1: Sitemap
sitemap_urls = get_sitemap_urls(base_url)
if sitemap_urls:
    all_product_urls.update(sitemap_urls)
    print(f"‚úÖ {len(sitemap_urls)} URLs depuis sitemap")

# Strat√©gie 2: Pagination (m√™me si sitemap existe, pour v√©rification)
page = 1
while page <= 100:
    page_url = f"{base_url}?page={page}"
    html = get(page_url)
    if not html:
        break
    products = discover_product_urls(html, base_url)
    all_product_urls.update(products)
    page += 1

# Strat√©gie 3: Cat√©gories
categories = find_categories(html, base_url)
for cat_url in categories:
    cat_products = discover_product_urls(get(cat_url), base_url)
    all_product_urls.update(cat_products)

# Strat√©gie 4: URLs d√©couvertes lors de l'exploration
explored_urls = exploration_result.get('product_urls', [])
all_product_urls.update(explored_urls)

# D√©dupliquer
all_product_urls = list(set(all_product_urls))
print(f"‚úÖ TOTAL: {len(all_product_urls)} URLs uniques trouv√©es")
```

---

### 2. **Validation et V√©rification**

V√©rifier que les URLs trouv√©es sont bien des pages de produits :

```python
def is_product_page(url, html):
    """V√©rifier si une page est une page produit"""
    # Indicateurs d'une page produit
    indicators = [
        'price' in html.lower(),
        'prix' in html.lower(),
        'add to cart' in html.lower(),
        'ajouter au panier' in html.lower(),
        'product-detail' in html.lower(),
        'inventory-item' in html.lower(),
    ]
    return any(indicators)

# Filtrer les URLs
valid_product_urls = []
for url in all_product_urls:
    html = get(url)
    if is_product_page(url, html):
        valid_product_urls.append(url)
```

---

### 3. **Gestion des Sites avec Chargement Lazy**

Pour les sites qui chargent les produits progressivement :

```python
# Utiliser Selenium pour scroller et charger plus de produits
html = browser_get(base_url)

# Scroller plusieurs fois pour charger le contenu lazy
# (Cette logique devrait √™tre dans le scraper g√©n√©r√©)

# Puis extraire tous les liens
all_links = get_all_links(html, base_url)
product_links = [link for link in all_links if is_product_url(link)]
```

---

## üìù Exemples de Code Complets

### Exemple 1 : Strat√©gie Multi-Couches

```python
def scrape(base_url):
    all_product_urls = set()
    
    # COUCHE 1: Sitemap (le plus fiable)
    sitemap_urls = get_sitemap_urls(base_url)
    if sitemap_urls:
        all_product_urls.update(sitemap_urls)
        print(f"‚úÖ {len(sitemap_urls)} URLs depuis sitemap")
    else:
        print("‚ö†Ô∏è Pas de sitemap, utilisation de strat√©gies alternatives")
    
    # COUCHE 2: Pagination exhaustive
    html = get(base_url)
    pagination_info = detect_pagination(html, base_url)
    
    if pagination_info:
        page = 1
        while page <= 1000:
            page_url = build_pagination_url(base_url, pagination_info, page)
            page_html = get(page_url)
            if not page_html or len(page_html) < 1000:
                break
            products = discover_product_urls(page_html, base_url)
            if not products:
                break
            all_product_urls.update(products)
            print(f"   Page {page}: {len(products)} produits")
            page += 1
    
    # COUCHE 3: Exploration de cat√©gories
    categories = find_all_categories(html, base_url)
    for cat_url in categories:
        cat_products = discover_product_urls(get(cat_url), base_url)
        all_product_urls.update(cat_products)
    
    # COUCHE 4: Liens d√©couverts
    all_links = get_all_links(html, base_url)
    product_candidates = filter_product_links(all_links)
    all_product_urls.update(product_candidates)
    
    # D√©dupliquer et retourner
    all_product_urls = list(set(all_product_urls))
    print(f"‚úÖ TOTAL: {len(all_product_urls)} URLs uniques")
    
    return all_product_urls
```

---

### Exemple 2 : Exploration R√©cursive avec Limites

```python
def scrape_with_recursive_exploration(base_url, max_depth=2):
    visited = set()
    all_product_urls = set()
    
    def explore(url, depth=0):
        if depth > max_depth or url in visited:
            return
        
        visited.add(url)
        html = get(url)
        if not html:
            return
        
        # Chercher produits sur cette page
        products = discover_product_urls(html, base_url)
        all_product_urls.update(products)
        
        # Si pas assez de produits, explorer les liens
        if len(products) < 5 and depth < max_depth:
            links = get_all_links(html, base_url)
            promising_links = [
                link for link in links
                if is_promising_link(link) and link not in visited
            ]
            
            for link in promising_links[:10]:  # Limiter √† 10 liens par page
                explore(link, depth + 1)
    
    explore(base_url)
    return list(set(all_product_urls))
```

---

## ‚úÖ Checklist pour l'Agent IA

Quand l'agent a de la difficult√© √† trouver tous les produits, il devrait :

### Strat√©gies de Base
1. ‚úÖ **Essayer le sitemap en premier** (`get_sitemap_urls`)
2. ‚úÖ **D√©tecter et suivre la pagination** (`detect_pagination` + boucles)
3. ‚úÖ **Explorer les cat√©gories** (`get_all_links` + filtrage)
4. ‚úÖ **Utiliser la d√©couverte heuristique** (`discover_product_urls`)
5. ‚úÖ **Parser avec s√©lecteurs CSS** (`parse_html` avec s√©lecteurs sp√©cifiques)

### Donn√©es Structur√©es
6. ‚úÖ **Extraire donn√©es structur√©es** (`extract_json_ld`, `extract_opengraph`, `extract_microdata`)
7. ‚úÖ **Extraire donn√©es JavaScript** (`extract_script_data`) pour SPA

### Sites Complexes
8. ‚úÖ **D√©tecter APIs** (`detect_api_endpoints`) et appeler les endpoints
9. ‚úÖ **Trouver formulaires de recherche** (`find_search_form`) et les utiliser
10. ‚úÖ **Explorer les filtres** (`find_filters`) et tester diff√©rentes combinaisons
11. ‚úÖ **G√©rer infinite scroll** (`detect_infinite_scroll`) avec Selenium

### Robustesse
12. ‚úÖ **Utiliser retry** (`retry_get`) pour g√©rer les erreurs temporaires
13. ‚úÖ **D√©tecter rate limiting** (`detect_rate_limit`) et attendre (`wait_between_requests`)
14. ‚úÖ **D√©tecter CAPTCHA** (`detect_captcha`) et utiliser Selenium si n√©cessaire
15. ‚úÖ **Explorer r√©cursivement** (si n√©cessaire, avec limites)
16. ‚úÖ **Utiliser Selenium** (`browser_get`) pour JavaScript
17. ‚úÖ **Combiner plusieurs strat√©gies** (ne pas s'arr√™ter √† la premi√®re)
18. ‚úÖ **Valider les URLs trouv√©es** (`validate_url` + v√©rifier que ce sont bien des produits)

---

## üéØ R√©sum√© des Outils Disponibles

### Outils de Base

| Outil | Description | Quand l'utiliser |
|-------|-------------|------------------|
| `get_sitemap_urls(url)` | R√©cup√®re toutes les URLs du sitemap | **TOUJOURS en premier** |
| `detect_pagination(html, url)` | D√©tecte le pattern de pagination | Sites avec pagination |
| `discover_product_urls(html, base_url)` | D√©couvre URLs produits via heuristiques | Exploration g√©n√©rale |
| `get_all_links(html, base_url)` | Tous les liens normalis√©s | Exploration compl√®te |
| `parse_html(html, selector)` | Parse avec s√©lecteurs CSS | Structure HTML connue |
| `browser_get(url)` | HTML rendu avec Selenium | Sites JavaScript |
| `check_robots_txt(url)` | V√©rifie robots.txt | Respect des restrictions |

### Donn√©es Structur√©es

| Outil | Description | Quand l'utiliser |
|-------|-------------|------------------|
| `extract_json_ld(html)` | Donn√©es structur√©es JSON-LD | Sites modernes |
| `extract_opengraph(html)` | M√©tadonn√©es Open Graph | Sites avec OG tags |
| `extract_microdata(html)` | Microdata schema.org | Sites avec microdata |
| `extract_script_data(html)` | Donn√©es depuis JavaScript | SPA (Single Page Apps) |

### Sites Complexes

| Outil | Description | Quand l'utiliser |
|-------|-------------|------------------|
| `detect_api_endpoints(html)` | D√©tecte endpoints API | Sites avec API REST/GraphQL |
| `find_search_form(html)` | Trouve formulaires de recherche | Sites n√©cessitant recherche |
| `find_filters(html)` | Trouve filtres avec options | Sites avec filtres complexes |
| `detect_infinite_scroll(html)` | D√©tecte infinite scroll | Sites modernes avec lazy loading |
| `find_iframes(html)` | Trouve iframes | Sites avec contenu dans iframes |
| `detect_captcha(html)` | D√©tecte CAPTCHA | Sites prot√©g√©s |

### Gestion d'Erreurs & Performance

| Outil | Description | Quand l'utiliser |
|-------|-------------|------------------|
| `retry_get(url, max_retries, backoff, use_selenium)` | Retry avec backoff | Erreurs temporaires |
| `detect_rate_limit(response_text, status_code)` | D√©tecte rate limiting | Sites avec restrictions |
| `wait_between_requests(seconds)` | Attendre entre requ√™tes | √âviter rate limiting |
| `validate_url(url)` | Valide URL | Avant de faire requ√™te |

---

## üÜï Utilisation des Nouveaux Outils pour Sites Complexes

### Sites SPA (Single Page Apps)

```python
# Beaucoup de sites modernes chargent les donn√©es dans JavaScript
html = browser_get(base_url)  # Utiliser Selenium pour le rendu complet

# Extraire donn√©es depuis window.__INITIAL_STATE__ ou similaire
script_data = extract_script_data(html)
if script_data:
    # Les produits peuvent √™tre dans script_data['products'] ou similaire
    products = script_data.get('products', [])
    for product in products:
        if 'url' in product:
            all_product_urls.add(product['url'])
```

### Sites avec API REST/GraphQL

```python
# D√©tecter les endpoints API
api_endpoints = detect_api_endpoints(html)

if api_endpoints:
    for endpoint in api_endpoints:
        # Construire l'URL compl√®te
        if endpoint.startswith('/'):
            api_url = f"{base_url}{endpoint}"
        elif not endpoint.startswith('http'):
            api_url = urljoin(base_url, endpoint)
        else:
            api_url = endpoint
        
        # Appeler l'API avec retry
        response_text = retry_get(api_url, max_retries=3)
        if response_text:
            try:
                api_data = json.loads(response_text)
                # Extraire URLs produits depuis la r√©ponse API
                if isinstance(api_data, dict) and 'products' in api_data:
                    for product in api_data['products']:
                        if 'url' in product:
                            all_product_urls.add(product['url'])
            except json.JSONDecodeError:
                pass
```

### Sites avec Formulaires de Recherche

```python
# Trouver le formulaire de recherche
search_form = find_search_form(html)

if search_form:
    # Essayer diff√©rentes requ√™tes de recherche
    search_queries = ['*', '', 'moto', 'quad', 'snowmobile']
    
    for query in search_queries:
        # Construire l'URL de recherche
        if search_form['method'] == 'get':
            search_url = f"{base_url}{search_form['action']}?{search_form['inputs'][0]['name']}={query}"
        else:
            # POST - n√©cessiterait requests.post()
            continue
        
        search_html = get(search_url)
        products = discover_product_urls(search_html, base_url)
        all_product_urls.update(products)
```

### Sites avec Filtres

```python
# Trouver les filtres disponibles
filters = find_filters(html)

if filters:
    # Explorer diff√©rentes combinaisons de filtres
    # Exemple: pour chaque cat√©gorie, trouver tous les produits
    for filter_item in filters:
        if filter_item['type'] == 'select':
            for option in filter_item['options']:
                filter_url = f"{base_url}?{filter_item['name']}={option['value']}"
                filter_html = get(filter_url)
                products = discover_product_urls(filter_html, base_url)
                all_product_urls.update(products)
```

### Sites avec Infinite Scroll

```python
# D√©tecter infinite scroll
has_infinite_scroll = detect_infinite_scroll(html)

if has_infinite_scroll:
    # Utiliser Selenium pour scroller et charger plus de contenu
    html = browser_get(base_url)
    
    # Scroller plusieurs fois (cette logique devrait √™tre dans le scraper g√©n√©r√©)
    # Puis extraire tous les liens
    all_links = get_all_links(html, base_url)
    product_links = [link for link in all_links if is_product_url(link)]
    all_product_urls.update(product_links)
```

### Gestion du Rate Limiting

```python
# Toujours utiliser wait_between_requests pour √©viter rate limiting
for url in urls_to_fetch:
    html = get(url)
    
    # V√©rifier si on est rate limit√©
    if detect_rate_limit(html, 200):  # status_code devrait √™tre pass√©
        print("‚ö†Ô∏è Rate limit d√©tect√©, attente de 60 secondes...")
        wait_between_requests(60)
        continue
    
    # Attendre entre chaque requ√™te
    wait_between_requests(1.0)  # 1 seconde entre chaque requ√™te
```

### Gestion des Erreurs avec Retry

```python
# Utiliser retry_get pour g√©rer les erreurs temporaires
urls = ['url1', 'url2', 'url3']

for url in urls:
    # Retry jusqu'√† 3 fois avec backoff exponentiel
    html = retry_get(url, max_retries=3, backoff=1.0)
    
    if html:
        products = discover_product_urls(html, base_url)
        all_product_urls.update(products)
    else:
        print(f"‚ùå Impossible de r√©cup√©rer {url} apr√®s plusieurs tentatives")
```

## üí° Conseils Finaux

1. **Ne jamais s'arr√™ter √† une seule strat√©gie** : Combiner plusieurs approches
2. **Toujours d√©dupliquer** : Utiliser `set()` pour √©viter les doublons
3. **Logger les √©tapes** : Utiliser `print()` pour d√©boguer
4. **G√©rer les erreurs** : Utiliser `try/except` et `retry_get()` pour robustesse
5. **Mettre des limites** : √âviter les boucles infinies (max_pages, max_depth)
6. **Valider les r√©sultats** : Utiliser `validate_url()` et v√©rifier que ce sont bien des produits
7. **Respecter les sites** : Utiliser `wait_between_requests()` pour √©viter rate limiting
8. **D√©tecter les obstacles** : Utiliser `detect_captcha()` et `detect_rate_limit()` pour adapter la strat√©gie

---

**Note** : Si aucune de ces strat√©gies ne fonctionne, le site peut avoir une structure tr√®s unique. Dans ce cas, l'agent devrait utiliser Gemini pour analyser le HTML et g√©n√©rer une strat√©gie personnalis√©e bas√©e sur la structure r√©elle du site.

