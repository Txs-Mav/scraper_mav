# Agent IA - Documentation ComplÃ¨te

## ğŸ¯ Vue d'ensemble

L'agent IA analyse automatiquement les sites web et gÃ©nÃ¨re des scrapers spÃ©cifiques pour chaque site. Il utilise Gemini pour comprendre la structure HTML et mapper chaque champ du schÃ©ma aux Ã©lÃ©ments du site.

## ğŸ› ï¸ Outils Disponibles

L'agent IA a accÃ¨s Ã  un ensemble complet d'outils pour explorer les sites :

### 1. Browser / Website Exploration
- `get(url, use_selenium=False)` : RÃ©cupÃ©rer le HTML brut (requests)
- `browser_get(url)` : RÃ©cupÃ©rer le HTML complÃ¨tement rendu (Selenium)

### 2. HTML Parsing
- `parse_html(html, selector)` : Extraire des Ã©lÃ©ments avec sÃ©lecteurs CSS
- `get_all_links(html, base_url)` : Extraire tous les liens normalisÃ©s
- `discover_product_urls(html, base_url)` : DÃ©couvrir les URLs de produits

### 3. Link Normalization
- `normalize_url(base, link)` : Convertir les liens relatifs en URLs complÃ¨tes

### 4. File / Storage
- `save_json(name, data)` : Sauvegarder des donnÃ©es JSON
- `load_json(name)` : Charger des donnÃ©es JSON

### 5. Sitemap Tool
- `get_sitemap_urls(url)` : RÃ©cupÃ©rer toutes les URLs depuis sitemap.xml

### 6. Pagination Detection
- `detect_pagination(html, url)` : DÃ©tecter le pattern de pagination

### 7. DonnÃ©es StructurÃ©es
- `extract_json_ld(html)` : Extraire donnÃ©es JSON-LD
- `extract_opengraph(html)` : Extraire mÃ©tadonnÃ©es Open Graph
- `extract_microdata(html)` : Extraire microdata (schema.org)
- `extract_script_data(html)` : Extraire donnÃ©es depuis variables JavaScript (window.__INITIAL_STATE__, etc.)

### 8. Formulaires & Recherche
- `find_search_form(html)` : Trouver formulaires de recherche
- `find_filters(html)` : Trouver filtres (selects, checkboxes) avec options

### 9. APIs & Endpoints
- `detect_api_endpoints(html)` : DÃ©tecter endpoints API depuis JavaScript

### 10. Gestion AvancÃ©e
- `retry_get(url, max_retries=3, backoff=1.0, use_selenium=False)` : Retry avec backoff exponentiel
- `detect_rate_limit(response_text, status_code)` : DÃ©tecter rate limiting
- `wait_between_requests(seconds=1.0)` : Attendre entre requÃªtes
- `validate_url(url)` : Valider qu'une URL est bien formÃ©e

### 11. DÃ©tection AvancÃ©e
- `detect_infinite_scroll(html)` : DÃ©tecter infinite scroll / lazy loading
- `detect_captcha(html)` : DÃ©tecter prÃ©sence de CAPTCHA
- `find_iframes(html)` : Trouver toutes les iframes

### 12. Utilitaires
- `clean_text(text)` : Nettoyer texte (espaces, caractÃ¨res spÃ©ciaux)
- `extract_price(text)` : Extraire prix depuis texte
- `extract_number(text)` : Extraire n'importe quel nombre depuis texte
- `get_text_content(html, selector=None)` : Extraire texte brut depuis HTML
- `check_robots_txt(url)` : VÃ©rifier robots.txt

### 13. Python Code Writer
- `write_file(path, content)` : Ã‰crire le code du scraper

## ğŸ”„ Workflow de l'Agent

1. **Exploration Automatique**
   - Extraction de tous les liens
   - DÃ©tection des URLs de produits
   - DÃ©tection de la pagination
   - Recherche du sitemap
   - Analyse de pages clÃ©s

2. **Analyse avec Gemini**
   - Gemini reÃ§oit tout le contexte d'exploration
   - Analyse la structure HTML
   - Identifie les patterns de pagination
   - Mappe chaque champ du schÃ©ma au HTML

3. **GÃ©nÃ©ration du Scraper**
   - Code Python gÃ©nÃ©rÃ© avec stratÃ©gies flexibles
   - Gestion de plusieurs approches (sitemap, navigation, pagination)
   - Mapping complet des champs
   - Code prÃªt Ã  exÃ©cuter

4. **Cache**
   - Scraper sauvegardÃ© dans `scraper_cache/`
   - RÃ©utilisation pour les prochaines fois

## ğŸ¨ FlexibilitÃ© Maximale

L'agent peut utiliser **n'importe quelle combinaison** de stratÃ©gies :

- âœ… Sitemap (si disponible)
- âœ… Navigation + Pagination
- âœ… Exploration de catÃ©gories
- âœ… URLs de produits dÃ©tectÃ©es
- âœ… APIs REST/GraphQL (si dÃ©tectÃ©es)
- âœ… DonnÃ©es dans JavaScript (SPA)
- âœ… Formulaires de recherche
- âœ… Filtres et combinaisons
- âœ… Combinaison de plusieurs approches

## ğŸŒ Types de Sites SupportÃ©s

L'agent peut maintenant gÃ©rer :

- âœ… Sites statiques classiques
- âœ… Sites avec pagination
- âœ… Sites SPA (Single Page Apps) avec donnÃ©es dans JavaScript
- âœ… Sites avec APIs REST/GraphQL
- âœ… Sites avec infinite scroll / lazy loading
- âœ… Sites avec formulaires de recherche
- âœ… Sites avec filtres complexes
- âœ… Sites avec CAPTCHA (dÃ©tection)
- âœ… Sites avec rate limiting (gestion automatique)

## ğŸ“‹ Mapping des Champs

Chaque champ du schÃ©ma est mappÃ© aux Ã©lÃ©ments HTML :

- `name` : Titre du produit
- `marque` : Marque du vÃ©hicule
- `modele` : ModÃ¨le du vÃ©hicule
- `prix` : Prix (OBLIGATOIRE)
- `image` : Image du produit
- `disponibilite` : DisponibilitÃ©
- `annee` : AnnÃ©e
- `kilometrage` : KilomÃ©trage
- `category` : CatÃ©gorie
- `sourceUrl` : URL de la page
- `sourceSite` : Site source
- `sourceCategorie` : CatÃ©gorie source

## ğŸš€ Utilisation

### Depuis le Dashboard
1. Aller dans l'onglet "Agent IA"
2. Entrer une URL
3. Cliquer sur "Analyser"
4. L'agent explore et gÃ©nÃ¨re le scraper
5. ExÃ©cuter le scraper depuis l'interface

### Automatique
Quand une nouvelle URL est scrapÃ©e, l'agent vÃ©rifie automatiquement le cache et gÃ©nÃ¨re un scraper si nÃ©cessaire.

## ğŸ“ Structure des Fichiers

```
scraper_ai/
â”œâ”€â”€ ai_tools.py          # Outils pour l'exploration
â”œâ”€â”€ html_analyzer.py     # Analyse et gÃ©nÃ©ration de scrapers
â”œâ”€â”€ scraper_executor.py  # ExÃ©cution des scrapers gÃ©nÃ©rÃ©s
â”œâ”€â”€ gemini_client.py     # Client Gemini
â”œâ”€â”€ config.py            # Configuration
â””â”€â”€ AGENT_IA_README.md   # Ce fichier

scraper_cache/           # Cache des scrapers gÃ©nÃ©rÃ©s
â””â”€â”€ {hash}.json         # Scrapers par site
```

## âœ… VÃ©rifications

- âœ… Tous les outils sont disponibles dans le namespace d'exÃ©cution
- âœ… Exploration flexible et exhaustive
- âœ… Prompt optimisÃ© pour la flexibilitÃ©
- âœ… Gestion des cas limites
- âœ… Cache pour performance
- âœ… Interface utilisateur dans le dashboard

