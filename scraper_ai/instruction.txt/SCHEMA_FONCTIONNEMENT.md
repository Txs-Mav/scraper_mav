# ğŸ¯ SchÃ©ma DÃ©taillÃ© du Fonctionnement du Scraper AI

## ğŸ“Š Vue d'Ensemble SimplifiÃ©e

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FLUX PRINCIPAL                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. ENTRÃ‰E
   â”œâ”€> main.py (CLI)
   â””â”€> Dashboard Web (Next.js)

2. CACHE ?
   â”œâ”€> OUI â†’ VÃ©rifie scraper Python (.py)
   â”‚   â”œâ”€> Scraper existe â†’ Utilise cache â†’ Ã‰TAPE 5
   â”‚   â””â”€> Scraper manquant mais donnÃ©es existent â†’ RÃ©gÃ©nÃ¨re scraper â†’ Ã‰TAPE 4
   â”‚
   â””â”€> NON â†’ Continue

3. EXPLORATION (ExplorationAgent)
       â””â”€> Gemini + AITools explorent le site
       â”œâ”€> DÃ©couvre URLs de produits (sitemap, pagination, navigation)
       â”œâ”€> RÃ©cupÃ¨re HTML de 20 pages Ã©chantillons
       â”œâ”€> Extrait infos produits via Gemini
       â”œâ”€> DÃ©tecte sÃ©lecteurs CSS automatiquement
       â””â”€> Retourne: product_urls, html_samples, extracted_products, detected_selectors

4. STOCKAGE (SiteDataStorage)
   â””â”€> Sauvegarde donnÃ©es structurÃ©es dans {cache_key}_data.json
       â””â”€> Contient: URLs, HTML Ã©chantillons, sÃ©lecteurs, structure du site

5. GÃ‰NÃ‰RATION (ScraperGenerator avec Gemini)
   â””â”€> Gemini gÃ©nÃ¨re un scraper Python personnalisÃ©
       â”œâ”€> Utilise le template comme rÃ©fÃ©rence structurelle
       â”œâ”€> Hardcode TOUTES les URLs de produits dans PRODUCT_URLS
       â”œâ”€> Hardcode TOUS les sÃ©lecteurs CSS dans SELECTORS
       â”œâ”€> Adapte le code au site spÃ©cifique
       â””â”€> Sauvegarde dans {cache_key}_scraper.py

6. EXÃ‰CUTION (ScraperExecutor)
   â””â”€> ExÃ©cute le code Python gÃ©nÃ©rÃ©
           â”‚
       â”œâ”€> Scraping parallÃ¨le (20 threads)
       â”œâ”€> Utilise les sÃ©lecteurs hardcodÃ©s
       â”œâ”€> Fallbacks robustes si sÃ©lecteurs Ã©chouent
       â””â”€> Retourne rÃ©sultats

7. SORTIE
   â””â”€> {companyInfo: {...}, products: [...]}
```

## ğŸ“Š Vue d'Ensemble du SystÃ¨me

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   main.py    â”‚  â† Point d'entrÃ©e (CLI)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ScraperExecutor     â”‚  â† Orchestrateur principal
â”‚ - scrape_site()     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                     â”‚
       â–¼                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HTMLAnalyzer     â”‚              â”‚ execute_scraper() â”‚
â”‚ - Analyse site   â”‚              â”‚ - ExÃ©cute code   â”‚
â”‚ - GÃ©nÃ¨re scraper â”‚              â”‚ - Scraping local â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                     â”‚
       â–¼                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ExplorationAgent â”‚              â”‚ ScraperGenerator  â”‚
â”‚ - DÃ©couvre URLs  â”‚              â”‚ - GÃ©nÃ¨re avec     â”‚
â”‚ - Extrait infos  â”‚              â”‚   Gemini         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                     â”‚
       â–¼                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AITools          â”‚              â”‚ GeminiClient      â”‚
â”‚ - Outils web     â”‚              â”‚ - Appels API     â”‚
â”‚ - Exploration    â”‚              â”‚ - GÃ©nÃ©ration     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SiteDataStorage  â”‚
â”‚ - Cache JSON     â”‚
â”‚ - {key}_data.jsonâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Flux Principal de Scraping

### Ã‰tape 1 : Initialisation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 1: DÃ‰MARRAGE                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

main.py (CLI) OU Dashboard Web
  â”‚
  â”œâ”€> Parse arguments (--force-refresh, --invalidate-cache)
  â”‚
  â””â”€> CrÃ©e ScraperExecutor()
      â”‚
      â””â”€> ScraperExecutor.__init__()
          â”œâ”€> CrÃ©e session requests
          â””â”€> CrÃ©e HTMLAnalyzer()
              â”œâ”€> CrÃ©e GeminiClient()
              â””â”€> Initialise cache_dir
```

### Ã‰tape 2 : VÃ©rification du Cache

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 2: VÃ‰RIFICATION CACHE                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ScraperExecutor.scrape_site(url, force_refresh=False)
  â”‚
  â”œâ”€> Si force_refresh == True
  â”‚   â””â”€> Skip cache, va directement Ã  Ã‰TAPE 3
  â”‚
  â””â”€> Sinon
      â”‚
      â””â”€> HTMLAnalyzer.analyze_and_generate_scraper()
          â”‚
          â”œâ”€> VÃ©rifie scraper Python (.py)
          â”‚   â””â”€> _load_cached_scraper()
          â”‚       â”œâ”€> Lit cache/{cache_key}_scraper.py
          â”‚       â”‚
          â”‚       â”œâ”€> Si scraper existe
          â”‚       â”‚   â””â”€> Retourne scraper_data â†’ Skip Ã  Ã‰TAPE 6
          â”‚       â”‚
          â”‚       â””â”€> Si scraper n'existe pas
          â”‚           â”‚
          â”‚           â””â”€> VÃ©rifie donnÃ©es d'exploration
          â”‚               â””â”€> SiteDataStorage.load_site_data()
          â”‚                   â”œâ”€> Lit cache/{cache_key}_data.json
          â”‚                   â”‚
          â”‚                   â”œâ”€> Si donnÃ©es existent
          â”‚                   â”‚   â””â”€> RÃ©utilise donnÃ©es â†’ Va Ã  Ã‰TAPE 5
          â”‚                   â”‚
          â”‚                   â””â”€> Si pas de donnÃ©es
          â”‚                       â””â”€> Continue Ã  Ã‰TAPE 3
```

### Ã‰tape 3 : Exploration du Site (ExplorationAgent)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 3: EXPLORATION AVEC EXPLORATIONAGENT                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ExplorationAgent.explore_and_extract(url, initial_html)
  â”‚
  â”œâ”€> 1. DÃ‰COUVRIR TOUTES LES URLs DE PRODUITS
  â”‚   â””â”€> _discover_product_urls()
  â”‚       â”‚
  â”‚       â”œâ”€> Sitemap (prioritÃ© absolue)
  â”‚       â”‚   â””â”€> get_sitemap_urls() â†’ Filtre URLs non-produits
  â”‚       â”‚
  â”‚       â”œâ”€> Pagination
  â”‚       â”‚   â””â”€> detect_pagination() â†’ Boucle pagination
  â”‚       â”‚
  â”‚       â”œâ”€> Navigation / CatÃ©gories
  â”‚       â”‚   â””â”€> discover_product_urls() â†’ Filtre URLs non-produits
  â”‚       â”‚
  â”‚       â””â”€> Validation stricte
  â”‚           â””â”€> _is_valid_product_url() â†’ Exclut service, article, blog, etc.
  â”‚
  â”œâ”€> 2. RÃ‰CUPÃ‰RER LE HTML DE CHAQUE URL PRODUIT
  â”‚   â””â”€> _fetch_product_html()
  â”‚       â”‚
  â”‚       â””â”€> Limite Ã  20 pages Ã©chantillons (pour Ã©viter rate limiting)
  â”‚           â””â”€> Chaque HTML limitÃ© Ã  50 000 caractÃ¨res
  â”‚
  â”œâ”€> 3. UTILISER GEMINI POUR EXTRAIRE LES INFOS PRODUITS
  â”‚   â””â”€> _extract_with_gemini()
  â”‚       â”‚
  â”‚       â”œâ”€> Limite Ã  10 pages pour l'extraction Gemini
  â”‚       â”œâ”€> Prompt Gemini avec HTML Ã©chantillons
  â”‚       â””â”€> Retourne: extracted_products, detected_selectors
  â”‚
  â”œâ”€> 4. DÃ‰TECTER LES SÃ‰LECTEURS CSS AUTOMATIQUEMENT
  â”‚   â””â”€> _detect_selectors()
  â”‚       â”‚
  â”‚       â””â”€> Analyse HTML + produits extraits
  â”‚           â””â”€> Identifie sÃ©lecteurs CSS pour chaque champ
  â”‚
  â””â”€> 5. ANALYSER LA STRUCTURE DU SITE
      â””â”€> _analyze_site_structure()
          â”‚
          â””â”€> Retourne: structure_type, domain, etc.

Retourne:
  - product_urls: Liste complÃ¨te des URLs de produits
  - html_samples: Dictionnaire {url: html} (max 20 pages)
  - extracted_products: Liste des produits extraits
  - detected_selectors: Dictionnaire des sÃ©lecteurs CSS
  - site_structure: Informations sur la structure
```

### Ã‰tape 4 : Stockage StructurÃ© (SiteDataStorage)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 4: STOCKAGE STRUCTURÃ‰                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SiteDataStorage.save_site_data()
  â”‚
  â””â”€> Ã‰crit cache/{cache_key}_data.json
      â”‚
      â””â”€> Contient:
          â”œâ”€> site_url
          â”œâ”€> exploration_date
          â”œâ”€> product_urls: [toutes les URLs dÃ©couvertes]
          â”œâ”€> html_samples: {url: html_content} (max 20 pages)
          â”œâ”€> extracted_products: [produits extraits par Gemini]
          â”œâ”€> detected_selectors: {champ: sÃ©lecteur_css}
          â”œâ”€> site_structure: {structure_type, domain, ...}
          â””â”€> metadata: {data_version: "1.0", ...}
```

### Ã‰tape 5 : GÃ©nÃ©ration du Scraper (ScraperGenerator avec Gemini)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 5: GÃ‰NÃ‰RATION DU SCRAPER PAR GEMINI                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ScraperGenerator.generate_scraper(site_data)
  â”‚
  â”œâ”€> Charge le template (scraper_template.py)
  â”‚   â””â”€> Template sert de RÃ‰FÃ‰RENCE structurelle uniquement
  â”‚
  â”œâ”€> PrÃ©pare les donnÃ©es pour Gemini:
  â”‚   â”œâ”€> product_urls: Liste complÃ¨te (limitÃ© Ã  500 pour le prompt)
  â”‚   â”œâ”€> detected_selectors: Tous les sÃ©lecteurs CSS
  â”‚   â”œâ”€> html_samples: Ã‰chantillons HTML (max 20 pages, 30k chars chacune)
  â”‚   â””â”€> extracted_products: Produits extraits (max 10 pour rÃ©fÃ©rence)
  â”‚
  â””â”€> Appelle Gemini avec prompt complet
      â”‚
      â”œâ”€> Prompt contient:
      â”‚   â”œâ”€> Template de rÃ©fÃ©rence (structure de base)
      â”‚   â”œâ”€> URLs de produits Ã  hardcoder
      â”‚   â”œâ”€> SÃ©lecteurs CSS Ã  hardcoder
      â”‚   â”œâ”€> Ã‰chantillons HTML (pour comprendre la structure)
      â”‚   â””â”€> Produits extraits (pour rÃ©fÃ©rence)
      â”‚
      â”œâ”€> Instructions Ã  Gemini:
      â”‚   â”œâ”€> GÃ©nÃ¨re un scraper Python complet et autonome
      â”‚   â”œâ”€> Hardcode TOUTES les URLs dans PRODUCT_URLS
      â”‚   â”œâ”€> Hardcode TOUS les sÃ©lecteurs dans SELECTORS
      â”‚   â”œâ”€> Utilise le template comme rÃ©fÃ©rence mais adapte au site
      â”‚   â”œâ”€> Support Selenium pour JavaScript
      â”‚   â”œâ”€> Scraping parallÃ¨le (20 threads)
      â”‚   â”œâ”€> Fallbacks robustes pour nom, prix, image
      â”‚   â””â”€> Valide que les produits ont un nom valide (pas de labels)
      â”‚
      â””â”€> Gemini gÃ©nÃ¨re le code Python
          â”‚
          â”œâ”€> Code gÃ©nÃ©rÃ© contient:
          â”‚   â”œâ”€> PRODUCT_URLS = [url1, url2, ...]  # HardcodÃ©
          â”‚   â”œâ”€> SELECTORS = {"name": "...", ...}   # HardcodÃ©
          â”‚   â”œâ”€> def scrape(base_url): ...
          â”‚   â”œâ”€> Scraping parallÃ¨le avec ThreadPoolExecutor
          â”‚   â”œâ”€> Extraction avec sÃ©lecteurs hardcodÃ©s
          â”‚   â”œâ”€> Fallbacks robustes
          â”‚   â””â”€> Support Selenium si nÃ©cessaire
          â”‚
          â””â”€> Sauvegarde dans cache/{cache_key}_scraper.py
              â”‚
              â””â”€> MÃ©tadonnÃ©es en commentaires:
                  â”œâ”€> Version prompt
                  â”œâ”€> Cache key
                  â”œâ”€> Site URL
                  â”œâ”€> Date gÃ©nÃ©ration
                  â””â”€> URLs et sÃ©lecteurs count
```

### Ã‰tape 6 : ExÃ©cution du Scraper

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 6: EXÃ‰CUTION DU SCRAPER GÃ‰NÃ‰RÃ‰                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ScraperExecutor.execute_scraper(url, scraper_data)
  â”‚
  â”œâ”€> Charge le code Python depuis scraper_data
  â”‚
  â”œâ”€> CrÃ©e un namespace d'exÃ©cution
  â”‚   â”œâ”€> requests, BeautifulSoup, re, etc.
  â”‚   â”œâ”€> ThreadPoolExecutor (pour parallÃ©lisme)
  â”‚   â””â”€> Selenium (si nÃ©cessaire)
  â”‚
  â””â”€> ExÃ©cute le code gÃ©nÃ©rÃ©
      â”‚
      â””â”€> exec(scraper_code, namespace)
          â”‚
          â””â”€> Appelle scrape(base_url)
              â”‚
              â”œâ”€> Utilise PRODUCT_URLS hardcodÃ©
              â”‚
              â”œâ”€> Scraping parallÃ¨le (20 threads)
              â”‚   â””â”€> ThreadPoolExecutor(max_workers=20)
              â”‚
              â”œâ”€> Pour chaque URL:
              â”‚   â”œâ”€> RÃ©cupÃ¨re HTML (requests ou Selenium)
              â”‚   â”œâ”€> Utilise SELECTORS hardcodÃ©
              â”‚   â”œâ”€> Fallbacks si sÃ©lecteur Ã©choue
              â”‚   â””â”€> Valide nom (rejette labels comme "Nom complet : *")
              â”‚
              â””â”€> Retourne rÃ©sultats
                  â”‚
                  â””â”€> {companyInfo: {...}, products: [...]}
```

---

## ğŸŒ Flux depuis le Dashboard Web

### Lancement du Scraping depuis le Dashboard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LANCEMENT DEPUIS LE DASHBOARD                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Interface Utilisateur (React)
   â””â”€> scraper-config.tsx
       â”‚
       â””â”€> Utilisateur clique sur "Lancer le scraping"
           â”‚
           â””â”€> Appelle /api/scraper-ai/run

2. API Route Next.js
   â””â”€> dashboard_web/src/app/api/scraper-ai/run/route.ts
       â”‚
       â”œâ”€> ReÃ§oit: {urls: [...], referenceUrl: "..."}
       â”‚
       â”œâ”€> Lance processus Python en arriÃ¨re-plan
       â”‚   â””â”€> nohup python -m scraper_ai.main ...
       â”‚
       â”œâ”€> DÃ©tache le processus (nohup)
       â”‚   â””â”€> Le serveur Next.js ne bloque pas
       â”‚
       â”œâ”€> Sauvegarde PID dans scraper_logs/{timestamp}.lock
       â”‚
       â””â”€> Retourne: {pid: ..., message: "Scraping dÃ©marrÃ©"}

3. Processus Python (en arriÃ¨re-plan)
   â””â”€> scraper_ai/main.py
       â”‚
       â”œâ”€> Pour chaque URL:
       â”‚   â””â”€> ScraperExecutor.scrape_site(url)
       â”‚       â”‚
       â”‚       â”œâ”€> VÃ©rifie cache (Ã‰TAPE 2)
       â”‚       â”œâ”€> Exploration si nÃ©cessaire (Ã‰TAPE 3)
       â”‚       â”œâ”€> GÃ©nÃ©ration scraper (Ã‰TAPE 5)
       â”‚       â””â”€> ExÃ©cution scraper (Ã‰TAPE 6)
       â”‚
       â””â”€> Sauvegarde rÃ©sultats dans scraped_data.json

4. Polling du Statut (Dashboard)
   â””â”€> scraper-config.tsx (useEffect)
          â”‚
       â”œâ”€> Poll /api/scraper/status?pid={pid} toutes les 5 secondes
       â”‚
       â”œâ”€> VÃ©rifie si processus est encore en cours
       â”‚   â””â”€> isProcessRunning(pid)
          â”‚
       â”œâ”€> Lit scraped_data.json pour compter produits
       â”‚
       â””â”€> Affiche statut en temps rÃ©el:
           â”œâ”€> "â³ Scraping en cours... X produits extraits"
           â””â”€> "âœ… Scraping terminÃ©! X produits extraits"
```

### Suppression d'un Scraper depuis le Dashboard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SUPPRESSION D'UN SCRAPER                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Interface Utilisateur (React)
   â””â”€> Utilisateur clique sur "Supprimer" pour un scraper
       â”‚
       â””â”€> Appelle DELETE /api/scraper-ai/cache?key={cacheKey}

2. API Route Next.js
   â””â”€> dashboard_web/src/app/api/scraper-ai/cache/route.ts
  â”‚
       â”œâ”€> ReÃ§oit: cacheKey ou url
       â”‚
       â”œâ”€> Cherche fichier dans scraper_cache/
       â”‚   â”œâ”€> Si cacheKey fourni:
       â”‚   â”‚   â””â”€> Supprime {cacheKey}.json
       â”‚   â”‚
       â”‚   â””â”€> Si url fourni:
       â”‚       â””â”€> Parcourt tous les fichiers .json
       â”‚           â””â”€> Trouve celui avec metadata.url === url
       â”‚               â””â”€> Supprime le fichier
       â”‚
       â””â”€> Retourne: {success: true, deleted: true}

3. Fichiers SupprimÃ©s
   â””â”€> scraper_cache/{cache_key}.json
       â”‚
       â””â”€> NOTE: Le fichier {cache_key}_scraper.py n'est PAS supprimÃ©
           â””â”€> Il reste dans le cache mais ne sera plus utilisÃ©
               (car _load_cached_scraper cherche d'abord le .py)

4. Impact sur le Prochain Scrape
   â””â”€> Lors du prochain scrape du mÃªme site:
       â”‚
       â”œâ”€> HTMLAnalyzer.analyze_and_generate_scraper()
       â”‚   â”‚
       â”‚   â”œâ”€> _load_cached_scraper()
       â”‚   â”‚   â””â”€> Cherche {cache_key}_scraper.py
       â”‚   â”‚       â”‚
       â”‚   â”‚       â”œâ”€> Si .py existe (pas supprimÃ©)
       â”‚   â”‚       â”‚   â””â”€> Utilise le scraper en cache â†’ Ã‰TAPE 6
       â”‚   â”‚       â”‚
       â”‚   â”‚       â””â”€> Si .py n'existe pas
       â”‚   â”‚           â”‚
       â”‚   â”‚           â””â”€> SiteDataStorage.load_site_data()
       â”‚   â”‚               â””â”€> Cherche {cache_key}_data.json
       â”‚   â”‚                   â”‚
       â”‚   â”‚                   â”œâ”€> Si _data.json existe
       â”‚   â”‚                   â”‚   â””â”€> RÃ©utilise donnÃ©es â†’ Ã‰TAPE 5
       â”‚   â”‚                   â”‚       (RÃ©gÃ©nÃ¨re scraper sans re-exploration)
       â”‚   â”‚                   â”‚
       â”‚   â”‚                   â””â”€> Si _data.json n'existe pas
       â”‚   â”‚                       â””â”€> Exploration complÃ¨te â†’ Ã‰TAPE 3
       â”‚   â”‚
       â”‚   â””â”€> Si force_refresh == True
       â”‚       â””â”€> Skip cache â†’ Exploration complÃ¨te â†’ Ã‰TAPE 3
```

### Analyse d'un Site depuis le Dashboard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ANALYSE D'UN SITE                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Interface Utilisateur (React)
   â””â”€> ai-agent.tsx
       â”‚
       â””â”€> Utilisateur entre une URL et clique sur "Analyser"
           â”‚
           â””â”€> Appelle POST /api/scraper-ai/analyze

2. API Route Next.js
   â””â”€> dashboard_web/src/app/api/scraper-ai/analyze/route.ts
       â”‚
       â”œâ”€> ReÃ§oit: {url: "https://example.com"}
       â”‚
       â”œâ”€> Lance HTMLAnalyzer.analyze_and_generate_scraper()
       â”‚   â”‚
       â”‚   â”œâ”€> VÃ©rifie cache (Ã‰TAPE 2)
       â”‚   â”œâ”€> Exploration si nÃ©cessaire (Ã‰TAPE 3)
       â”‚   â”œâ”€> Stockage donnÃ©es (Ã‰TAPE 4)
       â”‚   â””â”€> GÃ©nÃ©ration scraper (Ã‰TAPE 5)
       â”‚
       â””â”€> Retourne: {scraperCode, siteAnalysis, fieldMappings}

3. Affichage dans le Dashboard
   â””â”€> Affiche les rÃ©sultats de l'analyse:
       â”œâ”€> Nom du site
       â”œâ”€> Type de structure
       â”œâ”€> SÃ©lecteurs dÃ©tectÃ©s
       â””â”€> Code du scraper gÃ©nÃ©rÃ©
```

---

## ğŸ“¦ SystÃ¨me de Cache et Fichiers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SYSTÃˆME DE CACHE                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STRUCTURE DES FICHIERS:

scraper_cache/
â”œâ”€â”€ {cache_key}_data.json          â† DonnÃ©es d'exploration
â”‚   â”œâ”€> product_urls: [...]
â”‚   â”œâ”€> html_samples: {...}
â”‚   â”œâ”€> extracted_products: [...]
â”‚   â”œâ”€> detected_selectors: {...}
â”‚   â”œâ”€> site_structure: {...}
â”‚   â””â”€> metadata: {data_version: "1.0", ...}
â”‚
â””â”€â”€ {cache_key}_scraper.py         â† Scraper Python gÃ©nÃ©rÃ©
    â”œâ”€> MÃ©tadonnÃ©es en commentaires
    â”œâ”€> PRODUCT_URLS = [...]       (hardcodÃ©)
    â”œâ”€> SELECTORS = {...}          (hardcodÃ©)
    â””â”€> def scrape(base_url): ...  (code personnalisÃ©)

scraper_logs/
â””â”€â”€ {timestamp}.lock               â† Lock file pour processus en cours
    â”œâ”€> pid: 12345
    â”œâ”€> startTime: 1234567890
    â”œâ”€> urls: [...]
    â””â”€> referenceUrl: "..."

scraped_data.json                  â† RÃ©sultats finaux
â””â”€â”€ {
      "https://site1.com": {
        "companyInfo": {...},
        "products": [...]
      },
      ...
    }
```

### Logique de Cache

```
CHARGEMENT DU CACHE:

1. Cherche {cache_key}_scraper.py
   â”œâ”€> Si existe â†’ Utilise scraper en cache â†’ Ã‰TAPE 6
   â””â”€> Si n'existe pas â†’ Continue

2. Cherche {cache_key}_data.json
   â”œâ”€> Si existe â†’ RÃ©utilise donnÃ©es â†’ Ã‰TAPE 5 (rÃ©gÃ©nÃ¨re scraper)
   â””â”€> Si n'existe pas â†’ Exploration complÃ¨te â†’ Ã‰TAPE 3

INVALIDATION DU CACHE:

1. Suppression manuelle (Dashboard)
   â””â”€> Supprime {cache_key}.json (ancien format)
       â””â”€> Le .py reste mais n'est plus utilisÃ© si .json manquant

2. Force refresh (--force-refresh)
   â””â”€> Skip cache â†’ Exploration complÃ¨te

3. Version diffÃ©rente (PROMPT_VERSION)
   â””â”€> Invalide automatiquement â†’ RÃ©gÃ©nÃ¨re
```

---

## ğŸ” DÃ©tail : ExplorationAgent

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EXPLORATIONAGENT - DÃ‰COUVERTE DES URLs                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ExplorationAgent._discover_product_urls()
  â”‚
  â”œâ”€> 1. SITEMAP (PrioritÃ© absolue)
  â”‚   â””â”€> get_sitemap_urls(url)
  â”‚       â”‚
  â”‚       â”œâ”€> Cherche sitemap.xml
  â”‚       â”œâ”€> Parse sitemap
  â”‚       â””â”€> Filtre URLs non-produits
  â”‚           â””â”€> _is_valid_product_url()
  â”‚               â”œâ”€> Exclut: /service, /article, /blog, /contact, etc.
  â”‚               â””â”€> Inclut seulement: URLs avec indicateurs produits
  â”‚
  â”œâ”€> 2. PAGINATION
  â”‚   â””â”€> detect_pagination()
  â”‚       â”‚
  â”‚       â”œâ”€> DÃ©tecte pattern de pagination
  â”‚       â”œâ”€> Boucle pagination (max 200 pages)
  â”‚       â””â”€> discover_product_urls() sur chaque page
  â”‚
  â”œâ”€> 3. NAVIGATION / CATÃ‰GORIES
  â”‚   â””â”€> discover_product_urls()
  â”‚       â”‚
  â”‚       â”œâ”€> Parse HTML
  â”‚       â”œâ”€> Extrait liens
  â”‚       â””â”€> Filtre URLs non-produits
  â”‚
  â””â”€> 4. VALIDATION STRICTE
      â””â”€> _is_valid_product_url()
  â”‚
          â”œâ”€> Exclut segments:
          â”‚   â”œâ”€> /service, /services, /sav
          â”‚   â”œâ”€> /article, /articles, /blog
          â”‚   â”œâ”€> /contact, /about, /a-propos
          â”‚   â””â”€> /politique, /cgv, /mentions-legales
  â”‚
          â””â”€> Inclut seulement si:
              â”œâ”€> Contient indicateurs produits (moto, inventaire, etc.)
              â””â”€> Format structurÃ© (chiffres, segments multiples)
```

---

## ğŸ› ï¸ DÃ©tail : GÃ©nÃ©ration avec Gemini

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCRAPERGENERATOR - GÃ‰NÃ‰RATION AVEC GEMINI                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ScraperGenerator._generate_with_gemini()
  â”‚
  â”œâ”€> PrÃ©pare le prompt:
  â”‚   â”œâ”€> Template de rÃ©fÃ©rence (structure de base)
  â”‚   â”œâ”€> URLs de produits (JSON, max 500)
  â”‚   â”œâ”€> SÃ©lecteurs CSS (JSON)
  â”‚   â”œâ”€> Ã‰chantillons HTML (max 10 pages, 5k chars chacune)
  â”‚   â””â”€> Produits extraits (max 10, pour rÃ©fÃ©rence)
  â”‚
  â”œâ”€> Instructions Ã  Gemini:
  â”‚   â”œâ”€> GÃ©nÃ¨re scraper Python complet et autonome
  â”‚   â”œâ”€> Hardcode TOUTES les URLs dans PRODUCT_URLS
  â”‚   â”œâ”€> Hardcode TOUS les sÃ©lecteurs dans SELECTORS
  â”‚   â”œâ”€> Utilise template comme rÃ©fÃ©rence mais adapte
  â”‚   â”œâ”€> Support Selenium pour JavaScript
  â”‚   â”œâ”€> Scraping parallÃ¨le (20 threads)
  â”‚   â”œâ”€> Fallbacks robustes (nom, prix, image)
  â”‚   â””â”€> Valide nom (rejette labels)
  â”‚
  â”œâ”€> Appelle Gemini:
  â”‚   â””â”€> gemini_client.call(prompt, response_mime_type="text/plain")
  â”‚       â”‚
  â”‚       â””â”€> Retourne code Python brut
  â”‚
  â”œâ”€> Nettoie le code:
  â”‚   â”œâ”€> EnlÃ¨ve markdown code blocks si prÃ©sents
  â”‚   â””â”€> Valide syntaxe Python
  â”‚
  â””â”€> Fallback si Gemini Ã©choue:
      â””â”€> _generate_fallback()
          â”‚
          â””â”€> Utilise template.format() avec Ã©chappement correct
```

---

## ğŸ¯ Cas d'Usage Typiques

### Cas 1 : Premier Scrape (Pas de Cache)

```
1. Exploration complÃ¨te (ExplorationAgent)
   â”œâ”€> DÃ©couverte URLs (sitemap, pagination, navigation)
   â”œâ”€> RÃ©cupÃ©ration HTML (20 pages Ã©chantillons)
   â”œâ”€> Extraction Gemini (10 pages)
   â””â”€> DÃ©tection sÃ©lecteurs CSS

2. Stockage donnÃ©es (SiteDataStorage)
   â””â”€> Sauvegarde {cache_key}_data.json

3. GÃ©nÃ©ration scraper (ScraperGenerator avec Gemini)
   â””â”€> Gemini gÃ©nÃ¨re scraper personnalisÃ©
       â””â”€> Sauvegarde {cache_key}_scraper.py

4. ExÃ©cution scraper
   â””â”€> Scraping parallÃ¨le (20 threads)
       â””â”€> Utilise URLs et sÃ©lecteurs hardcodÃ©s
```

### Cas 2 : Scrape Suivant (Cache Existant)

```
1. VÃ©rification cache
   â”œâ”€> {cache_key}_scraper.py existe
   â””â”€> Utilise scraper en cache â†’ Ã‰TAPE 6

2. ExÃ©cution scraper
   â””â”€> Scraping parallÃ¨le (rapide, pas de re-exploration)
```

### Cas 3 : Scraper SupprimÃ© mais DonnÃ©es Existantes

```
1. VÃ©rification cache
   â”œâ”€> {cache_key}_scraper.py n'existe pas
   â””â”€> {cache_key}_data.json existe

2. RÃ©utilisation donnÃ©es
   â””â”€> Charge donnÃ©es d'exploration depuis _data.json

3. RÃ©gÃ©nÃ©ration scraper (ScraperGenerator avec Gemini)
   â””â”€> Gemini rÃ©gÃ©nÃ¨re scraper depuis donnÃ©es existantes
       â””â”€> Pas de re-exploration Gemini (Ã©conomise API calls)

4. ExÃ©cution scraper
   â””â”€> Scraping parallÃ¨le
```

### Cas 4 : Suppression ComplÃ¨te (Dashboard)

```
1. Utilisateur supprime scraper depuis dashboard
   â””â”€> DELETE /api/scraper-ai/cache?key={cacheKey}
       â””â”€> Supprime {cache_key}.json (ancien format)

2. Prochain scrape
   â”œâ”€> {cache_key}_scraper.py existe toujours
   â”‚   â””â”€> Utilise scraper en cache â†’ Ã‰TAPE 6
   â”‚
   â””â”€> Si .py supprimÃ© manuellement:
       â”œâ”€> {cache_key}_data.json existe
       â”‚   â””â”€> RÃ©utilise donnÃ©es â†’ Ã‰TAPE 5
       â””â”€> Si _data.json aussi supprimÃ©
           â””â”€> Exploration complÃ¨te â†’ Ã‰TAPE 3
```

---

## ğŸš¨ Gestion des Erreurs

```
ERREURS POSSIBLES:

1. IndexError lors de la gÃ©nÃ©ration
   â””â”€> Cause: Accolades non Ã©chappÃ©es dans template
   â””â”€> Solution: Template utilise {{ }} pour Ã©chapper

2. ModuleNotFoundError
   â””â”€> Cause: Import circulaire
   â””â”€> Solution: PROMPT_VERSION dÃ©placÃ© dans config.py

3. Scraper gÃ©nÃ¨re "MVM Motosport" au lieu de noms produits
   â””â”€> Cause: SÃ©lecteur pointe vers header/nav
   â””â”€> Solution: is_in_header_nav_footer() + is_generic_name()

4. ERR_CONNECTION_REFUSED depuis dashboard
   â””â”€> Cause: Serveur Next.js non dÃ©marrÃ©
   â””â”€> Solution: npm run dev dans dashboard_web/

5. Serveur crash lors du lancement scraper
   â””â”€> Cause: Processus Python non dÃ©tachÃ©
   â””â”€> Solution: nohup + shell script + unref()
```

---

## ğŸ“ RÃ©sumÃ© des Fichiers ClÃ©s

```
scraper_ai/
â”œâ”€â”€ main.py                    â† Point d'entrÃ©e CLI
â”œâ”€â”€ scraper_executor.py        â† Orchestrateur + exÃ©cution
â”œâ”€â”€ html_analyzer.py           â† Analyse + gÃ©nÃ©ration scraper
â”œâ”€â”€ exploration_agent.py       â† Exploration + extraction Gemini
â”œâ”€â”€ scraper_generator.py       â† GÃ©nÃ©ration scraper avec Gemini
â”œâ”€â”€ site_data_storage.py       â† Stockage donnÃ©es structurÃ©es
â”œâ”€â”€ ai_tools.py                â† Outils disponibles pour scraper
â”œâ”€â”€ gemini_client.py           â† Client API Gemini
â”œâ”€â”€ config.py                  â† Configuration + schÃ©mas
â”‚   â””â”€â”€ PROMPT_VERSION = "3.3" â† Version du prompt
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ scraper_template.py    â† Template de rÃ©fÃ©rence
â””â”€â”€ scraper_cache/             â† Cache des scrapers gÃ©nÃ©rÃ©s
    â”œâ”€â”€ {cache_key}_data.json  â† DonnÃ©es d'exploration
    â””â”€â”€ {cache_key}_scraper.py â† Scraper Python gÃ©nÃ©rÃ©

dashboard_web/
â”œâ”€â”€ src/app/api/
â”‚   â”œâ”€â”€ scraper-ai/
â”‚   â”‚   â”œâ”€â”€ analyze/route.ts    â† Analyse site
â”‚   â”‚   â”œâ”€â”€ run/route.ts        â† Lance scraping
â”‚   â”‚   â””â”€â”€ cache/route.ts     â† Gestion cache (GET/DELETE)
â”‚   â””â”€â”€ scraper/
â”‚       â”œâ”€â”€ run/route.ts        â† Lance scraping (ancien)
â”‚       â””â”€â”€ status/route.ts     â† Statut scraping
â””â”€â”€ src/components/
    â”œâ”€â”€ ai-agent.tsx            â† Interface analyse
    â”œâ”€â”€ scraper-config.tsx      â† Interface scraping
    â””â”€â”€ scraper-dashboard.tsx   â† Affichage produits
```

---

## ğŸ“ Points ClÃ©s Ã  Retenir

1. **Nouveau flux en 3 Ã©tapes** :
   - ExplorationAgent dÃ©couvre URLs et extrait infos
   - SiteDataStorage sauvegarde donnÃ©es structurÃ©es
   - ScraperGenerator gÃ©nÃ¨re scraper personnalisÃ© avec Gemini

2. **Template comme rÃ©fÃ©rence** :
   - Le template sert de structure de base
   - Gemini adapte le code au site spÃ©cifique
   - URLs et sÃ©lecteurs sont hardcodÃ©s dans le scraper gÃ©nÃ©rÃ©

3. **Cache en 2 fichiers** :
   - `{cache_key}_data.json` : DonnÃ©es d'exploration
   - `{cache_key}_scraper.py` : Scraper Python gÃ©nÃ©rÃ©

4. **RÃ©utilisation intelligente** :
   - Si scraper existe â†’ Utilise directement
   - Si scraper manque mais donnÃ©es existent â†’ RÃ©gÃ©nÃ¨re sans re-exploration
   - Si tout manque â†’ Exploration complÃ¨te

5. **Suppression depuis dashboard** :
   - Supprime seulement les fichiers `.json` (ancien format)
   - Les fichiers `.py` restent mais peuvent Ãªtre ignorÃ©s
   - Prochain scrape rÃ©utilise `.py` si existe, sinon rÃ©gÃ©nÃ¨re depuis `.json` si existe

6. **Scraping parallÃ¨le** :
   - 20 threads simultanÃ©s
   - Utilise ThreadPoolExecutor
   - URLs et sÃ©lecteurs hardcodÃ©s pour performance

---

**Date de mise Ã  jour :** 2025-01-27
**Version du prompt :** 3.3
**Architecture :** ExplorationAgent â†’ SiteDataStorage â†’ ScraperGenerator (Gemini)
