"""
Classe principale SupplierScraper refactoris√©e
"""
import os
import re
import json
import time
from typing import Dict, List, Optional, Set, Tuple, Any
from urllib.parse import urljoin, urlparse
from threading import Lock
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
from bs4 import BeautifulSoup
from google.genai import types

from .config import (
    EXTRACTION_SCHEMA, PAGE_SELECTION_SCHEMA, IMAGE_FILTER_SCHEMA, MAX_PAGES_TO_VISIT,
    MIN_PRODUCTS_TARGET, MIN_COMPANY_INFO_FIELDS, MAX_IMAGES,
    MAX_IMAGE_SIZE_MB, MAX_TOTAL_SIZE_MB, SUPPORTED_IMAGE_MIMES
)
from .selenium_utils import (
    SELENIUM_AVAILABLE, fetch_page_with_selenium, extract_navigation_links_selenium
)
from .extractors import (
    extract_contact_info_from_links, extract_visible_text, extract_images,
    download_image, extract_navigation_links
)
from .gemini_client import GeminiClient


class SupplierScraper:
    """Scraper principal pour extraire les donn√©es des fournisseurs"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url.rstrip('/')
        self.parsed_base = urlparse(base_url)
        self.visited_urls: Set[str] = set()
        self.visited_urls_list: List[str] = []
        self.all_data: Dict = {
            "companyInfo": {},
            "products": []
        }
        self.page_outputs: List[Dict] = []
        self.gemini_client = GeminiClient()
        self.session = requests.Session()
        
        # Cr√©er le dossier assets pour les images
        from pathlib import Path
        project_root = Path(__file__).parent.parent.parent
        self.assets_dir = project_root / 'assets' / 'scraped-images'
        os.makedirs(self.assets_dir, exist_ok=True)
        print(f"üìÅ Dossier assets: {self.assets_dir}")
        
        # Dictionnaires pour stocker les images
        self.page_product_images: Dict[str, List[Dict]] = {}
        self.page_logo_images: Dict[str, List[Dict]] = {}
        
        # Verrou pour la thread-safety
        self.lock = Lock()
    
    def is_same_domain(self, url: str) -> bool:
        """V√©rifie si l'URL appartient au m√™me domaine"""
        try:
            parsed = urlparse(url)
            return parsed.netloc == self.parsed_base.netloc or parsed.netloc == ''
        except:
            return False
    
    def normalize_image_url(self, url: str) -> str:
        """Normalise une URL d'image (www vs non-www, etc.)"""
        if not url or str(url).lower() in ["null", "none", ""]:
            return ""
        try:
            parsed = urlparse(url)
            base_netloc = self.parsed_base.netloc
            if base_netloc.startswith('www.'):
                if not parsed.netloc.startswith('www.'):
                    parsed = parsed._replace(netloc='www.' + parsed.netloc)
            else:
                if parsed.netloc.startswith('www.'):
                    parsed = parsed._replace(netloc=parsed.netloc[4:])
            return parsed.geturl()
        except:
            return url
    
    def normalize_url(self, url: str) -> str:
        """Normalise une URL (supprime les fragments, etc.)"""
        if url.startswith('#'):
            return None
        if url.startswith('javascript:'):
            return None
        if url.startswith('mailto:'):
            return None
        
        absolute_url = urljoin(self.base_url, url)
        parsed = urlparse(absolute_url)
        
        normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        if parsed.query:
            normalized += f"?{parsed.query}"
        
        return normalized
    
    def fetch_page(self, url: str, use_selenium: bool = False) -> Tuple[Optional[BeautifulSoup], Optional[Any]]:
        """R√©cup√®re et parse une page HTML"""
        driver = None
        try:
            print(f"üìÑ Fetching: {url}")
            
            if use_selenium and SELENIUM_AVAILABLE:
                print(f"   ü§ñ Utilisation de Selenium...")
                result = fetch_page_with_selenium(
                    url, return_driver=True,
                    normalize_url_func=self.normalize_url,
                    is_same_domain_func=self.is_same_domain
                )
                if result:
                    soup, driver = result
                    return soup, driver
                return None, None
            
            # Essayer d'abord avec requests
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = self.session.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # V√©rifier s'il y a des popups
            popup_indicators = soup.find_all(['div', 'section'], 
                class_=re.compile(r'popup|modal|language|welcome|overlay', re.I))
            text = soup.get_text().lower()
            has_language_popup = any(term in text for term in ['choisissez la langue', 'choose your language'])
            
            if (popup_indicators or has_language_popup) and SELENIUM_AVAILABLE:
                print(f"   üîç Popup d√©tect√©, utilisation de Selenium...")
                result = fetch_page_with_selenium(
                    url, return_driver=True,
                    normalize_url_func=self.normalize_url,
                    is_same_domain_func=self.is_same_domain
                )
                if result:
                    soup, driver = result
                    return soup, driver
            
            return soup, None
        except Exception as e:
            print(f"‚ùå Error fetching {url}: {e}")
            if SELENIUM_AVAILABLE:
                print(f"   üîÑ Tentative avec Selenium...")
                result = fetch_page_with_selenium(
                    url, return_driver=True,
                    normalize_url_func=self.normalize_url,
                    is_same_domain_func=self.is_same_domain
                )
                if result:
                    soup, driver = result
                    return soup, driver
            return None, None
    
    def select_pages_and_extract_contact_info(self, all_links: List[str], contact_info_from_links: Dict[str, str]) -> Dict:
        """Premier appel Gemini: s√©lectionner les 5 pages + extraire infos de contact"""
        print(f"\n{'='*60}")
        print(f"ü§ñ APPEL GEMINI #1: S√âLECTION DES PAGES + EXTRACTION CONTACT")
        print(f"{'='*60}")
        
        empty_schema = {"companyInfo": {}, "products": []}
        
        contact_info_text = ""
        if contact_info_from_links.get("email"):
            contact_info_text += f"üìß Email trouv√© dans mailto: {contact_info_from_links['email']}\n"
        if contact_info_from_links.get("phone"):
            contact_info_text += f"üìû T√©l√©phone trouv√© dans tel: {contact_info_from_links['phone']}\n"
        if contact_info_from_links.get("address"):
            contact_info_text += f"üìç Adresse trouv√©e dans Google Maps: {contact_info_from_links['address'][:100]}...\n"
        
        prompt = f"""Tu es un expert en navigation web pour un syst√®me de campagnes de financement scolaire.

T√ÇCHE: S√©lectionner les 5 pages les plus pertinentes (incluant la page d'accueil) pour extraire les donn√©es d'un fournisseur alimentaire selon le sch√©ma suivant:

SCH√âMA CIBLE (vide - ce qu'on doit remplir):
{json.dumps(empty_schema, ensure_ascii=False, indent=2)}

INFORMATIONS DE CONTACT D√âJ√Ä EXTRAITES DEPUIS LES LIENS:
{contact_info_text if contact_info_text else "Aucune information de contact trouv√©e dans les liens"}

LIENS DISPONIBLES (incluant la page d'accueil):
{chr(10).join([f"{i+1}. {link}" for i, link in enumerate(all_links[:50])])}

INSTRUCTIONS CRITIQUES:
1. **S√âLECTION DES 5 PAGES**:
   - Tu dois s√©lectionner EXACTEMENT 5 pages maximum
   - PRIORIT√â #1: Page de catalogue/produits avec LISTE DE PRODUITS INDIVIDUELS
   - PRIORIT√â #2: Pages de cat√©gories qui LISTENT des produits individuels
   - PRIORIT√â #3: Page contact/nous joindre
   - PRIORIT√â #4: Page √† propos
   - ‚ö†Ô∏è IMPORTANT: Privil√©gie les pages qui contiennent des LISTES de produits individuels

2. **EXTRACTION DES INFOS DE CONTACT**:
   - Si tu vois des liens mailto:, tel:, ou Google Maps, extrais-les
   - Compl√®te les infos d√©j√† extraites

Retourne un JSON avec cette structure:
{{
  "selectedPages": ["url1", "url2", "url3", "url4", "url5"],
  "contactInfo": {{
    "email": "email@example.com" ou "",
    "phone": "819 295-3325" ou "",
    "address": "Adresse compl√®te" ou ""
  }},
  "reasoning": "Explication"
}}"""
        
        schema = PAGE_SELECTION_SCHEMA
        
        try:
            # Utiliser gemini-flash-latest pour la s√©lection (rapide)
            result = self.gemini_client.call(prompt, schema, show_prompt=False, use_flash=True)
            
            selected_pages = result.get("selectedPages", [])
            contact_info = result.get("contactInfo", {})
            reasoning = result.get("reasoning", "")
            
            # Fusionner les infos de contact
            final_contact_info = contact_info_from_links.copy()
            for key, value in contact_info.items():
                if value and not final_contact_info.get(key):
                    final_contact_info[key] = value
            
            print(f"‚úÖ {len(selected_pages)} pages s√©lectionn√©es:")
            for i, page in enumerate(selected_pages, 1):
                print(f"   {i}. {page}")
            print(f"\nüí≠ Raisonnement: {reasoning[:300]}...")
            print(f"\nüìß Infos de contact extraites:")
            print(f"   Email: {final_contact_info.get('email', 'N/A')}")
            print(f"   T√©l√©phone: {final_contact_info.get('phone', 'N/A')}")
            print(f"   Adresse: {final_contact_info.get('address', 'N/A')[:80]}...")
            print(f"{'='*60}\n")
            
            return {
                "selectedPages": selected_pages,
                "contactInfo": final_contact_info
            }
        except Exception as e:
            print(f"‚ùå Erreur lors de la s√©lection des pages: {e}")
            # Fallback
            fallback_pages = [self.base_url]
            product_keywords = ["produit", "catalogue", "menu", "gamme"]
            contact_keywords = ["contact", "nous-joindre", "about", "a-propos"]
            
            for link in all_links[:20]:
                link_lower = link.lower()
                if len(fallback_pages) < 5:
                    if any(kw in link_lower for kw in product_keywords) or any(kw in link_lower for kw in contact_keywords):
                        if link not in fallback_pages:
                            fallback_pages.append(link)
            
            print(f"üîÑ Fallback: {len(fallback_pages)} pages s√©lectionn√©es")
            return {
                "selectedPages": fallback_pages[:5],
                "contactInfo": contact_info_from_links
            }
    
    def filter_images_with_gemini(self, pages_data: List[Dict]) -> Dict[str, List[str]]:
        """Deuxi√®me appel Gemini: Filtrer les images pour ne garder que logo + produits
        NOTE: On ne passe QUE les m√©tadonn√©es (URL, alt text, contexte), pas les images elles-m√™mes
        pour √©viter d'uploader trop d'images. On uploadera seulement les images filtr√©es dans le mega call."""
        print(f"\n{'='*60}")
        print(f"üîç APPEL GEMINI #2: FILTRAGE DES IMAGES (M√âTADONN√âES SEULEMENT)")
        print(f"{'='*60}")
        
        # Collecter toutes les images avec leur contexte (sans uploader les images)
        all_images_info = []
        for page_data in pages_data:
            url = page_data["url"]
            images = page_data["images"]
            
            for img in images:
                if img.get("url"):
                    all_images_info.append({
                        "url": img["url"],
                        "alt": img.get("alt", ""),
                        "context_before": img.get("context_before", ""),
                        "context_after": img.get("context_after", ""),
                        "page_url": url,
                        "is_logo": img.get("is_logo", False)
                    })
        
        if not all_images_info:
            print("   ‚ö†Ô∏è Aucune image √† filtrer")
            return {"logoImage": None, "productImages": []}
        
        print(f"üìä {len(all_images_info)} images √† analyser pour filtrage (m√©tadonn√©es seulement)")
        
        # Construire le prompt avec toutes les m√©tadonn√©es d'images (pas les images elles-m√™mes)
        images_context = ""
        for i, img_info in enumerate(all_images_info, 1):
            images_context += f"\n--- Image {i} ---\n"
            images_context += f"URL: {img_info['url']}\n"
            if img_info.get('alt'):
                images_context += f"Alt text: {img_info['alt']}\n"
            if img_info.get('context_before'):
                images_context += f"Contexte avant: {img_info['context_before']}\n"
            if img_info.get('context_after'):
                images_context += f"Contexte apr√®s: {img_info['context_after']}\n"
            images_context += f"Page: {img_info['page_url']}\n"
            if img_info.get('is_logo'):
                images_context += f"‚ö†Ô∏è Potentiel logo d√©tect√©\n"
        
        prompt = f"""Tu es un expert en analyse d'images pour un syst√®me de campagnes de financement scolaire.

T√ÇCHE: Filtrer les images pour ne garder que:
1. **LE LOGO PRINCIPAL** (un seul, le meilleur) - pour l'entreprise
2. **LES IMAGES DE PRODUITS INDIVIDUELS** (pas de cat√©gories, pas de doublons) - pour le catalogue

IMAGES TROUV√âES ({len(all_images_info)} images) - M√âTADONN√âES SEULEMENT:
{images_context}

INSTRUCTIONS CRITIQUES:
1. **LOGO**:
   - S√©lectionne UN SEUL logo principal (le meilleur, le plus clair)
   - Utilise l'URL, l'alt text et le contexte pour identifier le logo
   - Ignore les logos dupliqu√©s, les logos de footer, les logos flous
   - Le logo doit √™tre de bonne qualit√© et repr√©sentatif de l'entreprise

2. **IMAGES DE PRODUITS**:
   - ‚ö†Ô∏è CRITIQUE: Garde SEULEMENT les images de PRODUITS INDIVIDUELS avec NOMS SP√âCIFIQUES
   - Exemples de PRODUITS INDIVIDUELS (‚úÖ GARDER): "Le Cendrillon" (fromage), "Tarte aux pommes artisanale", "Chocolat noir 70%"
   - Exemples de CAT√âGORIES (‚ùå IGNORER): "S√©lection de fromages", "Assortiment d'√©picerie", "Gamme de produits"
   - Utilise l'alt text et le contexte (texte avant/apr√®s) pour identifier le nom du produit
   - Ignore les images d√©coratives, les banni√®res, les ic√¥nes, les images de cat√©gories
   - Ignore les doublons (m√™me produit, m√™me image)
   - Les images doivent √™tre de qualit√© suffisante pour un magasin en ligne
   - PRIORIT√â: Produits adapt√©s aux campagnes de financement scolaire

3. **FILTRAGE**:
   - Utilise l'alt text et le contexte autour de l'image pour identifier le type
   - Si l'image n'a pas de nom de produit sp√©cifique dans son contexte, ignore-la
   - Garde seulement les images qui peuvent √™tre utilis√©es directement dans le catalogue

Retourne un JSON avec:
- logoImage: {{url, reason}} ou null si aucun logo valide
- productImages: [{{url, productName, reason}}] - liste des images de produits individuels
- reasoning: Explication du filtrage"""
        
        try:
            # Appel Gemini avec seulement le texte (pas d'images upload√©es)
            result = self.gemini_client.call(prompt, IMAGE_FILTER_SCHEMA, show_prompt=False)
            
            logo_url = result.get("logoImage", {}).get("url") if result.get("logoImage") else None
            product_images = result.get("productImages", [])
            reasoning = result.get("reasoning", "")
            
            print(f"‚úÖ Filtrage termin√©:")
            if logo_url:
                print(f"   üè¢ Logo: {logo_url[:80]}...")
            else:
                print(f"   üè¢ Logo: Aucun logo valide trouv√©")
            print(f"   üì¶ Images produits: {len(product_images)}")
            print(f"   üí≠ Raisonnement: {reasoning[:200]}...")
            print(f"{'='*60}\n")
            
            return {
                "logoImage": logo_url,
                "productImages": [img["url"] for img in product_images],
                "productImageDetails": product_images  # Garder les d√©tails pour r√©f√©rence
            }
        except Exception as e:
            print(f"‚ùå Erreur lors du filtrage d'images: {e}")
            # Fallback: garder toutes les images
            return {"logoImage": None, "productImages": [img["url"] for img in all_images_info[:50]]}
    
    def extract_all_data_mega_call(self, pages_data: List[Dict], contact_info: Dict[str, str]) -> Dict:
        """Deuxi√®me appel Gemini: mega prompt avec HTML complet des pages
        On passe le HTML complet pour que Gemini puisse voir la structure DOM et associer automatiquement les images aux produits."""
        print(f"\n{'='*60}")
        print(f"ü§ñ APPEL GEMINI #2: EXTRACTION MEGA CALL (HTML COMPLET)")
        print(f"{'='*60}")
        print(f"üìä {len(pages_data)} pages √† analyser avec HTML complet")
        
        # Pr√©parer le contexte avec HTML complet
        pages_html = ""
        total_size = 0
        
        for i, page_data in enumerate(pages_data, 1):
            url = page_data["url"]
            html_content = page_data.get("html", "")
            
            if not html_content:
                print(f"   ‚ö†Ô∏è Pas de HTML pour {url}, utilisation du texte")
                html_content = f"<html><body>{page_data.get('text', '')}</body></html>"
            
            html_size = len(html_content)
            total_size += html_size
            
            pages_html += f"\n{'‚îÄ'*60}\n"
            pages_html += f"PAGE {i}: {url}\n"
            pages_html += f"{'‚îÄ'*60}\n"
            pages_html += f"HTML COMPLET ({html_size} caract√®res, {html_size/1024:.1f} KB):\n"
            pages_html += html_content
            pages_html += f"\n\n"
        
        total_size_mb = total_size / 1024 / 1024
        print(f"üìä Taille totale HTML: {total_size} caract√®res ({total_size_mb:.2f} MB)")
        
        # V√©rifier si √ßa rentre dans le contexte (1M tokens ‚âà 4M caract√®res)
        # On est largement en dessous m√™me avec 5 pages HTML compl√®tes
        if total_size > 3_000_000:  # ~750K tokens, laisser de la marge
            print(f"‚ö†Ô∏è HTML tr√®s volumineux ({total_size_mb:.2f} MB), tronquage possible")
            # Tronquer chaque page proportionnellement
            max_size_per_page = 3_000_000 // len(pages_data)
            pages_html = ""
            for i, page_data in enumerate(pages_data, 1):
                url = page_data["url"]
                html_content = page_data.get("html", "")
                if len(html_content) > max_size_per_page:
                    html_content = html_content[:max_size_per_page] + "\n[... HTML tronqu√© ...]"
                pages_html += f"\n{'‚îÄ'*60}\nPAGE {i}: {url}\n{'‚îÄ'*60}\n{html_content}\n\n"
        
        # Pr√©parer les infos de contact
        contact_info_text = ""
        if contact_info.get("email"):
            contact_info_text += f"üìß Email: {contact_info['email']}\n"
        if contact_info.get("phone"):
            contact_info_text += f"üìû T√©l√©phone: {contact_info['phone']}\n"
        if contact_info.get("address"):
            contact_info_text += f"üìç Adresse: {contact_info['address']}\n"
        
        prompt = f"""Tu es un expert en extraction de donn√©es pour un syst√®me de campagnes de financement scolaire.

T√ÇCHE: Extraire TOUTES les donn√©es d'un fournisseur alimentaire depuis {len(pages_data)} pages web analys√©es.

INFORMATIONS DE CONTACT D√âJ√Ä EXTRAITES DEPUIS LES LIENS:
{contact_info_text if contact_info_text else "Aucune information de contact trouv√©e dans les liens"}

HTML COMPLET DES {len(pages_data)} PAGES (apr√®s chargement JavaScript):
{pages_html}

IMPORTANT: Le HTML contient la structure DOM compl√®te apr√®s chargement JavaScript. Utilise cette structure pour:
- Identifier les produits individuels et leurs images associ√©es (m√™me div, m√™me container)
- Comprendre la hi√©rarchie et les relations entre √©l√©ments
- Extraire les images directement depuis les balises <img> dans le HTML (attributs src, data-src, data-lazy-src)
- Associer chaque image √† son produit correspondant bas√© sur la structure DOM (proximit√© dans le HTML)
- Les images sont d√©j√† dans le HTML, pas besoin de les t√©l√©charger s√©par√©ment

INSTRUCTIONS CRITIQUES:
1. **EXTRACTION DES PRODUITS (PRIORIT√â ABSOLUE)**:
   - ‚ö†Ô∏è CRITIQUE: Extrais des PRODUITS INDIVIDUELS avec NOMS SP√âCIFIQUES, pas des cat√©gories!
   - Exemples de PRODUITS INDIVIDUELS (‚úÖ BON): "Le Cendrillon" (fromage), "Tarte aux pommes artisanale"
   - Exemples de CAT√âGORIES (‚ùå √Ä √âVITER): "S√©lection de fromages", "Assortiment d'√©picerie"
   - PRIORIT√â: Produits adapt√©s aux campagnes de financement scolaire
   - Chaque produit doit avoir: name, description, pricePickup, image
   - **IMAGES**: Utilise la structure DOM pour trouver l'image associ√©e √† chaque produit
     * Les images sont g√©n√©ralement dans le m√™me container/div que le nom du produit
     * Cherche les balises <img> proches du nom/description du produit
     * Utilise les attributs src, data-src, ou data-lazy-src des images
     * Si plusieurs images sont dans le m√™me container produit, prends la premi√®re/principale
   - Si un prix n'est pas trouv√©, mets 0

2. **INFORMATIONS ENTREPRISE**:
   - Utilise les infos de contact d√©j√† extraites
   - Compl√®te avec les infos trouv√©es dans les pages
   - Champs requis: name, email, phone, address, logo, description, website

3. **IMAGES**:
   - Les images sont dans le HTML - utilise les balises <img> pour les extraire
   - Identifie les images de produits vs logos (logos souvent dans header/footer)
   - Associe chaque image √† son produit en utilisant la structure DOM (m√™me container/div)
   - Pour chaque produit, trouve l'image la plus proche dans le HTML

4. **COMPL√âTUDE DES CHAMPS (TR√àS IMPORTANT)**:
   - ‚ö†Ô∏è CRITIQUE: Remplis TOUS les champs du sch√©ma pour chaque produit, m√™me si les donn√©es exactes ne sont pas disponibles
   - Si une donn√©e exacte n'est pas disponible, fais une ESTIMATION RAISONNABLE bas√©e sur:
     * Le type de produit (ex: fromage = r√©frig√©r√©, chocolat = non r√©frig√©r√©)
     * Les images du produit
     * Le contexte du site
     * Les standards de l'industrie
   - Pour les prix manquants, utilise 0
   - Pour les autres champs, fais ton meilleur effort pour les remplir avec des valeurs r√©alistes

5. **QUALIT√â**:
   - Assure-toi que le sch√©ma final est le PLUS COMPLET possible
   - Ne perds AUCUNE information importante
   - √âlimine les doublons de produits (m√™me nom = m√™me produit)

Retourne un JSON conforme au sch√©ma EXTRACTION_SCHEMA avec TOUTES les donn√©es extraites."""
        
        try:
            # Construire le contenu avec seulement le prompt (HTML inclus dans le prompt)
            # Pas besoin d'uploader les images - elles sont dans le HTML
            contents = [prompt]
            
            print(f"üìä Envoi √† Gemini:")
            print(f"   - {len(pages_data)} pages HTML compl√®tes")
            print(f"   - {total_size} caract√®res HTML ({total_size_mb:.2f} MB)")
            print(f"   - Images incluses dans le HTML (structure DOM)")
            
            # Utiliser gemini-2.5-pro pour l'extraction (pr√©cis)
            final_schema = self.gemini_client.call(
                contents, EXTRACTION_SCHEMA, show_prompt=False, use_flash=False
            )
            
            print(f"‚úÖ Extraction termin√©e!")
            print(f"üì¶ Produits extraits: {len(final_schema.get('products', []))}")
            print(f"üè¢ Champs entreprise: {len([k for k, v in final_schema.get('companyInfo', {}).items() if v])}")
            print(f"{'='*60}\n")
            
            return final_schema
        except Exception as e:
            print(f"‚ùå Erreur lors de l'extraction mega call: {e}")
            return {"companyInfo": {}, "products": []}
    
    def scrape(self) -> Dict:
        """Fonction principale de scraping - OPTIMIS√âE: 2 appels Gemini seulement"""
        print(f"üöÄ D√©marrage du scraping pour: {self.base_url}")
        print(f"‚öôÔ∏è Configuration: MAX_PAGES={MAX_PAGES_TO_VISIT}")
        print(f"üìã Flow optimis√©: 2 appels Gemini (s√©lection pages + extraction mega call avec HTML complet)")
        
        # √âTAPE 1: R√©cup√©rer la page d'accueil et extraire tous les liens + infos de contact
        print(f"\n{'='*60}")
        print(f"üìç √âTAPE 1: R√âCUP√âRATION PAGE D'ACCUEIL + EXTRACTION LIENS")
        print(f"{'='*60}")
        
        soup, driver = self.fetch_page(self.base_url)
        if not soup:
            print(f"‚ùå Impossible de r√©cup√©rer la page d'accueil")
            return self.all_data
        
        # Extraire les informations de contact
        contact_info_from_links = extract_contact_info_from_links(soup)
        print(f"üìß Infos de contact extraites depuis les liens:")
        if contact_info_from_links.get("email"):
            print(f"   Email: {contact_info_from_links['email']}")
        if contact_info_from_links.get("phone"):
            print(f"   T√©l√©phone: {contact_info_from_links['phone']}")
        if contact_info_from_links.get("address"):
            print(f"   Adresse: {contact_info_from_links['address'][:80]}...")
        
        # Extraire tous les liens de navigation
        if driver:
            all_links = extract_navigation_links_selenium(
                driver, self.base_url, self.normalize_url, self.is_same_domain
            )
            driver.quit()
        else:
            all_links = extract_navigation_links(
                soup, self.base_url, self.normalize_url, self.is_same_domain
            )
        
        # Ajouter la page d'accueil si elle n'est pas dans la liste
        if self.base_url not in all_links:
            all_links.insert(0, self.base_url)
        
        print(f"üîó {len(all_links)} liens trouv√©s (incluant la page d'accueil)")
        
        # √âTAPE 2: Premier appel Gemini - S√©lectionner les 5 pages + extraire infos de contact
        selection_result = self.select_pages_and_extract_contact_info(all_links, contact_info_from_links)
        selected_pages = selection_result["selectedPages"]
        final_contact_info = selection_result["contactInfo"]
        
        # Limiter √† 5 pages maximum
        selected_pages = selected_pages[:5]
        
        print(f"\n{'='*60}")
        print(f"üìç √âTAPE 2: T√âL√âCHARGEMENT PARALL√àLE DES {len(selected_pages)} PAGES S√âLECTIONN√âES")
        print(f"{'='*60}")
        
        # Fonction pour t√©l√©charger une page
        def fetch_single_page(url: str) -> Optional[Dict]:
            """T√©l√©charge une page et retourne ses donn√©es"""
            try:
                print(f"üì• T√©l√©chargement: {url}")
                page_soup, page_driver = self.fetch_page(url, use_selenium=True)
                if not page_soup:
                    print(f"   ‚ö†Ô∏è Impossible de r√©cup√©rer {url}")
                    return None
                
                # R√©cup√©rer le HTML complet apr√®s chargement JavaScript
                if page_driver:
                    # Attendre que la page soit compl√®tement charg√©e
                    time.sleep(2)
                    # Scroll pour charger le contenu dynamique
                    page_driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(2)
                    page_driver.execute_script("window.scrollTo(0, 0);")
                    time.sleep(1)
                    # R√©cup√©rer le HTML complet
                    html_content = page_driver.page_source
                    page_driver.quit()
                else:
                    # Fallback: utiliser BeautifulSoup pour obtenir le HTML
                    html_content = str(page_soup)
                
                # Extraire aussi le texte pour r√©f√©rence (mais on utilisera le HTML)
                text = extract_visible_text(page_soup)
                
                html_size_kb = len(html_content) / 1024
                print(f"   ‚úÖ {url}: {len(html_content)} caract√®res HTML ({html_size_kb:.1f} KB), {len(text)} caract√®res texte")
                
                return {
                    "url": url,
                    "html": html_content,
                    "text": text  # Garder pour r√©f√©rence/debug
                }
            except Exception as e:
                print(f"   ‚ùå Erreur sur {url}: {e}")
                return None
        
        # T√©l√©charger toutes les pages en parall√®le
        pages_data = []
        with ThreadPoolExecutor(max_workers=5) as executor:
            # Lancer tous les t√©l√©chargements en parall√®le
            future_to_url = {executor.submit(fetch_single_page, url): url for url in selected_pages}
            
            # Collecter les r√©sultats au fur et √† mesure qu'ils arrivent
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    page_data = future.result()
                    if page_data:
                        pages_data.append(page_data)
                except Exception as e:
                    print(f"   ‚ùå Exception lors du t√©l√©chargement de {url}: {e}")
        
        print(f"‚úÖ {len(pages_data)}/{len(selected_pages)} pages t√©l√©charg√©es avec succ√®s")
        
        if not pages_data:
            print(f"‚ùå Aucune page t√©l√©charg√©e avec succ√®s")
            return self.all_data
        
        # √âTAPE 3: Deuxi√®me appel Gemini - Extraction mega call avec HTML complet
        # On passe le HTML complet pour que Gemini puisse voir la structure DOM et associer les images aux produits
        final_schema = self.extract_all_data_mega_call(pages_data, final_contact_info)
        
        # Mettre √† jour all_data avec le sch√©ma final
        self.all_data = final_schema
        
        # Afficher les r√©sultats finaux
        company_info = self.all_data.get('companyInfo', {})
        company_fields_filled = len([k for k, v in company_info.items() if v and str(v).lower() not in ["null", "none", ""]])
        products_count = len(self.all_data.get('products', []))
        
        print(f"\n{'='*60}")
        print(f"‚úÖ SCRAPING TERMIN√â!")
        print(f"üè¢ Informations entreprise: {company_fields_filled} champs remplis (objectif: {MIN_COMPANY_INFO_FIELDS})")
        print(f"üì¶ Produits trouv√©s: {products_count} (objectif: {MIN_PRODUCTS_TARGET})")
        
        # Afficher si les objectifs sont atteints
        if products_count >= MIN_PRODUCTS_TARGET and company_fields_filled >= MIN_COMPANY_INFO_FIELDS:
            print(f"‚úÖ Objectifs atteints - scraping optimis√©!")
        elif products_count >= MIN_PRODUCTS_TARGET:
            print(f"‚ö†Ô∏è Produits OK mais infos entreprise incompl√®tes")
        elif company_fields_filled >= MIN_COMPANY_INFO_FIELDS:
            print(f"‚ö†Ô∏è Infos entreprise OK mais produits insuffisants")
        else:
            print(f"‚ö†Ô∏è Objectifs non atteints")
        
        print(f"{'='*60}\n")
        
        return self.all_data

