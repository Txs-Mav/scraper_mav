import { NextResponse } from 'next/server'
import { spawn } from 'child_process'
import path from 'path'
import fs from 'fs'
import { createClient } from '@/lib/supabase/server'

export async function POST(request: Request) {
  try {
    // R√©cup√©rer l'utilisateur connect√©
    const supabase = await createClient()
    const { data: { user }, error: authError } = await supabase.auth.getUser()

    if (authError || !user) {
      console.error('[ScraperAI] ‚ùå Utilisateur non authentifi√©')
      return NextResponse.json(
        { error: 'Authentication required', message: 'Vous devez √™tre connect√© pour lancer un scraping' },
        { status: 401 }
      )
    }

    const userId = user.id
    console.log(`[ScraperAI] ‚úÖ Utilisateur authentifi√©: ${userId}`)

    const body = await request.json()
    const { referenceUrl, urls, forceRefresh, ignoreColors, inventoryOnly } = body

    // Log d√©taill√© de ce qui est re√ßu du frontend
    console.log(`[ScraperAI] üì• Body re√ßu:`)
    console.log(`[ScraperAI]   - referenceUrl: ${referenceUrl}`)
    console.log(`[ScraperAI]   - urls re√ßues (${(urls || []).length}):`, JSON.stringify(urls || []))
    console.log(`[ScraperAI]   - forceRefresh: ${forceRefresh}`)
    console.log(`[ScraperAI]   - ignoreColors: ${ignoreColors}`)
    console.log(`[ScraperAI]   - inventoryOnly: ${inventoryOnly}`)

    if (!referenceUrl) {
      return NextResponse.json(
        { error: 'referenceUrl is required' },
        { status: 400 }
      )
    }

    // Filtrer les URLs vides et s'assurer que le site de r√©f√©rence est dans la liste
    let allUrls = (urls || []).filter((url: string) => url && url.trim() !== '')
    if (!allUrls.includes(referenceUrl)) {
      allUrls.unshift(referenceUrl)
    }

    console.log(`[ScraperAI] üìä URLs apr√®s filtrage (${allUrls.length}):`, JSON.stringify(allUrls))

    if (allUrls.length === 0) {
      return NextResponse.json(
        { error: 'At least one valid URL is required' },
        { status: 400 }
      )
    }

    // Construire la commande avec l'user-id
    const scraperModule = 'scraper_ai.main'
    const args = ['-m', scraperModule]

    // IMPORTANT: Ajouter l'ID utilisateur pour l'authentification
    args.push('--user-id', userId)

    // Ajouter l'option --reference pour la comparaison de prix
    if (referenceUrl) {
      args.push('--reference', referenceUrl)
    }

    // Ajouter l'option --force-refresh si demand√©
    if (forceRefresh) {
      args.push('--force-refresh')
    }

    // Ajouter l'option --ignore-colors si demand√© (permet plus de matchs)
    if (ignoreColors) {
      args.push('--ignore-colors')
    }

    // Ajouter l'option --inventory-only si activ√© (exclut les pages catalogue)
    if (inventoryOnly) {
      args.push('--inventory-only')
    }

    // Ajouter toutes les URLs √† scraper
    args.push(...allUrls)

    console.log(`[ScraperAI] üìã Configuration:`)
    console.log(`[ScraperAI]   - Module: ${scraperModule}`)
    console.log(`[ScraperAI]   - User ID: ${userId}`)
    console.log(`[ScraperAI]   - URLs: ${allUrls.length} site(s)`)
    console.log(`[ScraperAI]   - R√©f√©rence: ${referenceUrl}`)
    console.log(`[ScraperAI]   - Force refresh: ${forceRefresh || false}`)
    console.log(`[ScraperAI]   - Ignorer couleurs: ${ignoreColors || false}`)
    console.log(`[ScraperAI]   - Inventaire seulement: ${inventoryOnly || false}`)
    console.log(`[ScraperAI] üöÄ Commande: python3 ${args.join(' ')}`)

    const pythonCmd = process.platform === 'win32' ? 'python' : 'python3'
    const logsDir = path.join(process.cwd(), '..', 'scraper_logs')
    const timestamp = Date.now()
    const lockFile = path.join(logsDir, `scraper_${timestamp}.lock`)
    const logFile = path.join(logsDir, `scraper_${timestamp}.log`)
    const scriptFile = path.join(logsDir, `scraper_${timestamp}.sh`)

    // Cr√©er le dossier de logs
    try {
      if (!fs.existsSync(logsDir)) {
        fs.mkdirSync(logsDir, { recursive: true })
      }
    } catch (e) {
      console.error(`[ScraperAI] ‚ùå Erreur cr√©ation dossier logs:`, e)
    }

    // √âchapper les arguments pour le shell
    const escapedArgs = args.map(arg => {
      const escaped = arg.replace(/\\/g, '\\\\').replace(/"/g, '\\"').replace(/\$/g, '\\$')
      return `"${escaped}"`
    }).join(' ')

    const urlsJson = JSON.stringify(allUrls).replace(/\\/g, '\\\\').replace(/\$/g, '\\$')
    const refUrlEscaped = referenceUrl.replace(/\\/g, '\\\\').replace(/"/g, '\\"').replace(/\$/g, '\\$')

    const scriptContent = `#!/bin/bash
# Script g√©n√©r√© automatiquement pour lancer le scraper Python
cd "${path.join(process.cwd(), '..').replace(/"/g, '\\"')}"
nohup ${pythonCmd} ${escapedArgs} > "${logFile.replace(/"/g, '\\"')}" 2>&1 &
PYTHON_PID=$!
sleep 0.5
if kill -0 $PYTHON_PID 2>/dev/null; then
  cat > "${lockFile.replace(/"/g, '\\"')}" << LOCKEOF
{
  "pid": $PYTHON_PID,
  "startTime": ${timestamp},
  "userId": "${userId}",
  "urls": ${urlsJson},
  "referenceUrl": "${refUrlEscaped}",
  "logFile": "${logFile.replace(/"/g, '\\"')}"
}
LOCKEOF
  echo $PYTHON_PID
else
  echo "ERROR: Process failed to start" >&2
  exit 1
fi
`

    try {
      fs.writeFileSync(scriptFile, scriptContent, { mode: 0o755 })

      const shellProcess = spawn('bash', [scriptFile], {
        cwd: path.join(process.cwd(), '..'),
        stdio: 'pipe',
        shell: false,
        detached: false
      })

      let scriptOutput = ''
      shellProcess.stdout?.on('data', (data) => {
        scriptOutput += data.toString()
      })

      shellProcess.stderr?.on('data', (data) => {
        console.error(`[ScraperAI] ‚ö†Ô∏è Script stderr: ${data.toString()}`)
      })

      shellProcess.on('close', (code) => {
        try {
          if (fs.existsSync(scriptFile)) {
            fs.unlinkSync(scriptFile)
          }
        } catch (e) {
          // Ignorer
        }

        if (code === 0) {
          const pid = scriptOutput.trim()
          if (pid && !isNaN(parseInt(pid))) {
            console.log(`[ScraperAI] ‚úÖ Scraping lanc√© avec PID: ${pid}`)
          }
        } else {
          console.error(`[ScraperAI] ‚ùå Erreur script (code ${code}): ${scriptOutput}`)
          try {
            if (fs.existsSync(lockFile)) {
              fs.unlinkSync(lockFile)
            }
          } catch (e) {
            // Ignorer
          }
        }
      })

    } catch (error: any) {
      console.error(`[ScraperAI] ‚ùå Erreur cr√©ation/ex√©cution script:`, error)
      try {
        if (fs.existsSync(scriptFile)) fs.unlinkSync(scriptFile)
        if (fs.existsSync(lockFile)) fs.unlinkSync(lockFile)
      } catch (e) {
        // Ignorer
      }
      return NextResponse.json(
        { error: 'Failed to start scraper', message: error.message },
        { status: 500 }
      )
    }

    // Attendre que le script shell se termine
    await new Promise(resolve => setTimeout(resolve, 1500))

    // Lire le PID depuis le fichier de lock
    let pid: number | null = null
    try {
      if (fs.existsSync(lockFile)) {
        const lockData = JSON.parse(fs.readFileSync(lockFile, 'utf-8'))
        if (lockData.pid) {
          pid = lockData.pid
          console.log(`[ScraperAI] ‚úÖ Processus d√©marr√© avec PID: ${pid}`)
        }
      }
    } catch (e) {
      console.warn(`[ScraperAI] ‚ö†Ô∏è Lock file non lisible:`, e)
    }

    return NextResponse.json({
      success: true,
      message: `Scraping lanc√© pour ${allUrls.length} site(s)`,
      pid: pid,
      lockFile: lockFile,
      logFile: logFile,
      timestamp: timestamp,
      urls: allUrls,
      referenceUrl: referenceUrl
    })
  } catch (error: any) {
    console.error('[ScraperAI] ‚ùå Erreur:', error)
    return NextResponse.json(
      { error: 'Failed to run scraper', message: error.message },
      { status: 500 }
    )
  }
}

// API pour lire les logs en temps r√©el
export async function GET(request: Request) {
  try {
    const { searchParams } = new URL(request.url)
    const logFile = searchParams.get('logFile')
    const lastLine = parseInt(searchParams.get('lastLine') || '0')

    if (!logFile) {
      return NextResponse.json({ error: 'logFile parameter required' }, { status: 400 })
    }

    // V√©rifier que le fichier existe
    if (!fs.existsSync(logFile)) {
      return NextResponse.json({
        lines: [],
        totalLines: 0,
        isComplete: false,
        error: 'Log file not found yet'
      })
    }

    const content = fs.readFileSync(logFile, 'utf-8')
    const allLines = content.split('\n')
    const newLines = allLines.slice(lastLine)

    // V√©rifier si le scraping est VRAIMENT termin√©
    // On attend des patterns de FIN D√âFINITIVE qui n'apparaissent qu'une fois √† la toute fin
    const contentLower = content.toLowerCase()

    // CONDITION DE FIN : Le message "üìã APER√áU" ou "... et X autres" qui n'appara√Æt QU'√Ä LA TOUTE FIN
    // Ces messages sont les DERNIERS √† √™tre √©crits dans le log
    const hasFinalApercu = content.includes('üìã APER√áU') && (content.includes('... et') || content.includes('R√âPARTITION PAR √âTAT'))
    const hasDataLocation = content.includes('‚òÅÔ∏è  Donn√©es dans:')
    const hasSavedSuccess = content.includes('üíæ Donn√©es sauvegard√©es dans:')

    // Erreurs qui terminent le scraping
    const hasFatalError =
      contentLower.includes('erreur fatale') ||
      contentLower.includes('erreur critique') ||
      content.includes('AUTHENTIFICATION REQUISE') ||
      content.includes('‚ùå Aucun site de r√©f√©rence configur√©')

    // Le scraping est complet seulement si on a l'aper√ßu final OU la localisation des donn√©es
    const isComplete = hasFinalApercu || hasDataLocation || hasSavedSuccess || hasFatalError

    return NextResponse.json({
      lines: newLines,
      totalLines: allLines.length,
      isComplete: isComplete,
      content: lastLine === 0 ? content : undefined
    })
  } catch (error: any) {
    return NextResponse.json(
      { error: 'Failed to read log', message: error.message },
      { status: 500 }
    )
  }
}
